[{"categories":["Skills"],"content":"SOPS stands for Secrets OPerationS, with an official description as \"Simple and flexible tool for managing secrets\". With the age, it offers more convenient private key management, more intuitive operations, and easier integration with CI compared to git-crypt.","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"SOPS stands for Secrets OPerationS, with an official description as “Simple and flexible tool for managing secrets”. With the age, it offers more convenient private key management, more intuitive operations, and easier integration with CI compared to git-crypt. ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:0:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"Install packages Install SOPS and AGE ❯ brew install sops age ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:1:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"Create directories and file Create required directories to simulate multi-user collaboration ❯ mkdir -p /Users/damonguo/Workspace/demo/{repo,sops_keys} ❯ cd /Users/damonguo/Workspace/demo/ ❯ ls -1 repo sops_keys As user Alice, create secrets.yml ❯ cd repo ❯ mkdir sops-demo ❯ cd sops-demo ❯ cat \u003e secrets.yml \u003c\u003cEOF db: username: super-secret password: ultra-secret EOF ❯ cat secrets.yml db: username: super-secret password: ultra-secret ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:2:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"Generate age keys For users Alice, Bob, and Jack, generate AGE Keys ❯ touch /Users/damonguo/Workspace/demo/sops_keys/{alice,bob,jack}.key ❯ chmod 600 /Users/damonguo/Workspace/demo/sops_keys/*.key ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/alice.key ❯ age-keygen \u003e $SOPS_AGE_KEY_FILE Public key: age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/bob.key ❯ age-keygen \u003e $SOPS_AGE_KEY_FILE Public key: age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/jack.key ❯ age-keygen \u003e $SOPS_AGE_KEY_FILE Public key: age1h0ufdryerkpy39xkun9zd2hrece3g7nu9l63ws927cngwk633dcspuvgal ❯ cat /Users/damonguo/Workspace/demo/sops_keys/alice.key # created: 2026-02-03T17:41:06+08:00 # public key: age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 AGE-SECRET-KEY-1ZU2XLLK73TEAY4Z4JQFDJH4VUA796QTN3GJ246YJKV2C5N7Z7TUSM24TWZ ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:3:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"Authorize and verify As user Alice, encrypt secrets.yml and decrypt to verify ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/alice.key ❯ sops --encrypt --in-place --age age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 secrets.yml ❯ cat secrets.yml db: username: ENC[AES256_GCM,data:K4o7b88VYy1fiIRK,iv:WMPXoFF0aaxktEjaFxliYpsDgb77MVvbwviuwmYCRsk=,tag:qM3PzdjkyqYL2U+Ryhw/+Q==,type:str] password: ENC[AES256_GCM,data:SBIn28H5hlTNb0Xg,iv:BouoeNldrph5vph1ti2Y5Xny/qI5dQaFHCbsAFy/Ezo=,tag:X6SYXPgDU0R+WIVO0da+cA==,type:str] sops: age: - recipient: age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBkSkNRLzYrUjFNRTNQdHAr YitPZXNPZk8xdXZ3ZWNsRkdqaE1RSUpkVXc0Cll1TDJ5SFI2ckdIR0FNay9HWjFm QmJXSk8rdm9vQXltL3ZBbVVldGxZVU0KLS0tIFJRRlBsZmVtRW4yVStxUVFobjRo bmxUS2xPWlMydlFlTm1IZWp5OWdERncKkU/H0ovZpSCjrBRlitwp0iXiKB2VesL7 BeYvdfvC0DeNZOyUYeAageEnYdm4z5Su7EmievmiYADVB20v/37Qog== -----END AGE ENCRYPTED FILE----- lastmodified: \"2026-02-03T10:00:15Z\" mac: ENC[AES256_GCM,data:5rbslKqxuIVbsilV3BFvlKw6BV2mUjGV0dmizTAXJgW5iC6n3I4ByGanc40lU/OjE02upvNo11NuoUtbDv7h1K+/XxFU7DlwMEJ7BGVdWU9Dq5fhf4eF2tcV5Q+Dw4JawUiMbBE25T3J0m+CMRx/Fb9KSRryYF7jb07l+QlTSrI=,iv:PasAhpZQI8h95vBu4szxxTyENYDgUimsYykS540svF8=,tag:/uHDwWrvqY6OYVjullgwhA==,type:str] unencrypted_suffix: _unencrypted version: 3.11.0 ❯ sops --decrypt secrets.yml db: username: super-secret password: ultra-secret As user Bob, try to decrypt in unauthorized state ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/bob.key ❯ sops --decrypt secrets.yml Failed to get the data key required to decrypt the SOPS file. Group 0: FAILED age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9: FAILED - | failed to create reader for decrypting sops data key with | age: no identity matched any of the recipients. Did not find | keys in locations 'SOPS_AGE_SSH_PRIVATE_KEY_FILE', | '/Users/damonguo/.ssh/id_ed25519', 'SOPS_AGE_KEY', and | 'SOPS_AGE_KEY_CMD'. Recovery failed because no master key was able to decrypt the file. In order for SOPS to recover the file, at least one key has to be successful, but none were. As user Alice, authorize Bob ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/alice.key ❯ vim .sops.yaml creation_rules: - path_regex: ^secrets\\.yml$ age: - age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 - age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m ❯ sops updatekeys secrets.yml 2026/02/03 18:16:58 Syncing keys for file /Users/damonguo/Workspace/demo/repo/sops-demo/secrets.yml The following changes will be made to the file's groups: Group 1 age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 +++ age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m Is this okay? (y/n):y 2026/02/03 18:17:10 File /Users/damonguo/Workspace/demo/repo/sops-demo/secrets.yml synced with new keys As user Bob, try to decrypt in authorized state ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/bob.key ❯ sops --decrypt secrets.yml db: username: super-secret password: ultra-secret ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:4:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"Remove authorization and verify The authorized user Bob can remove Alice’s key and add Jack’s authorization ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/bob.key ❯ vim .sops.yaml creation_rules: - path_regex: ^secrets\\.yml$ age: - age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m - age1h0ufdryerkpy39xkun9zd2hrece3g7nu9l63ws927cngwk633dcspuvgal ❯ sops updatekeys secrets.yml 2026/02/03 18:23:11 Syncing keys for file /Users/damonguo/Workspace/demo/repo/sops-demo/secrets.yml The following changes will be made to the file's groups: Group 1 age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m +++ age1h0ufdryerkpy39xkun9zd2hrece3g7nu9l63ws927cngwk633dcspuvgal --- age1lz4xs2z4rwcd9t4g3ek7vlj49x7uqzjnug3l8996tac40y4saatsw0ezx9 Is this okay? (y/n):y 2026/02/03 18:23:28 File /Users/damonguo/Workspace/demo/repo/sops-demo/secrets.yml synced with new keys User Alice cannot decrypt secrets.yml anymore ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/alice.key ❯ sops --decrypt secrets.yml Failed to get the data key required to decrypt the SOPS file. Group 0: FAILED age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m: FAILED - | failed to create reader for decrypting sops data key with | age: no identity matched any of the recipients. Did not find | keys in locations 'SOPS_AGE_SSH_PRIVATE_KEY_FILE', | '/Users/damonguo/.ssh/id_ed25519', 'SOPS_AGE_KEY', and | 'SOPS_AGE_KEY_CMD'. age1h0ufdryerkpy39xkun9zd2hrece3g7nu9l63ws927cngwk633dcspuvgal: FAILED - | failed to create reader for decrypting sops data key with | age: no identity matched any of the recipients. Did not find | keys in locations 'SOPS_AGE_SSH_PRIVATE_KEY_FILE', | '/Users/damonguo/.ssh/id_ed25519', 'SOPS_AGE_KEY', and | 'SOPS_AGE_KEY_CMD'. Recovery failed because no master key was able to decrypt the file. In order for SOPS to recover the file, at least one key has to be successful, but none were. Users Bob and Jack can decrypt secrets.yml ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/jack.key ❯ sops --decrypt secrets.yml db: username: super-secret password: ultra-secret ❯ export SOPS_AGE_KEY_FILE=/Users/damonguo/Workspace/demo/sops_keys/jack.key ❯ sops --decrypt secrets.yml db: username: super-secret password: ultra-secret View the contents of the encrypted secrets.yml file ❯ cat secrets.yml db: username: ENC[AES256_GCM,data:K4o7b88VYy1fiIRK,iv:WMPXoFF0aaxktEjaFxliYpsDgb77MVvbwviuwmYCRsk=,tag:qM3PzdjkyqYL2U+Ryhw/+Q==,type:str] password: ENC[AES256_GCM,data:SBIn28H5hlTNb0Xg,iv:BouoeNldrph5vph1ti2Y5Xny/qI5dQaFHCbsAFy/Ezo=,tag:X6SYXPgDU0R+WIVO0da+cA==,type:str] sops: age: - recipient: age1ckhckhz2jpzu574u83vcx88twfu2zqx9t42lf9623ysqx3h23c7s2gwj7m enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBhV04ydEp2SENrTk8xN01r MjFMZHREWDZydDc2dlM2V1FoUXEzMGFQY3hrCjdkeGVyNi9xRzBscllsU21jMHRh OCtMcXNRUFZXbHN2TXFtb0ZmQ0pXVkEKLS0tIHVGOSswWExyRThCWElVYi9Sa2hV cmM2NlRhRDAzYTcyRktPVU1abzBib3cKxevxN88yTfShd8+7TdrLYfY53krYOYtk rnJbrrH0+owJhyyiAFgzyphvzfbC1IwJA4gNR3OMc+R1vwngtfS+hg== -----END AGE ENCRYPTED FILE----- - recipient: age1h0ufdryerkpy39xkun9zd2hrece3g7nu9l63ws927cngwk633dcspuvgal enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSA2bGFjeXQwc2hvcGE5c3pW ejgreFEzRVdhS2hqKzJ4M1BLdFJPLzlDSXpjCldrTTZSNUxtdk5BbXJYTHg5UHNw RWxpQURuZ0pWVDFxUGhYZnhPdCs3K00KLS0tIG8ybXVqT3QwQW85MmdENllQZU0z aW5NSEZBUWwrZDZBaHBRTUlBRExhQ00KV8F3UnarwNmFdNRdmYfMjgOha5yqYzvM b7AdSIKiVpZ92WgGDS7RJ4ouKYYkBQEmsaMFZwjqG9xGRRBkrI8YlA== -----END AGE ENCRYPTED FILE----- lastmodified: \"2026-02-03T10:00:15Z\" mac: ENC[AES256_GCM,data:5rbslKqxuIVbsilV3BFvlKw6BV2mUjGV0dmizTAXJgW5iC6n3I4ByGanc40lU/OjE02upvNo11NuoUtbDv7h1K+/XxFU7DlwMEJ7BGVdWU9Dq5fhf4eF2tcV5Q+Dw4JawUiMbBE25T3J0m+CMRx/Fb9KSRryYF7jb07l+QlTSrI=,iv:PasAhpZQI8h95vBu4szxxTyENYDgUimsYykS540svF8=,tag:/uHDwWrvqY6OYVjullgwhA==,type:str] unencrypted_suffix: _unencrypted version: 3.11.0 ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:5:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"References https://blog.gitguardian.com/a-comprehensive-guide-to-sops/ ","date":"2026-02-03","objectID":"/en/2026/02/using-sops-age-to-encrypt-files/:6:0","tags":["Security"],"title":"Using SOPS + age to Encrypt Files","uri":"/en/2026/02/using-sops-age-to-encrypt-files/"},{"categories":["Skills"],"content":"uv is a Python package and project manager written in Rust and designed to replace tools such as pip and virtualenv.","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"uv is a Python package and project manager developed by Astral, written in Rust and designed to replace tools such as pip and virtualenv. uv provides performance 10 to 100 times faster than existing tools while maintaining compatibility with requirements.txt and pyproject.toml files. ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:0:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"Initialize Project Install uv ❯ brew install uv Initialize project (specify Python version, automatically generate .python-version and pyproject.toml) ❯ uv init myapp --python 3.12 Initialized project `myapp` at `/Users/damonguo/Workspace/demo/myapp` Enter project directory ❯ cd myapp Create virtual environment (automatically reads .python-version) ❯ uv venv Using CPython 3.12.12 interpreter at: /opt/homebrew/opt/python@3.12/bin/python3.12 Creating virtual environment at: .venv Activate with: source .venv/bin/activate ❯ cat pyproject.toml [project] name = \"myapp\" version = \"0.1.0\" description = \"Add your description here\" readme = \"README.md\" requires-python = \"\u003e=3.12\" dependencies = [] Install dependencies ❯ uv add fastapi uvicorn Resolved 16 packages in 5.06s Prepared 13 packages in 2.37s Installed 14 packages in 20ms + annotated-doc==0.0.3 + annotated-types==0.7.0 + anyio==4.11.0 + click==8.3.0 + fastapi==0.120.0 + h11==0.16.0 + idna==3.11 + pydantic==2.12.3 + pydantic-core==2.41.4 + sniffio==1.3.1 + starlette==0.48.0 + typing-extensions==4.15.0 + typing-inspection==0.4.2 + uvicorn==0.38.0 Write a minimal FastAPI example ❯ mkdir -p src/myapp ❯ cat \u003c\u003c'EOT' \u003e main.py from myapp.app import app if __name__ == \"__main__\": import uvicorn uvicorn.run(\"myapp.app:app\", host=\"0.0.0.0\", port=8000, reload=True) EOT ❯ cat \u003c\u003c'EOT' \u003e src/myapp/app.py from fastapi import FastAPI app = FastAPI() @app.get(\"/\") def hi(): return {\"ok\": True} EOT Run the minimal FastAPI example ❯ export PYTHONPATH=src ❯ uv run python main.py INFO: Will watch for changes in these directories: ['/Users/damonguo/Workspace/demo/myapp'] INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) INFO: Started reloader process [25820] using StatReload INFO: Started server process [25825] INFO: Waiting for application startup. INFO: Application startup complete. ❯ curl http://127.0.0.1:8000 {\"ok\":true} Export requirements.txt (optional) Not required for uv projects, as uv projects use pyproject.toml and uv.lock ❯ uv pip freeze \u003e requirements.txt Project file list (.venv is automatically added to .gitignore) ❯ ls -a1 . .. .git .gitignore .python-version .venv main.py pyproject.toml README.md requirements.txt src uv.lock Update uv.lock (supports dependency locking in production environment uv sync --locked) ❯ uv lock Resolved 16 packages in 1.05s ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:1:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"Load Project Remove .venv directory ❯ rm -r .venv Create virtual environment (automatically reads .python-version) With --locked parameter, install exact dependency versions (automatically reads pyproject.toml and uv.lock, does not check for the latest versions, and does not update pyproject.toml and uv.lock) ❯ uv sync --locked Using CPython 3.12.12 interpreter at: /opt/homebrew/opt/python@3.12/bin/python3.12 Creating virtual environment at: .venv Resolved 16 packages in 10ms Installed 14 packages in 18ms + annotated-doc==0.0.3 + annotated-types==0.7.0 + anyio==4.11.0 + click==8.3.0 + fastapi==0.120.0 + h11==0.16.0 + idna==3.11 + pydantic==2.12.3 + pydantic-core==2.41.4 + sniffio==1.3.1 + starlette==0.48.0 + typing-extensions==4.15.0 + typing-inspection==0.4.2 + uvicorn==0.38.0 ❯ uv tree Resolved 16 packages in 18ms myapp v0.1.0 ├── fastapi v0.120.0 │ ├── annotated-doc v0.0.3 │ ├── pydantic v2.12.3 │ │ ├── annotated-types v0.7.0 │ │ ├── pydantic-core v2.41.4 │ │ │ └── typing-extensions v4.15.0 │ │ ├── typing-extensions v4.15.0 │ │ └── typing-inspection v0.4.2 │ │ └── typing-extensions v4.15.0 │ ├── starlette v0.48.0 │ │ ├── anyio v4.11.0 │ │ │ ├── idna v3.11 │ │ │ ├── sniffio v1.3.1 │ │ │ └── typing-extensions v4.15.0 │ │ └── typing-extensions v4.15.0 │ └── typing-extensions v4.15.0 └── uvicorn v0.38.0 ├── click v8.3.0 └── h11 v0.16.0 ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:2:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"Migrate from pip to a uv project Remove uv related directories and files ❯ rm -r .venv ❯ rm .python-version ❯ rm pyproject.toml ❯ rm uv.lock ❯ ls -a1 . .. .git .gitignore main.py README.md requirements.txt src Initialize a project (specify Python version, automatically generate .python-version and pyproject.toml) ❯ uv init --python 3.12 Initialized project `myapp` Parse the existing requirements.txt Install exact dependency versions (automatically generate uv.lock) ❯ uv add -r requirements.txt Using CPython 3.12.12 interpreter at: /opt/homebrew/opt/python@3.12/bin/python3.12 Creating virtual environment at: .venv Resolved 16 packages in 2.46s Installed 14 packages in 29ms + annotated-doc==0.0.3 + annotated-types==0.7.0 + anyio==4.11.0 + click==8.3.0 + fastapi==0.120.0 + h11==0.16.0 + idna==3.11 + pydantic==2.12.3 + pydantic-core==2.41.4 + sniffio==1.3.1 + starlette==0.48.0 + typing-extensions==4.15.0 + typing-inspection==0.4.2 + uvicorn==0.38.0 ❯ ls -a1 . .. .git .gitignore .python-version .venv main.py pyproject.toml README.md requirements.txt src uv.lock Update uv.lock (supports dependency locking in production environment uv sync --locked) ❯ uv lock Resolved 16 packages in 1.05s ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:3:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"Dockerfile in production Create .dockerignore ❯ cat \u003c\u003c'EOT' \u003e .dockerignore # macOS .DS_Store # Git .git/ # Python __pycache__/ *.py[cod] *$py.class EOT Create Dockerfile ❯ cat \u003c\u003c'EOT' \u003e Dockerfile # use uv to build dependencies FROM ghcr.io/astral-sh/uv:python3.12-alpine AS builder # set working directory WORKDIR /app # install dependencies using uv (only lockfile deps first) RUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --locked --no-install-project --no-editable # copy source code ADD . /app # install project itself into .venv RUN --mount=type=cache,target=/root/.cache/uv \\ uv sync --locked --no-editable # final runtime image FROM python:3.12-alpine # set working directory WORKDIR /app # create non-root user RUN addgroup -S app \u0026\u0026 adduser -S app -G app # copy project and virtual environment from builder COPY --from=builder --chown=app:app /app /app # add /app/src to Python module search path ENV PYTHONPATH=/app/src # exposed port EXPOSE 8000 # run as non-root user USER app # start uvicorn app (expand WORKERS_NUM env) CMD [\"/bin/sh\", \"-c\", \"/app/.venv/bin/uvicorn myapp.app:app --host 0.0.0.0 --port 8000 --workers ${WORKERS_NUM:-1}\"] EOT Build Docker image ❯ docker build -f ./Dockerfile --platform linux/amd64 -t myapp:2025.10.27_1 . [+] Building 5.3s (15/15) FINISHED docker:orbstack =\u003e [internal] load build definition from Dockerfile 0.0s =\u003e =\u003e transferring dockerfile: 1.15kB 0.0s =\u003e [internal] load metadata for docker.io/library/python:3.12-alpine 0.3s =\u003e [internal] load metadata for ghcr.io/astral-sh/uv:python3.12-alpine 1.6s =\u003e [internal] load .dockerignore 0.0s =\u003e =\u003e transferring context: 115B 0.0s =\u003e [builder 1/5] FROM ghcr.io/astral-sh/uv:python3.12-alpine 0.0s =\u003e [internal] load build context 0.1s =\u003e =\u003e transferring context: 94.47kB 0.1s =\u003e [stage-1 1/4] FROM docker.io/library/python:3.12-alpine 0.0s =\u003e CACHED [builder 2/5] WORKDIR /app 0.0s =\u003e CACHED [stage-1 2/4] WORKDIR /app 0.0s =\u003e CACHED [stage-1 3/4] RUN addgroup -S app \u0026\u0026 adduser -S app -G app 0.0s =\u003e [builder 3/5] RUN --mount=type=cache,target=/root/.cache/uv \\ --mount=type=bind,source=uv.lock,target=uv.lock \\ --mount=type=bind,source=pyproject.toml,target=pyproject.toml \\ uv sync --locked --no-install-project --no-editable 2.3s =\u003e [builder 4/5] ADD . /app 0.2s =\u003e [builder 5/5] RUN --mount=type=cache,target=/root/.cache/uv \\ uv sync --locked --no-editable 0.8s =\u003e [stage-1 4/4] COPY --from=builder --chown=app:app /app /app 0.1s =\u003e exporting to image 0.1s =\u003e =\u003e exporting layers 0.0s =\u003e =\u003e writing image 0.0s =\u003e =\u003e naming to docker.io/library/myapp:2025.10.27_1 0.0s Run Docker container via docker-compose ❯ cat \u003c\u003c'EOT' \u003e docker-compose.yml services: myapp: image: myapp:2025.10.27_1 platform: linux/amd64 ports: - \"8000:8000\" environment: - WORKERS_NUM=2 EOT ❯ docker-compose up -d [+] Running 2/2 ✔ Network myapp_default Created 0.0s ✔ Container myapp-myapp-1 Started 0.3s ❯ docker-compose ps NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS myapp-myapp-1 myapp:2025.10.27_1 \"/bin/sh -c '/app/.v…\" myapp 5 minutes ago Up 5 minutes 0.0.0.0:8000-\u003e8000/tcp ❯ curl http://127.0.0.1:8000 {\"ok\":true} ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:4:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"References https://docs.astral.sh/uv/guides/projects/ https://docs.astral.sh/uv/guides/migration/pip-to-project/ https://docs.astral.sh/uv/guides/integration/docker/#non-editable-installs ","date":"2025-10-27","objectID":"/en/2025/10/using-uv-to-create-and-manage-python-project/:5:0","tags":["Python"],"title":"Using uv to create and manage Python project","uri":"/en/2025/10/using-uv-to-create-and-manage-python-project/"},{"categories":["Skills"],"content":"Set up a minimal CDC (Change Data Capture) and Unified Batch and Streaming processing using Docker, MySQL, and Flink SQL.","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Set up a minimal CDC (Change Data Capture) and Unified Batch and Streaming processing using Docker, MySQL, and Flink SQL. This article demonstrates how to quickly set up a minimal CDC (Change Data Capture) and Unified Batch and Streaming processing using Docker, MySQL, and Flink SQL, with the following features: The entire process requires no Java/Scala code, only Flink SQL Flink CDC captures MySQL data changes Flink streaming processing writes to Kafka + Paimon (Lakehouse Storage Engine) Flink batch processing writes statistics to CSV Visualization tools: Adminer (MySQL) and Kafdrop (Kafka) It is designed to help operations engineers and data analysts quickly get hands-on experience with Flink Data Source → CDC Capture → Batch-Stream Analysis → Downstream Results. ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:0:0","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Setup Services ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:0","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Structure . ├── flink │ ├── conf │ │ └── sql-client-defaults.yaml │ └── lib │ ├── flink-connector-jdbc-3.2.0-1.19.jar │ ├── flink-shaded-hadoop-2-uber-2.8.3-10.0.jar │ ├── flink-sql-connector-kafka-3.2.0-1.19.jar │ ├── flink-sql-connector-mysql-cdc-3.2.0.jar │ ├── mysql-connector-j-8.4.0.jar │ ├── paimon-flink-1.19-1.2.0.jar │ └── paimon-format-1.2.0.jar ├── output │ └── checkpoints ├── sql │ ├── 01_kafka_cdc.sql │ ├── 02_paimon_cdc.sql │ ├── 03_topn_batch.sql │ └── 04_paimon_read.sql ├── docker-compose.yml └── mysql-init.sql ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:1","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Download JARs mkdir flink-amd64 cd flink-amd64 mkdir -p output/checkpoints mkdir -p flink/lib cd flink/lib wget https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.2.0-1.19/flink-connector-jdbc-3.2.0-1.19.jar wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/3.2.0-1.19/flink-sql-connector-kafka-3.2.0-1.19.jar wget https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-mysql-cdc/3.2.0/flink-sql-connector-mysql-cdc-3.2.0.jar wget https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.4.0/mysql-connector-j-8.4.0.jar wget https://repo1.maven.org/maven2/org/apache/paimon/paimon-flink-1.19/1.2.0/paimon-flink-1.19-1.2.0.jar wget https://repo1.maven.org/maven2/org/apache/paimon/paimon-format/1.2.0/paimon-format-1.2.0.jar wget https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.8.3-10.0/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:2","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create docker-compose.yml cd ../../ vim docker-compose.yml services: mysql: image: mysql:8.0 platform: linux/amd64 ports: - \"3306:3306\" environment: - MYSQL_ROOT_PASSWORD=rootpw - MYSQL_DATABASE=sales command: \u003e --server-id=857 --log-bin=binlog --binlog_format=ROW --binlog_row_image=FULL --gtid_mode=ON --enforce-gtid-consistency=ON --binlog_expire_logs_seconds=600 volumes: - ./mysql-init.sql:/docker-entrypoint-initdb.d/mysql-init.sql kafka: image: bitnami/kafka:3.7 platform: linux/amd64 ports: - \"9092:9092\" environment: - KAFKA_ENABLE_KRAFT=yes - KAFKA_CFG_NODE_ID=1 - KAFKA_CFG_PROCESS_ROLES=broker,controller - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093 - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true - ALLOW_PLAINTEXT_LISTENER=yes kafdrop: image: obsidiandynamics/kafdrop:4.2.0 platform: linux/amd64 ports: - \"9000:9000\" environment: - KAFKA_BROKERCONNECT=kafka:9092 depends_on: - kafka adminer: image: adminer:5.4.0 platform: linux/amd64 ports: - \"8080:8080\" jobmanager: image: flink:1.19.1-scala_2.12-java11 platform: linux/amd64 command: jobmanager environment: - JOB_MANAGER_RPC_ADDRESS=jobmanager ports: - \"8081:8081\" volumes: - ./flink/conf/sql-client-defaults.yaml:/opt/flink/conf/sql-client-defaults.yaml:ro - ./flink/lib/flink-sql-connector-mysql-cdc-3.2.0.jar:/opt/flink/lib/flink-sql-connector-mysql-cdc-3.2.0.jar:ro - ./flink/lib/flink-sql-connector-kafka-3.2.0-1.19.jar:/opt/flink/lib/flink-sql-connector-kafka-3.2.0-1.19.jar:ro - ./flink/lib/flink-connector-jdbc-3.2.0-1.19.jar:/opt/flink/lib/flink-connector-jdbc-3.2.0-1.19.jar:ro - ./flink/lib/mysql-connector-j-8.4.0.jar:/opt/flink/lib/mysql-connector-j-8.4.0.jar:ro - ./flink/lib/paimon-flink-1.19-1.2.0.jar:/opt/flink/lib/paimon-flink-1.19-1.2.0.jar:ro - ./flink/lib/paimon-format-1.2.0.jar:/opt/flink/lib/paimon-format-1.2.0.jar:ro # self-contained and isolated set of Hadoop client libraries - ./flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:ro - ./sql:/opt/sql:ro - ./output:/opt/flink/output taskmanager: image: flink:1.19.1-scala_2.12-java11 platform: linux/amd64 command: taskmanager environment: - JOB_MANAGER_RPC_ADDRESS=jobmanager depends_on: - jobmanager volumes: - ./flink/conf/sql-client-defaults.yaml:/opt/flink/conf/sql-client-defaults.yaml:ro - ./flink/lib/flink-sql-connector-mysql-cdc-3.2.0.jar:/opt/flink/lib/flink-sql-connector-mysql-cdc-3.2.0.jar:ro - ./flink/lib/flink-sql-connector-kafka-3.2.0-1.19.jar:/opt/flink/lib/flink-sql-connector-kafka-3.2.0-1.19.jar:ro - ./flink/lib/flink-connector-jdbc-3.2.0-1.19.jar:/opt/flink/lib/flink-connector-jdbc-3.2.0-1.19.jar:ro - ./flink/lib/mysql-connector-j-8.4.0.jar:/opt/flink/lib/mysql-connector-j-8.4.0.jar:ro - ./flink/lib/paimon-flink-1.19-1.2.0.jar:/opt/flink/lib/paimon-flink-1.19-1.2.0.jar:ro - ./flink/lib/paimon-format-1.2.0.jar:/opt/flink/lib/paimon-format-1.2.0.jar:ro # self-contained and isolated set of Hadoop client libraries - ./flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:ro - ./sql:/opt/sql:ro - ./output:/opt/flink/output ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:3","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create mysql-init.sql vim mysql-init.sql CREATE DATABASE IF NOT EXISTS sales; USE sales; CREATE TABLE orders ( order_id INT PRIMARY KEY, customer_id INT, region VARCHAR(10), amount DOUBLE, status VARCHAR(10), order_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); INSERT INTO orders VALUES (1001, 1, 'US', 20.5, 'NEW', NOW()), (1002, 2, 'EU', 35.2, 'NEW', NOW()), (1003, 3, 'CN', 66.6, 'NEW', NOW()), (1004, 4, 'UK', 38.9, 'NEW', NOW()), (1005, 5, 'AU', 25.3, 'NEW', NOW()), (1006, 6, 'JP', 33.8, 'NEW', NOW()); CREATE USER 'flink'@'%' IDENTIFIED BY 'flinkpw'; GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'flink'@'%'; FLUSH PRIVILEGES; ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:4","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create Flink Configuration mkdir -p flink/conf vim flink/conf/sql-client-defaults.yaml execution: type: streaming result-mode: table parallelism: 1 configuration: execution.checkpointing.interval: 5 s execution.checkpointing.mode: EXACTLY_ONCE execution.checkpointing.min-pause: 1 s execution.checkpointing.timeout: 5 min state.checkpoints.dir: file:///opt/flink/output/checkpoints restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 10 restart-strategy.fixed-delay.delay: 5 s table.exec.sink.not-null-enforcer: drop table.exec.sink.upsert-materialize: none table.exec.source.idle-timeout: 5 s deployment: response-timeout: 10000 ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:5","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create 01_kafka_cdc.sql mkdir -p sql vim sql/01_kafka_cdc.sql -- 01_kafka_cdc.sql CREATE TABLE orders_cdc ( order_id INT, customer_id INT, region STRING, amount DOUBLE, status STRING, order_time TIMESTAMP(3), PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'hostname' = 'mysql', 'port' = '3306', 'username' = 'flink', 'password' = 'flinkpw', 'database-name' = 'sales', 'table-name' = 'orders', 'server-id' = '985', 'scan.startup.mode' = 'initial' ); CREATE TABLE orders_kafka ( order_id INT, customer_id INT, region STRING, amount DOUBLE, status STRING, order_time TIMESTAMP(3), PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'upsert-kafka', 'topic' = 'orders_topic', 'properties.bootstrap.servers' = 'kafka:9092', 'key.format' = 'json', 'value.format' = 'json' ); INSERT INTO orders_kafka SELECT * FROM orders_cdc; ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:6","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create 02_paimon_cdc.sql vim sql/02_paimon_cdc.sql -- 02_paimon_cdc.sql SET 'execution.runtime-mode' = 'streaming'; SET 'execution.checkpointing.interval' = '5 s'; SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE'; SET 'execution.checkpointing.min-pause' = '1 s'; SET 'execution.checkpointing.timeout' = '5 min'; CREATE TABLE orders_cdc ( order_id INT, customer_id INT, region STRING, amount DOUBLE, status STRING, order_time TIMESTAMP(3), PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'hostname' = 'mysql', 'port' = '3306', 'username' = 'flink', 'password' = 'flinkpw', 'database-name' = 'sales', 'table-name' = 'orders', 'server-id' = '996', 'scan.startup.mode' = 'initial' ); CREATE CATALOG paimon_catalog WITH ( 'type' = 'paimon', 'warehouse' = 'file:///opt/flink/output/warehouse' ); USE CATALOG paimon_catalog; CREATE DATABASE IF NOT EXISTS dwd; USE dwd; CREATE TABLE IF NOT EXISTS orders_paimon ( order_id INT, customer_id INT, region STRING, amount DOUBLE, status STRING, order_time TIMESTAMP(3), PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'paimon', 'changelog-producer' = 'input', 'bucket' = '1' ); USE CATALOG default_catalog; INSERT INTO paimon_catalog.dwd.orders_paimon SELECT * FROM orders_cdc; ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:7","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create 03_topn_batch.sql vim sql/03_topn_batch.sql -- 03_topn_batch.sql SET 'execution.runtime-mode' = 'batch'; SET 'sql-client.execution.result-mode' = 'TABLEAU'; SET 'table.dml-sync' = 'true'; CREATE TABLE orders_jdbc ( order_id INT, customer_id INT, region STRING, amount DOUBLE, status STRING, order_time TIMESTAMP(3), PRIMARY KEY (order_id) NOT ENFORCED ) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://mysql:3306/sales', 'table-name' = 'orders', 'username' = 'flink', 'password' = 'flinkpw' ); CREATE TABLE top_customers ( customer_id INT, total_amount DOUBLE ) WITH ( 'connector' = 'filesystem', 'path' = '/opt/flink/output/top_customers', 'format' = 'csv' ); INSERT OVERWRITE top_customers SELECT customer_id, CAST(SUM(amount) AS DOUBLE) AS total_amount FROM orders_jdbc GROUP BY customer_id ORDER BY total_amount DESC LIMIT 5; SELECT * FROM top_customers; ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:8","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Create 04_paimon_read.sql vim sql/04_paimon_read.sql -- 04_paimon_read.sql SET 'sql-client.execution.result-mode' = 'TABLEAU'; SET 'execution.runtime-mode' = 'batch'; SET 'table.dml-sync' = 'true'; CREATE CATALOG paimon_catalog WITH ( 'type' = 'paimon', 'warehouse' = 'file:///opt/flink/output/warehouse' ); USE CATALOG paimon_catalog; USE dwd; SELECT COUNT(*) AS cnt FROM orders_paimon; SELECT * FROM orders_paimon; ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:1:9","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Launch Batch and Streaming Jobs ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:0","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Run docker-compose docker-compose pull docker-compose up -d --scale taskmanager=4 [+] Running 7/7 ✔ Network flink-amd64_default Created ✔ Container flink-amd64-jobmanager-1 Started ✔ Container flink-amd64-kafka-1 Started ✔ Container flink-amd64-adminer-1 Started ✔ Container flink-amd64-mysql-1 Started ✔ Container flink-amd64-kafdrop-1 Started ✔ Container flink-amd64-taskmanager-1 Started ✔ Container flink-amd64-taskmanager-2 Started ✔ Container flink-amd64-taskmanager-3 Started ✔ Container flink-amd64-taskmanager-4 Started docker-compose ps --format \"table {{.Name}}\\t{{.Service}}\\t{{.Status}}\" NAME SERVICE STATUS flink-amd64-adminer-1 adminer Up 3 minutes flink-amd64-jobmanager-1 jobmanager Up 3 minutes flink-amd64-kafdrop-1 kafdrop Up 3 minutes flink-amd64-kafka-1 kafka Up 3 minutes flink-amd64-mysql-1 mysql Up 3 minutes flink-amd64-taskmanager-1 taskmanager Up 3 minutes flink-amd64-taskmanager-2 taskmanager Up 3 minutes flink-amd64-taskmanager-3 taskmanager Up 3 minutes flink-amd64-taskmanager-4 taskmanager Up 3 minutes ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:1","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Run 01_kafka_cdc.sql docker-compose exec -T jobmanager /opt/flink/bin/sql-client.sh -f /opt/sql/01_kafka_cdc.sql ... Flink SQL\u003e [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 7537135a66b3ead1213fcdbc334fa192 Flink SQL\u003e Shutting down the session... done. ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:2","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Run 02_paimon_cdc.sql docker-compose exec -T jobmanager /opt/flink/bin/sql-client.sh -f /opt/sql/02_paimon_cdc.sql ... Flink SQL\u003e [INFO] Execute statement succeed. Flink SQL\u003e [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 852a7286763bdf109cdf36aa1b9c3627 Flink SQL\u003e Shutting down the session... done. ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:3","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Verify Streaming CDC Insert a new data docker-compose exec -T mysql mysql -uroot -prootpw -e \\ \"USE sales; INSERT INTO orders VALUES (3001, 31, 'US', 12.3, 'PAID', NOW());\" Check Flink jobs Visit http://localhost:8081 Check Flink running jobs Check MySQL table Visit http://localhost:8080 Login as: Server: mysql Username: root Password: rootpw Database: sales Check the table sales.orders Check Kafka events Visit http://localhost:9000 Check the topic orders_topic ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:4","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Verify Batch Job docker-compose exec -T jobmanager /opt/flink/bin/sql-client.sh -f /opt/sql/03_topn_batch.sql ... Flink SQL\u003e \u003e INSERT OVERWRITE top_customers \u003e SELECT customer_id, \u003e CAST(SUM(amount) AS DOUBLE) [INFO] Complete execution of the SQL update statement. Flink SQL\u003e +-------------+--------------+ | customer_id | total_amount | +-------------+--------------+ | 3 | 66.6 | | 4 | 38.9 | | 2 | 35.2 | | 6 | 33.8 | | 5 | 25.3 | +-------------+--------------+ 5 rows in set (0.94 seconds) Flink SQL\u003e Shutting down the session... done. ","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:5","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Verify Paimon Table docker-compose exec -T mysql mysql -uroot -prootpw -e \" INSERT INTO sales.orders (order_id, customer_id, region, amount, status, order_time) VALUES (6001, 61, 'US', 18.5, 'NEW', NOW()), (6002, 62, 'EU', 27.0, 'PAID', NOW()), (6003, 63, 'APAC',33.3, 'NEW', NOW()); UPDATE sales.orders SET amount = amount + 5, status='PAID' WHERE order_id IN (6001, 6002); DELETE FROM sales.orders WHERE order_id = 6003; \" docker-compose exec -T jobmanager /opt/flink/bin/sql-client.sh -f /opt/sql/04_paimon_read.sql ... Flink SQL\u003e [INFO] Execute statement succeed. Flink SQL\u003e \u003e SELECT COUNT(*) +-----+ | cnt | +-----+ | 9 | +-----+ 1 row in set (7.52 seconds) Flink SQL\u003e +----------+-------------+--------+--------+--------+-------------------------+ | order_id | customer_id | region | amount | status | order_time | +----------+-------------+--------+--------+--------+-------------------------+ | 1001 | 1 | US | 20.5 | NEW | 2025-09-23 02:23:53.000 | | 1002 | 2 | EU | 35.2 | NEW | 2025-09-23 02:23:53.000 | | 1003 | 3 | CN | 66.6 | NEW | 2025-09-23 02:23:53.000 | | 1004 | 4 | UK | 38.9 | NEW | 2025-09-23 02:23:53.000 | | 1005 | 5 | AU | 25.3 | NEW | 2025-09-23 02:23:53.000 | | 1006 | 6 | JP | 33.8 | NEW | 2025-09-23 02:23:53.000 | | 3001 | 31 | US | 12.3 | PAID | 2025-09-23 02:27:03.000 | | 6001 | 61 | US | 23.5 | PAID | 2025-09-23 02:30:49.000 | | 6002 | 62 | EU | 32.0 | PAID | 2025-09-23 02:30:49.000 | +----------+-------------+--------+--------+--------+-------------------------+ 9 rows in set (0.92 seconds) Flink SQL\u003e Shutting down the session... done. Check Flink finished jobs Check Output artifacts tree output output ├── checkpoints ├── top_customers │ └── part-b36f808b-b393-4359-bfb0-05eaf732ee2f-task-0-file-0 └── warehouse ├── default.db └── dwd.db └── orders_paimon ├── bucket-0 │ ├── changelog-1e5892ad-7d3b-4dbb-8801-deb523e8bb9c-0.parquet │ ├── changelog-49f7184f-cbd9-4ecb-bc93-6479fe5dd88d-0.parquet │ ├── changelog-7a66b3ac-64bf-454a-8ca8-3185e281d5d0-0.parquet │ ├── data-1e5892ad-7d3b-4dbb-8801-deb523e8bb9c-1.parquet │ ├── data-49f7184f-cbd9-4ecb-bc93-6479fe5dd88d-1.parquet │ └── data-7a66b3ac-64bf-454a-8ca8-3185e281d5d0-1.parquet ├── manifest │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-0 │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-1 │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-2 │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-3 │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-4 │ ├── manifest-18b7fa7b-7b72-48cf-93e9-3bc50d57dd3b-5 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-0 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-1 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-10 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-11 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-12 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-13 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-14 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-15 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-16 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-17 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-18 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-19 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-2 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-20 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-3 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-4 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-5 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-6 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-7 │ ├── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-8 │ └── manifest-list-770c3f24-9fbe-48b4-b29f-c3eabe5c8d35-9 ├── schema │ └── schema-0 └── snapshot ├── EARLIEST ├── LATEST ├── snapshot-1 ├── snapshot-2 ├── snapshot-3 ├── snapshot-4 ├── snapshot-5 ├── snapshot-6 ├── snapshot-7 ├── snapshot-8 └── snapshot-9","date":"2025-09-22","objectID":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/:2:6","tags":["Flink","CDC"],"title":"Quickstart Flink SQL Unified Batch and Streaming","uri":"/en/2025/09/quickstart-flink-sql-unified-batch-and-streaming/"},{"categories":["Skills"],"content":"Set up a minimal Debezium MySQL Kafka CDC stack using Docker, KRaft, and Kafdrop.","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Skills"],"content":"Set up a minimal Debezium MySQL Kafka CDC (Change Data Capture) stack using Docker, KRaft, and Kafdrop. ","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/:0:0","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Skills"],"content":"Setup Base Services Create docker-compose files mkdir debezium-amd64 cd debezium-amd64 vim docker-compose.yml services: kafka: image: bitnami/kafka:3.7 platform: linux/amd64 ports: - \"9092:9092\" environment: # enable KRaft (no Zookeeper) - KAFKA_ENABLE_KRAFT=yes - KAFKA_CFG_NODE_ID=1 - KAFKA_CFG_PROCESS_ROLES=broker,controller - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093 - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true - ALLOW_PLAINTEXT_LISTENER=yes connect: image: quay.io/debezium/connect:3.2 platform: linux/amd64 ports: - \"8083:8083\" environment: - BOOTSTRAP_SERVERS=kafka:9092 - GROUP_ID=1 - CONFIG_STORAGE_TOPIC=connect_configs - OFFSET_STORAGE_TOPIC=connect_offsets - STATUS_STORAGE_TOPIC=connect_statuses - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter - VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter - KEY_CONVERTER_SCHEMAS_ENABLE=false - VALUE_CONVERTER_SCHEMAS_ENABLE=false depends_on: - kafka kafdrop: image: obsidiandynamics/kafdrop:4.2.0 platform: linux/amd64 ports: - \"9000:9000\" environment: - KAFKA_BROKERCONNECT=kafka:9092 depends_on: - kafka mysql: image: mysql:8.0 platform: linux/amd64 ports: - \"3306:3306\" environment: - MYSQL_ROOT_PASSWORD=debezium - MYSQL_USER=debezium - MYSQL_PASSWORD=dbz - MYSQL_DATABASE=inventory command: \u003e --server-id=857 --log-bin=binlog --binlog_format=ROW --binlog_row_image=FULL --gtid_mode=ON --enforce-gtid-consistency=ON --binlog_expire_logs_seconds=600 volumes: - ./mysql-init.sql:/docker-entrypoint-initdb.d/mysql-init.sql vim mysql-init.sql -- ensure the database exists and is selected CREATE DATABASE IF NOT EXISTS inventory; USE inventory; -- minimal demo table (keep it simple for CDC) CREATE TABLE IF NOT EXISTS customers ( id INT PRIMARY KEY AUTO_INCREMENT, first_name VARCHAR(50), last_name VARCHAR(50), email VARCHAR(100), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); -- seed data for snapshot INSERT INTO customers (first_name, last_name, email) VALUES ('Alice', 'Smith', 'alice@example.com'), ('Bob', 'Johnson', 'bob@example.com'); vim register-mysql.json { \"name\": \"mysql-inventory-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"857\", \"topic.prefix\": \"mysql_server\", \"database.include.list\": \"inventory\", \"table.include.list\": \"inventory.customers\", \"snapshot.mode\": \"initial\", \"snapshot.locking.mode\": \"none\", \"include.schema.changes\": \"false\", \"tombstones.on.delete\": \"false\", \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\", \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\" } } ls -1 docker-compose.yml mysql-init.sql register-mysql.json Start services docker-compose up -d [+] Running 5/5 ✔ Network debezium-amd64_default Created ✔ Container debezium-amd64-mysql-1 Started ✔ Container debezium-amd64-kafka-1 Started ✔ Container debezium-amd64-kafdrop-1 Started ✔ Container debezium-amd64-connect-1 Started ","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/:1:0","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Skills"],"content":"Configure Source Connector Grant MySQL access to Source Connector docker-compose exec -T mysql mysql -uroot -pdebezium -e \" GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT, LOCK TABLES ON *.* TO 'debezium'@'%'; FLUSH PRIVILEGES;\" Register Source Connector curl -s -X POST http://localhost:8083/connectors \\ -H \"Content-Type: application/json\" \\ -d @register-mysql.json Update Source Connector config (optional) jq '.config' register-mysql.json | \\ curl -s -X PUT http://localhost:8083/connectors/mysql-inventory-connector/config \\ -H \"Content-Type: application/json\" \\ -d @- | jq . Check Source Connector status curl -s localhost:8083/connectors/mysql-inventory-connector/status | jq . { \"name\": \"mysql-inventory-connector\", \"connector\": { \"state\": \"RUNNING\", \"worker_id\": \"192.168.97.5:8083\" }, \"tasks\": [ { \"id\": 0, \"state\": \"RUNNING\", \"worker_id\": \"192.168.97.5:8083\" } ], \"type\": \"source\" } Trigger CDC events INSERT / UPDATE / DELETE some rows to see c / u / d events docker-compose exec -T mysql mysql -udebezium -pdbz -e \"USE inventory; INSERT INTO customers (first_name,last_name,email) VALUES ('Charlie','Wang','charlie@example.com'); UPDATE customers SET email='alice_new@example.com' WHERE first_name='Alice'; DELETE FROM customers WHERE first_name='Bob';\" Check the topic on Kafdrop Visit http://localhost:9000 Check the topic mysql_server.inventory.customers See new events \"op\":\"c\" / \"op\":\"u\" / \"op\":\"d\" ","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/:2:0","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Skills"],"content":"Configure Sink Connector Grant MySQL access to Sink Connector docker-compose exec -T mysql mysql -uroot -pdebezium -e \" CREATE USER IF NOT EXISTS 'sink'@'%' IDENTIFIED BY 'sinkpw'; GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, ALTER ON inventory.* TO 'sink'@'%'; FLUSH PRIVILEGES;\" vim register-jdbc-sink.json { \"name\": \"jdbc-sink-mysql\", \"config\": { \"connector.class\": \"io.debezium.connector.jdbc.JdbcSinkConnector\", \"tasks.max\": \"1\", \"topics\": \"mysql_server.inventory.customers\", \"connection.url\": \"jdbc:mysql://mysql:3306/inventory\", \"connection.username\": \"sink\", \"connection.password\": \"sinkpw\", \"insert.mode\": \"upsert\", \"delete.enabled\": \"true\", \"primary.key.mode\": \"record_key\", \"primary.key.fields\": \"id\", \"schema.evolution\": \"basic\", \"collection.name.format\": \"customers_mirror\", \"consumer.override.auto.offset.reset\": \"earliest\" } } Register Sink Connector curl -s -X POST http://localhost:8083/connectors \\ -H \"Content-Type: application/json\" \\ -d @register-jdbc-sink.json Update Sink Connector (optional) jq '.config' register-jdbc-sink.json | \\ curl -s -X PUT http://localhost:8083/connectors/mysql-inventory-connector/config \\ -H \"Content-Type: application/json\" \\ -d @- | jq . Check Sink Connector status curl -s http://localhost:8083/connectors/jdbc-sink-mysql/status | jq . { \"name\": \"jdbc-sink-mysql\", \"connector\": { \"state\": \"RUNNING\", \"worker_id\": \"192.168.97.5:8083\" }, \"tasks\": [ { \"id\": 0, \"state\": \"RUNNING\", \"worker_id\": \"192.168.97.5:8083\" } ], \"type\": \"sink\" } ","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/:3:0","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Skills"],"content":"Verify End-to-End Data Sync Check source table docker-compose exec -T mysql mysql -usink -psinkpw -e \"SELECT * FROM inventory.customers;\" id first_name last_name email created_at 1 Alice Smith alice_new@example.com 2025-09-19 01:50:16 3 Charlie Wang charlie@example.com 2025-09-19 01:58:59 Check mirror table docker-compose exec -T mysql mysql -usink -psinkpw -e \"SELECT * FROM inventory.customers_mirror;\" id first_name last_name email created_at 1 Alice Smith alice_new@example.com 2025-09-19 01:50:16 3 Charlie Wang charlie@example.com 2025-09-19 01:58:59 ","date":"2025-09-18","objectID":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/:4:0","tags":["Debezium","CDC"],"title":"Quickstart Debezium MySQL Kafka CDC","uri":"/en/2025/09/quickstart-debezium-mysql-kafka-cdc/"},{"categories":["Notes"],"content":"In a crowd, the individual loses conscious personality and rational judgment, becoming dominated by emotion, suggestion, and the collective unconscious, which leads to impulsive, credulous, and irrational behavior.","date":"2025-09-11","objectID":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/","tags":["Psychology"],"title":"Notes from \"The Crowd: A Study of the Popular Mind\"","uri":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/"},{"categories":["Notes"],"content":"In a crowd, the individual loses conscious personality and rational judgment, becoming dominated by emotion, suggestion, and the collective unconscious, which leads to impulsive, credulous, and irrational behavior. ","date":"2025-09-11","objectID":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/:0:0","tags":["Psychology"],"title":"Notes from \"The Crowd: A Study of the Popular Mind\"","uri":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/"},{"categories":["Notes"],"content":"Preface “The Crowd: A Study of the Popular Mind” is a work of social psychology by the French scholar Gustave Le Bon, first published in 1895. Le Bon wrote in the decades following the French Revolution: from the storming of the Bastille to the Thermidorian Reaction, France had entered an era of consolidating the Revolution’s legacy, in which the voice of the multitude gained ascendancy and ordinary people entered political life. A century and more later, the background he depicts still resonates - and in some respects has only grown more salient. The term “crowd” here conveys the image of people gathered like a flock of crows - an assemblage without order or discipline, easily swayed and lacking independent thought. Le Bon’s main argument is that in a crowd, the individual loses conscious personality and rational judgment, becoming dominated by emotion, suggestion, and the collective unconscious, which leads to impulsive, credulous, and irrational behavior. ","date":"2025-09-11","objectID":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/:1:0","tags":["Psychology"],"title":"Notes from \"The Crowd: A Study of the Popular Mind\"","uri":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/"},{"categories":["Notes"],"content":"Summary Through careful observation of crowd behavior, Le Bon argues that the psychology of crowds differs fundamentally from that of individuals. Basic traits of crowd psychology Once immersed in a crowd, the individual undergoes a profound psychological change. Rational deliberation gives way to the collective mind’s unconscious; independent judgment rapidly declines, and distinct crowd traits emerge: emotionality, suggestibility, extremity, and simplification. The dual nature of crowd conduct Crowds can be brutal, impulsive, and destructive - easily stirred and manipulated, acting against private interest and reason; They can also display heroism, self-sacrifice, and exalted moral sentiment that surpass the individual. This duality makes crowds capable both of advancing civilization and of unsettling it. How crowd opinions arise and spread Indirect factors - race, traditions, time, institutions, and education - form the deep substratum of the crowd mind; Direct factors - vivid words and images, illusions, experience, and feeling - move crowds far more than reason. Crowd leaders shape opinion chiefly by affirmation, repetition, and contagion, using their prestige to imprint beliefs. Types of crowds and concrete arenas Criminal crowds, juries, electorates, and parliamentary assemblies all display common crowd traits, though each has its own distinctive manifestations. Historical import and present cautions Writing in the late nineteenth century, as Europe was becoming more democratic, Le Bon questioned the power of crowds and favored elite governance - a view shared by many intellectuals of his era. In our current age of information overload and social media, his observations are strikingly relevant: polarization, echo chambers, and online mob behavior can all be understood through his framework. Value and limits of the theory What makes this book valuable is how it provides a basic framework for understanding crowd behavior and reveals the psychological mechanisms behind crowd phenomena. However, we should also recognize its historical limitations: Le Bon presents theories rather than systematic, quantitative research; Compared to works with rigorous analysis and evidence, his book is more of a collection of viewpoints; He also overemphasizes the negative aspects of crowds, reflecting the elitist attitudes of his time. Modern psychology has since shown crowd behavior to be more varied and complex than Le Bon described. ","date":"2025-09-11","objectID":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/:2:0","tags":["Psychology"],"title":"Notes from \"The Crowd: A Study of the Popular Mind\"","uri":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/"},{"categories":["Notes"],"content":"Thinking Reading is not only for acquiring information but for transforming how one thinks and lives. Every work deserves to be approached with care and discernment. “The Crowd: A Study of the Popular Mind” reminds us to stay aware in our daily lives: Am I currently part of a crowd? What characteristics does this crowd display? Within it, collective strength can be harnessed, but we must guard against blindness and fanaticism. By keeping a clear head and cultivating independent judgment, we can preserve our individual character while joining with others harmoniously without surrendering our individuality. ","date":"2025-09-11","objectID":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/:3:0","tags":["Psychology"],"title":"Notes from \"The Crowd: A Study of the Popular Mind\"","uri":"/en/2025/09/notes-from-the-crowd-a-study-of-the-popular-mind/"},{"categories":["Notes"],"content":"Four levels of reading: Elementary(Basic), Inspectional(Review), Analytical, and Syntopical(Subject).","date":"2025-09-10","objectID":"/en/2025/09/notes-from-how-to-read-a-book/","tags":["Method"],"title":"Notes from \"How to Read a Book: The Classic Guide to Intelligent Reading\"","uri":"/en/2025/09/notes-from-how-to-read-a-book/"},{"categories":["Notes"],"content":"Four levels of reading: Elementary(Basic), Inspectional(Review), Analytical, and Syntopical(Subject). ","date":"2025-09-10","objectID":"/en/2025/09/notes-from-how-to-read-a-book/:0:0","tags":["Method"],"title":"Notes from \"How to Read a Book: The Classic Guide to Intelligent Reading\"","uri":"/en/2025/09/notes-from-how-to-read-a-book/"},{"categories":["Notes"],"content":"Preface “How to Read a Book: The Classic Guide to Intelligent Reading” was published by American philosopher Mortimer J. Adler in 1940, advocating for the improvement of personal thinking and learning abilities through systematic reading. The book proposes four levels of reading: Elementary(Basic) reading, Inspectional(Review) reading, Analytical reading, and Syntopical(Subject) reading, and provides reading strategies for different types of books. ","date":"2025-09-10","objectID":"/en/2025/09/notes-from-how-to-read-a-book/:1:0","tags":["Method"],"title":"Notes from \"How to Read a Book: The Classic Guide to Intelligent Reading\"","uri":"/en/2025/09/notes-from-how-to-read-a-book/"},{"categories":["Notes"],"content":"Summary This book covers a very broad topic. The gap from Elementary(Basic) reading to Syntopical(Subject) reading is like the difference between a middle school student and a graduate student. Syntopical(Subject) reading goes beyond just “reading a book” and belongs more to research methods rather than reading techniques. And the writing can be quite wordy, so the book may not be worth reading cover to cover. Checking out mind maps might be enough. ","date":"2025-09-10","objectID":"/en/2025/09/notes-from-how-to-read-a-book/:2:0","tags":["Method"],"title":"Notes from \"How to Read a Book: The Classic Guide to Intelligent Reading\"","uri":"/en/2025/09/notes-from-how-to-read-a-book/"},{"categories":["Notes"],"content":"Thinking The four levels of reading from “How to Read a Book: The Classic Guide to Intelligent Reading” offer real practical value. These are Elementary(Basic) reading, Inspectional(Review) reading, Analytical reading, and Syntopical(Subject) reading. Inspectional(Review) reading helps quickly decide if a book deserves deeper attention, saving time on low-value books. Analytical reading helps understand and absorb key ideas from important books more thoroughly. I will apply these methods to improve my reading efficiency, gain more valuable knowledge within limited time, and take this opportunity to develop a habit of continuous reading, constantly expanding my knowledge boundaries and enriching my cognition. ","date":"2025-09-10","objectID":"/en/2025/09/notes-from-how-to-read-a-book/:3:0","tags":["Method"],"title":"Notes from \"How to Read a Book: The Classic Guide to Intelligent Reading\"","uri":"/en/2025/09/notes-from-how-to-read-a-book/"},{"categories":["Notes"],"content":"Lean Startup is about quickly building a simple version of a concept, testing it with real users, learning from the results, and improving quickly to reduce the wastage of time and resources.","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Notes"],"content":"Lean Startup is about quickly building a simple version of a concept, testing it with real users, learning from the results, and improving quickly to reduce the wastage of time and resources. ","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/:0:0","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Notes"],"content":"Preface Eric Ries, author of “The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses”, co-founded IMVU, a 3D avatar virtual social platform, where he also served as CTO. His background includes roles as Entrepreneur-in-Residence at Harvard Business School and providing business and product strategy consulting to startups, large companies, and venture capital firms. The book breaks down the startup process into a simple cycle: “Ideas - Build - Measure - Learn - New ideas”. The core approach focuses on turning ideas into basic product versions with minimal investment, getting these products to market quickly, and collecting real user feedback. Based on this feedback data, startups can rapidly shift direction, improve products, and discover what customers actually want. The key message is clear: startups need to find valuable business insights as quickly and cheaply as possible before running out of money. ","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/:1:0","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Notes"],"content":"Summary Lean Startup changes how people think about starting businesses by offering a scientific approach to deal with the high uncertainty that new ventures face. The core philosophy rests on five principles: entrepreneurs exist everywhere, entrepreneurship requires management, learning must be validated, the “Build-Measure-Learn” cycle drives progress, and innovation needs proper accounting. This approach uses scientific testing instead of gut feelings to check business ideas, turning entrepreneurship from talent-based art into learnable, repeatable science. In practice, Lean Startup centers around the “Build-Measure-Learn” cycle. This involves creating Minimum Viable Products (MVP) to quickly test markets and gather real user feedback, using innovation accounting to track progress, and making data-driven decisions about whether to change direction or keep going. This process works alongside three growth methods (sticky, viral, and paid) and small-batch work styles to help startups learn the most while spending the least time and money, avoiding the waste common in traditional startup approaches. Lean Startup goes beyond product development - it’s a management approach for building learning organizations. Tools like “Five Whys” create continuous learning systems, balancing quality with speed while building innovation culture. Both startups and large companies can use this methodology to boost innovation success rates, cut project failures, and create lasting business value. In modern product development practices, Lean Startup can integrate with advanced methodologies “Design Thinking” and “Agile”: Design thinking is used to deeply understand users, clarify needs, and inspire creativity; Lean Startup is used to transform creativity into viable business models; Agile development efficiently implements ideas into products through rapid iteration and continuous delivery. ","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/:2:0","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Notes"],"content":"Thinking After reading “The Lean Startup,” I have gained a completely new understanding of entrepreneurship. As the book states, “Validated Learning” is the most valuable asset for new ventures, not those elaborate plans and complex features based on assumptions. This has given me a profound understanding that entrepreneurship is essentially a process of continuous learning and experimentation under extreme uncertainty. I will continue to enrich my business knowledge, and keep an open mindset to embrace failure and change. Hopefully, at some point in the future, I can apply what I’ve learned to create my own business venture. ","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/:3:0","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Notes"],"content":"References https://slidemodel.com/lean-startup-methodology/ ","date":"2025-09-08","objectID":"/en/2025/09/notes-from-the-lean-startup/:4:0","tags":["Business","Startup"],"title":"Notes from \"The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses\"","uri":"/en/2025/09/notes-from-the-lean-startup/"},{"categories":["Skills"],"content":"Presenting conventional commits, branching models, semantic versioning, and source-based versioning in YAML format.","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Presenting conventional commits, branching models, semantic versioning, and source-based versioning in YAML format. ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:0:0","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Conventional Commits ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:1:0","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Specification website: - https://www.conventionalcommits.org - https://github.com/commitizen/conventional-commit-types/blob/master/index.json conventional_commits: feat: new feature fix: bug fix docs: documentation only changes style: changes that do not affect the meaning of the code, e.g., white-space, formatting, missing semi-colons refactor: code change that neither fixes a bug nor adds a feature perf: code change that improves performance test: adding missing tests or correcting existing tests build: changes that affect the build system or external dependencies, e.g., gulp, broccoli, npm ci: changes to our CI configuration files and scripts, e.g., Travis, Circle, BrowserStack, SauceLabs chore: other changes that do not modify src or test files revert: reverts a previous commit structure: \u003ctype\u003e(scope): \u003csubject\u003e verb_aliases: fix: - resolve - handle - correct - prevent - update ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:1:1","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Examples With description and breaking change footer feat: allow provided config object to extend other configs BREAKING CHANGE: `extends` key in config file is now used for extending other config files With ! to draw attention to breaking change feat!: send an email to the customer when a product is shipped With scope and ! to draw attention to breaking change feat(api)!: send an email to the customer when a product is shipped With both ! and BREAKING CHANGE footer chore!: drop support for Node 6 BREAKING CHANGE: use JavaScript features not available in Node 6. With no body docs: correct spelling of CHANGELOG With scope feat(lang): add Polish language With multi-paragraph body and multiple footers fix: prevent racing of requests Introduce a request id and a reference to latest request. Dismiss incoming responses other than from latest request. Remove timeouts which were used to mitigate the racing issue but are obsolete now. Reviewed-by: Z Refs: #123 With a footer that references the commit SHA that is being reverted revert: feat(lang): add Polish language This reverts commit 667ecc1654a317a13331b17617d973392f415f02. ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:1:2","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Branching Models ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:2:0","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Gitflow Gitflow: website: - https://nvie.com/posts/a-successful-git-branching-model/ - https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow description: Traditional workflow, suitable for projects with clear release cycles and multi-person collaboration branches: feature/*: feature development branches, e.g., feature/login, feature/JIRA-985-login bugfix/*: fix known non-urgent defects, e.g., bugfix/sso-auth, bugfix/JIRA-211-sso-auth develop: development integration branch, merge multiple features and bugfixes release/*: release candidate branches, e.g., release/1.6.2 hotfix/*: production hotfixes, e.g., hotfix/1.6.3 main: production stable branch workflow: # Development flow: feature/bugfix branches from develop, merge back to develop, then through release to main, and tag on main develop → feature/bugfix → develop → release → main → tag(vX.Y.Z)[recommended] # Emergency fix: hotfix branches from main, merge back to develop and main, and tag on main main → hotfix ↘ develop ↘ main → tag(vX.Y.Z)[recommended] advantages: - Clear branch roles, suitable for version management - Easy to manage release and fix processes disadvantages: - Many branches, complex CI/CD processes - Not suitable for high-frequency continuous delivery extensions: branches: user/\u003cname\u003e/*: personal branches, e.g., user/john/login, user/alice/sso-auth env/*: environment branches, e.g., env/uat, env/prod notes: - user/\u003cname\u003e/* only for personal experiments or independent development, no guarantee of merge, avoid polluting formal feature branches - env/* only for tracking current deployment versions in each environment, no direct code development, updates through merge commits ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:2:1","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"GitHub Flow GitHubFlow: website: https://docs.github.com/en/get-started/using-github/github-flow description: Simplified workflow, suitable for continuous delivery and small teams branches: main: the only long-term branch, always in deployable state, whether to deploy depends on team strategy feature/*: feature branches derived from main workflow: # Feature development: feature branches from main, merge back to main through Pull/Merge Request # Release version: tag on main to mark deployable versions main → feature → main → tag(vX.Y.Z)[recommended] advantages: - Simple process, suitable for rapid iteration - Combined with Pull/Merge Request review disadvantages: - Lacks stable development integration branch - Version control depends on tags ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:2:2","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Trunk Based Development TrunkBasedDevelopment: website: https://trunkbaseddevelopment.com description: Trunk-based development, minimalist workflow, suitable for continuous integration/daily builds branches: main: the only trunk branch feature/*: very short-term feature branches, quickly merge back to main workflow: # Feature development: feature branches from main, short-cycle development then quickly merge back to main # Continuous release: each main merge can trigger build and release main → feature → main → tag(vX.Y.Z)[optional] notes: - Due to frequent merges, usually use `datetime + commit_hash` as version number, e.g., 2025.07.30.1906.3f9a7c1d - Semantic versioning (vX.Y.Z) only for important milestone tags advantages: - Supports high-frequency releases, automation CI/CD friendly - Avoids code drift from long-term branches disadvantages: - Requires strong testing and automation support - High requirements for team collaboration ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:2:3","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"GitLab Flow GitLabFlow: website: https://about.gitlab.com/blog/gitlab-flow-duo/ description: GitLab's officially recommended workflow, combines trunk development with environment branches, suitable for multi-environment CI/CD deployment branches: feature/*: feature development branches, derived from main, merge back to main through Merge Request main: main branch, for integration and testing, always in mergeable state staging: pre-release environment branch, for pre-launch validation production: production environment branch, tracks current online deployment version workflow: # Feature development: feature branches from main, merge back to main after completion # Deployment process: main merge to staging triggers pre-release, main merge to production triggers production deployment # Optional release: tag on production to mark production version main → feature → main ↘ staging ↘ production → tag(vX.Y.Z)[recommended] notes: - staging/production branches for tracking current deployment versions in each environment - Environment branch updates through merge commits, no direct code development on these branches - Recommend tagging when main merges to production to mark official release version advantages: - Supports multi-environment deployment, CI/CD friendly - Manages releases through environment branches, intuitive tracking of current deployment status - Keeps main branch clean, features controlled through Merge Request disadvantages: - Requires strict merge policies and CI/CD conventions - May be overly complex for small projects ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:2:4","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Version Management ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:3:0","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Label Naming version_label: snapshot: development snapshot alpha: internal testing beta: public testing rc: release candidate release: official release hotfix: emergency fix ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:3:1","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Semantic Versioning semantic_versioning: website: https://semver.org format: MAJOR.MINOR.PATCH initial_development_version: 0.1.0 description: MAJOR: description: major version number scenario: incompatible changes example: 0.12.8 → 1.0.0 rule: major version +1, minor and patch versions reset to zero MINOR: description: minor version number scenario: backward-compatible feature additions example: 1.5.1 → 1.6.0 rule: minor version +1, patch version resets to zero PATCH: description: patch version number scenario: backward-compatible bug fixes example: 1.6.1 → 1.6.2 rule: patch version +1 pre_release_versions: format: MAJOR.MINOR.PATCH-\u003clabel\u003e[.\u003cidentifier\u003e] labels: alpha: internal testing version beta: public testing version rc: release candidate examples: - 1.12.6-alpha - 1.12.6-beta - 1.12.6-rc.1 - 1.12.6-rc.2 metadata: format: MAJOR.MINOR.PATCH[-\u003clabel\u003e]+\u003cmetadata\u003e examples: - 1.12.6-alpha+3f9a7c1d - 1.12.7+20250730 version_evolution_example: | 0.1.0 → 0.1.1 → ... ↓ ... ↓ 0.12.8 ↓ 1.0.0 ↓ ... ↓ 1.5.1 ↓ 1.6.0 → 1.6.1 → 1.6.2 ↓ ... ↓ 1.12.6-alpha(1.12.6-alpha+3f9a7c1d) → 1.12.6-beta → 1.12.6-rc.1 → 1.12.6-rc.2 ↓ 1.12.7(1.12.7+20250730) ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:3:2","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"Source-based Versioning source_based_versioning: format: YYYY.mm.dd.HHMM-\u003cbranch\u003e-\u003ccommit_hash\u003e description: YYYY.mm.dd.HHMM: build date and time branch: branch name commit_hash: short hash value (first 8 characters) examples: - 2025.07.28.1802-feature-login-c4d8b21e - 2025.07.29.1752-develop-b17e5a9f - 2025.07.30.1906-main-3f9a7c1d advantages: - Directly shows build source branch - Each version is unique and traceable to specific commit - Suitable for automated continuous integration extended_rules: with_release_labels: format: YYYY.mm.dd.HHMM-\u003cbranch\u003e-\u003ccommit_hash\u003e-\u003clabel\u003e labels: alpha: internal testing version beta: public testing version rc: release candidate examples: - 2025.07.30.1906-main-3f9a7c1d-alpha ","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:3:3","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Skills"],"content":"GitLab CI Version Generation Generate MAGIC_VERSION variable # .gitlab-ci.magic-version.yml # Use GitLab's reserved stage `.pre` to ensure the version is generated before all custom stages # The artifacts.reports.dotenv exported in `.pre` will be automatically injected into all later jobs # Later jobs can directly use $MAGIC_VERSION without `dependencies:` or `needs:` # Version based on tag generate-magic-version-tag: stage: .pre rules: - if: $CI_COMMIT_TAG # When building from a tag script: # Get the current $CI_COMMIT_TAG, e.g. v1.2.3 - export MAGIC_VERSION=$CI_COMMIT_TAG # Write the version to build.env - echo \"MAGIC_VERSION=$MAGIC_VERSION\" \u003e\u003e build.env # Print build.env for debugging - cat build.env # Export build.env as artifacts.reports.dotenv # GitLab will inject it into later stages automatically artifacts: reports: dotenv: build.env # Version based on branch generate-magic-version-branch: stage: .pre rules: - if: $CI_COMMIT_BRANCH # When building from a branch script: # Generate datetime in format YYYY.mm.dd.HHMM, e.g. 2025.07.30.1906 - VERSION_DATETIME=$(date +'%Y.%m.%d.%H%M') # Combine version: YYYY.mm.dd.HHMM-\u003cbranch\u003e-\u003ccommit_hash\u003e # Example: 2025.07.30.1906-main-3f9a7c1d # Variable notes: # $CI_COMMIT_REF_SLUG -\u003e branch slug, e.g. feature-login / main # $CI_COMMIT_SHORT_SHA -\u003e short commit hash (first 8 chars) - export MAGIC_VERSION=${VERSION_DATETIME}-${CI_COMMIT_REF_SLUG}-${CI_COMMIT_SHORT_SHA} # Write the version to build.env - echo \"MAGIC_VERSION=$MAGIC_VERSION\" \u003e\u003e build.env # Print build.env for debugging - cat build.env # Export build.env as artifacts.reports.dotenv # GitLab will inject it into later stages automatically artifacts: reports: dotenv: build.env Reference the MAGIC_VERSION variable directly in the subsequent stage Jobs # .gitlab-ci.delivery.yml # Build and publish Docker image publish-dockerimage: stage: delivery cache: [] image: docker:24.0.5-cli services: - docker:24.0.5-dind variables: # CS_IMAGE is the default Docker image variable recognized by GitLab Container Scanning # https://docs.gitlab.com/user/application_security/container_scanning/ CS_IMAGE: $CI_REGISTRY_IMAGE:$MAGIC_VERSION script: # Login to GitLab Docker Registry - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY # Build image using $CS_IMAGE - docker build -t $CS_IMAGE . # Push image to GitLab Docker Registry - docker push $CS_IMAGE # Write the image to build.env - echo \"CS_IMAGE=$CS_IMAGE\" \u003e\u003e build.env # Print build.env for debugging - cat build.env # Export build.env as artifacts.reports.dotenv # For later use in container scan or deploy stages artifacts: reports: dotenv: build.env","date":"2025-07-30","objectID":"/en/2025/07/commit-branching-versioning-best-practices/:3:4","tags":["Git","DevOps","CICD"],"title":"Best Practices for Commits, Branching and Versioning","uri":"/en/2025/07/commit-branching-versioning-best-practices/"},{"categories":["Notes"],"content":"Reference: https://glossary.cncf.io/","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"Reference: https://glossary.cncf.io/ ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:0:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🧱 Architecture \u0026 Applications Monolithic Apps: A monolithic application contains all functionality in a single deployable program. Devolving an application into microservices increases its operational overhead — there are more things to test, deploy, and keep running. Early in a product’s lifecycle, it may be advantageous to defer this complexity and build a monolithic application until the product is determined successful. A well-designed monolith can uphold lean principles by being the simplest way to get an application up and running. When the business value of the monolithic application proves successful, it can be decomposed into microservices. Crafting a microservices-based app before it has proven valuable may be premature spending of engineering effort. If the application yields no value, that effort becomes wasted. Microservices: Separating functionality into different microservices, making them easier to deploy, update, and scale independently. Distributed Apps: Functionality broken down into multiple smaller independent parts that can run simultaneously in multiple locations, tolerate more failures, and have scaling capabilities that individual application instances don’t possess. Distributed Systems: A collection of autonomous computing elements connected by a network, appearing as a single coherent system to users. Client Server Architecture: Application logic implemented in remote servers, allowing application updates without modifying client logic. Loosely Coupled Architecture: Application components built independently, each designed to perform specific functions for use by any number of other services. Tightly Coupled Architecture: Many application components depend on each other, where changes to one component may affect others, potentially speeding up initial development cycles. Event Driven Architecture: Events are any changes to application state, defining event producers (sources) and consumers (receivers), with central event hubs (like Kafka) ensuring event flow. Stateless Apps: State refers to any data an application needs to store to run as designed; these apps don’t save any client session state data, with each session executing as if it’s the first time, responses not depending on previous session data (like search engines). Stateful Apps: Client session state temporarily stored in memory or persistently in local disk or database systems. Vertical Scaling: Technique to increase system capacity by adding CPU and memory to a single node when workload increases. Horizontal Scaling: Technique to increase system capacity by adding more nodes rather than adding more computing resources to a single node. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:1:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🚢 Containers \u0026 Orchestration Container: Provides an isolated runtime environment, allowing different applications to run in independent environments with separate file systems, networks, and process spaces. Container Image: Immutable static files containing applications with their runtime dependencies. Containerization: Process of bundling applications with their runtime dependencies into container images. Container Orchestration: Container orchestration tools act like conductors directing numerous containers (musicians), ensuring each container performs its role. Kubernetes: Often abbreviated as K8S, a popular open-source container orchestration tool. Pod: The most basic deployable unit in Kubernetes environment, can contain one or more containers. Cluster: A group of computers or applications connected by a network working together for a common goal, eliminating single points of failure. Nodes: A computer that can work with other computers (or nodes) to accomplish a common task. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:2:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🧩 Service Governance API Application Programming Interface: A clear, understandable way for computer programs to interact and share information. API Gateway: Tools that aggregate APIs from multiple applications for one-stop management. Load Balancer: Acts as a traffic proxy, distributing network traffic across multiple servers, improving application reliability during traffic peaks. Service: Specific definitions vary by context; in microservices architecture, typically refers to independently deployable functional modules. Service Proxy: Acts as a “middleman” intercepting and forwarding service traffic, collecting traffic information and applying rules for traffic management and security control. Service Discovery: Continuously tracks applications in the network, providing a common place to find and identify different services so applications can find each other. Service Mesh: Unified management of inter-service communication, adding reliability, observability, and security features without code changes. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:3:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"☁️ Service Models Cloud Computing: On-demand provision of computing resources (like CPU, network, and disk capabilities) via the internet, divided into private and public clouds depending on whether cloud infrastructure is dedicated to an organization or shared as a public service. IaaS Infrastructure-as-a-Service: Cloud providers offer physical or virtual computing, storage, and network resources with on-demand, pay-as-you-go billing. PaaS Platform-as-a-Service: Provides general infrastructure to application developers in a fully automated manner, allowing developers to focus more time and effort on writing application code. CaaS Container-as-a-Service: No need to manage underlying infrastructure for container operation, automating container deployment and management. DBaaS Database-as-a-Service: Cloud providers manage database configuration, backup, patching, upgrades, monitoring, etc., while developers only use the database. SaaS Software-as-a-Service: Software installed, maintained, upgraded, and secured by vendors, handling scaling, availability, and capacity issues; users access software via the internet with pay-as-you-go models. Serverless: Cloud-native development model where developers build and run applications without managing servers, with cloud providers handling configuration, maintenance, and scaling of server infrastructure. Managed Services: Software services operated and managed by third parties, allowing users to focus on core business and reduce operational burden. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:4:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🛠 DevOps DevOps: Integration of development and operations, emphasizing collaboration, automation, and continuous delivery/deployment, giving teams ownership of the entire application lifecycle, minimizing handoffs, improving code quality, and reducing deployment risks. CI Continuous Integration: Regular integration of code changes with automated testing, improving collaboration efficiency and code quality. CD Continuous Delivery: Automatic deployment of code changes to acceptance environments, ensuring software is fully tested before production deployment. CD Continuous Deployment: Automatic deployment of tested environment code changes to production, going one step further than continuous delivery. Canary Deployment: Gradually shifting traffic from old to new versions, testing on a small scale first, allowing quick rollback if issues arise, reducing deployment risks; named after miners using canaries’ sensitivity to harmful gases for safety warnings. Blue-Green Deployment: Maintaining two environments simultaneously, “blue” as current production and “green” as new production, achieving zero-downtime deployment through rapid traffic switching, suitable for scenarios requiring complete synchronous changes due to lack of backward compatibility. Auto Scaling: System automatically increases or decreases resources based on demand, improving elasticity and efficiency. Self Healing: Recovery from failures without any human intervention. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:5:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🔐 Reliability \u0026 Security SRE Site Reliability Engineering: Discipline combining operations and software engineering; DevOps focuses on getting code into production, SRE ensures code runs properly in production. CE Chaos Engineering: Actively injecting failures in production to validate system resilience and self-healing capabilities, building confidence in system ability to withstand turbulence and unexpected conditions. Security Chaos Engineering: Executing proactive security experiments on distributed systems, building confidence in system ability to resist turbulence and malicious conditions. Cloud Native Security: Integrating security throughout the cloud-native application lifecycle, adapting to cloud-native environment characteristics of rapid code changes and highly ephemeral infrastructure. Zero Trust Architecture: Never trust, always verify; a method of IT system design and implementation that completely eliminates trust. Firewall: Systems that filter network traffic based on specific rules; firewalls can be hardware, software, or a combination. TLS Transport Layer Security: Protocol designed to provide higher security for network communications, preventing data eavesdropping and tampering. mTLS mutual Transport Layer Security: Ensures bidirectional traffic between clients and servers is secure and trusted, with no unauthorized parties eavesdropping or impersonating legitimate requests. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:6:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🔄 Infrastructure IaC Infrastructure-as-Code: Defining and managing infrastructure with code, achieving automation, reproducibility, and version control, reducing human configuration errors. PaC Policy-as-Code: Defining and automatically executing policies with machine-readable files, improving compliance and automation, reducing human errors. Immutable Infrastructure: Computer infrastructure (like VMs, containers, network devices) that cannot be changed once deployed, making it easier to identify and mitigate security risks. Bare Metal Machine: Physical servers running operating systems and applications directly, without virtualization layer, with exclusive resources and optimal performance. Data Center: Buildings or facilities designed specifically to house servers, ensuring secure, stable, and efficient operation of computing resources. Edge Computing: Moving computing and storage resources from the center to near data sources, improving real-time performance and efficiency, reducing latency. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:7:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🧠 System Characteristics Scalability: System can easily expand capacity by adding nodes or resources to meet growing business demands. Portability: Software can run across different operating systems or cloud environments, reducing dependency on specific platforms, facilitating migration and reuse. Observability: Gaining insights into system state and taking corrective measures through collection and analysis of system information. Reliability: System’s ability to respond to failures, continuing to operate when components fail. Idempotence: Producing the same result regardless of how many times executed; if parameters are the same, an idempotent operation won’t affect the application it calls. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:8:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"📁 Development \u0026 Collaboration Version Control: System that continuously records changes to individual files or groups of files, helping developers act quickly and maintain efficiency while storing change records and providing conflict resolution tools. Agile Software Development: Emphasizing iterative development and team self-organization, continuously delivering value, responding quickly to changes, adapting to complex requirements. Debugging: Faults are defects or problems causing incorrect or unexpected results; software development is complex, and writing code without introducing faults is nearly impossible; debugging is the process of finding and resolving faults to achieve expected results. Shift Left: Implementing testing, security, or other development practices in early stages of the software development lifecycle (viewing the lifecycle as a left-to-right execution timeline), discovering and solving problems early, reducing later repair costs. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:9:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Notes"],"content":"🧬 Cloud Native Philosophy Cloud Native Apps: Applications specifically designed to leverage cloud computing innovations, easily integrating with cloud architecture and fully utilizing cloud resources and scalability features. Cloud Native Tech: Also called cloud-native technology stack, enabling organizations to build and run scalable applications in modern dynamic environments like public, private, and hybrid clouds, fully leveraging cloud computing advantages. Multitenancy: Sharing the same software while providing each tenant with an isolated environment (work data, settings, credential lists, etc.), serving multiple tenants simultaneously. ","date":"2025-07-28","objectID":"/en/2025/07/cloud-native-glossary-lite/:10:0","tags":["Cloud Native"],"title":"Cloud Native Glossary Lite","uri":"/en/2025/07/cloud-native-glossary-lite/"},{"categories":["Skills"],"content":"Health check endpoint with dependency-aware critical checks and independent external monitoring.","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"Health check endpoint with dependency-aware critical checks and independent external monitoring. Tip “Talk is cheap. Show me the code.” - Linus Torvalds Code Sharing: https://github.com/mcsrainbow/mock-healthz-metrics ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:0:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"✨ Features /healthz: Return code: Healthy 200, Unhealthy 500 Output: Plaintext, JSON /metrics: Prometheus metrics Two-tier health checks: 🔴 Critical checks Affect overall health status: 🔌 Database connection: Core dependency, must be healthy ⚙️ Config service: Core dependency, must be healthy 🔁 Internal APIs billing, usage: Depend on upstream services (DB + Config), skipped if upstream fails 🟡 External checks Independent: 🌍 External APIs alipay, sms: Run independently, don’t affect overall health status Dependencies have built-in error probability and timeout simulation Fully compatible with Kubernetes probes and Prometheus scraping Dependency Chain: Database + Config Service → Internal APIs → Overall Health Status External APIs → Independent Monitoring Only ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:1:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"🚀 Getting Started ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:2:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"1. Install dependencies pip install bottle ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:2:1","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"2. Run the script python mock-healthz-metrics.py Service endpoints available: Healthcheck (Text): http://0.0.0.0:8080/healthz Healthcheck (JSON): http://0.0.0.0:8080/healthz?format=json Prometheus metrics: http://0.0.0.0:8080/metrics ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:2:2","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"✅ Healthcheck ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:3:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"Plaintext http://127.0.0.1:8080/healthz http://127.0.0.1:8080/healthz?format=text Healthy HEALTH CHECK SNAPSHOT [2025-07-18 11:03:15] ------------------------------------------- CHECK STATUS MESSAGE ----- CRITICAL ----- db_connection ✔ Database is connected config_service ✔ Config service is reachable internal_api/billing ✔ internal_api/billing OK (392ms) internal_api/usage ✔ internal_api/usage OK (348ms) ----- EXTERNAL ----- external_api/alipay ✔ external_api/alipay OK (308ms) external_api/sms ✖ external_api/sms timed out Unhealthy HEALTH CHECK SNAPSHOT [2025-07-18 11:05:27] ------------------------------------------- CHECK STATUS MESSAGE ----- CRITICAL ----- db_connection ✔ Database is connected config_service ✔ Config service is reachable internal_api/billing ✔ internal_api/billing OK (253ms) internal_api/usage ✖ internal_api/usage returned error ----- EXTERNAL ----- external_api/alipay ✔ external_api/alipay OK (101ms) external_api/sms ✔ external_api/sms OK (183ms) ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:3:1","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"JSON Format http://127.0.0.1:8080/healthz?format=json Healthy { \"status\": \"ok\", \"data\": { \"message\": \"All critical checks passed\", \"snapshot_time\": \"2025-07-18 11:03:15\", \"checks\": { \"critical\": [ { \"name\": \"db_connection\", \"status\": \"ok\", \"message\": \"Database is connected\" }, { \"name\": \"config_service\", \"status\": \"ok\", \"message\": \"Config service is reachable\" }, { \"name\": \"internal_api/billing\", \"status\": \"ok\", \"message\": \"internal_api/billing OK (392ms)\" }, { \"name\": \"internal_api/usage\", \"status\": \"ok\", \"message\": \"internal_api/usage OK (348ms)\" } ], \"external\": [ { \"name\": \"external_api/alipay\", \"status\": \"ok\", \"message\": \"external_api/alipay OK (308ms)\" }, { \"name\": \"external_api/sms\", \"status\": \"error\", \"message\": \"external_api/sms timed out\" } ] } } } Unhealthy { \"status\": \"error\", \"data\": { \"message\": \"Some critical checks failed\", \"snapshot_time\": \"2025-07-18 11:05:27\", \"checks\": { \"critical\": [ { \"name\": \"db_connection\", \"status\": \"ok\", \"message\": \"Database is connected\" }, { \"name\": \"config_service\", \"status\": \"ok\", \"message\": \"Config service is reachable\" }, { \"name\": \"internal_api/billing\", \"status\": \"ok\", \"message\": \"internal_api/billing OK (253ms)\" }, { \"name\": \"internal_api/usage\", \"status\": \"error\", \"message\": \"internal_api/usage returned error\" } ], \"external\": [ { \"name\": \"external_api/alipay\", \"status\": \"ok\", \"message\": \"external_api/alipay OK (101ms)\" }, { \"name\": \"external_api/sms\", \"status\": \"ok\", \"message\": \"external_api/sms OK (183ms)\" } ] } } } ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:3:2","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"📈 Prometheus http://127.0.0.1:8080/metrics ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:4:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"Config scrape_configs: - job_name: 'mock-healthz-metrics' static_configs: - targets: ['127.0.0.1:8080'] Scrape http://127.0.0.1:8080/metrics by default. ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:4:1","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"Metrics Healthy # HELP healthcheck_status Health check status (1=ok,0=error) # TYPE healthcheck_status gauge healthcheck_status{check=\"db_connection\",type=\"critical\"} 1 healthcheck_status{check=\"config_service\",type=\"critical\"} 1 healthcheck_status{check=\"internal_api/billing\",type=\"critical\"} 1 healthcheck_status{check=\"internal_api/usage\",type=\"critical\"} 1 healthcheck_status{check=\"external_api/alipay\",type=\"external\"} 1 healthcheck_status{check=\"external_api/sms\",type=\"external\"} 0 Unhealthy # HELP healthcheck_status Health check status (1=ok,0=error) # TYPE healthcheck_status gauge healthcheck_status{check=\"db_connection\",type=\"critical\"} 1 healthcheck_status{check=\"config_service\",type=\"critical\"} 1 healthcheck_status{check=\"internal_api/billing\",type=\"critical\"} 1 healthcheck_status{check=\"internal_api/usage\",type=\"critical\"} 0 healthcheck_status{check=\"external_api/alipay\",type=\"external\"} 1 healthcheck_status{check=\"external_api/sms\",type=\"external\"} 1 ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:4:2","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"🐳 Kubernetes For Kubernetes livenessProbe and readinessProbe: ❯ curl -I http://127.0.0.1:8080/healthz HTTP/1.0 200 OK Date: Fri, 18 Jul 2025 03:03:31 GMT Server: WSGIServer/0.2 CPython/3.11.11 Content-Type: text/plain Content-Length: 444 ❯ curl -I http://127.0.0.1:8080/healthz HTTP/1.0 500 Internal Server Error Date: Fri, 18 Jul 2025 03:05:36 GMT Server: WSGIServer/0.2 CPython/3.11.11 Content-Type: text/plain Content-Length: 438 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 10 periodSeconds: 30 failureThreshold: 5 timeoutSeconds: 5 readinessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 5 periodSeconds: 20 failureThreshold: 3 successThreshold: 2 timeoutSeconds: 2 For more detailed probe design, please refer to the article: Kubernetes Container Healthcheck and Graceful Termination. ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:5:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Skills"],"content":"📌 Notes This script uses randomized logic to simulate service instability For production usage, replace simulation logic with real health check implementations (e.g., database pings, HTTP client calls, etc.) ","date":"2025-07-18","objectID":"/en/2025/07/health-check-endpoint-healthz-design-practice/:6:0","tags":["Kubernetes"],"title":"Health Check Endpoint /healthz Design Practice","uri":"/en/2025/07/health-check-endpoint-healthz-design-practice/"},{"categories":["Thinking"],"content":"Good work mindsets develop professional behaviors.","date":"2025-01-21","objectID":"/en/2025/01/work-mindsets/","tags":["Growth","Self-management"],"title":"Work Mindsets","uri":"/en/2025/01/work-mindsets/"},{"categories":["Thinking"],"content":"Good work mindsets develop professional behaviors. Principle-minded Focus on the “why” behind things, not just “how”. This helps enhance problem-solving skills and makes decisions more forward-looking and adaptable. Resilience-minded Back up all data, and ensure every change has a rollback plan. This minimizes the risk of unexpected failures and prevents irreversible damage. Ticket-minded Track all tasks through tickets, rather than relying on emails or chats. This keeps work organized and transparent, making follow-up easier. Automation-minded Use tools and scripts to automate repetitive tasks, avoiding manual operations. This saves time, increases efficiency, and reduces errors. Test-minded Every change must be tested first. Experience, trust, and confidence are all unreliable. Concise-minded Prioritize data metrics, screenshots, flowcharts, architecture diagrams, mind maps, and spreadsheets when documenting and presenting. Clear visual communication is more efficient and easier to understand than lengthy text. Knowledge-minded Document experiences, acquired knowledge, and lessons learned. This will prevent repeated mistakes, improve efficiency and transparency, and boost the team’s overall capability. Team-minded Properly dividing tasks and collaborating helps avoid over-reliance on any individual and efficiently accomplish seemingly impossible tasks. One person may go faster alone, but a team can go further together. ","date":"2025-01-21","objectID":"/en/2025/01/work-mindsets/:0:0","tags":["Growth","Self-management"],"title":"Work Mindsets","uri":"/en/2025/01/work-mindsets/"},{"categories":["Skills"],"content":"By implementing a Kubernetes multi-environment and multi-application orchestration with the same configuration by YAML(Manifests), Kustomize and Helm, we can quickly get started with Kustomize and Helm and understand the differences between them.","date":"2024-09-10","objectID":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/","tags":["Kubernetes","Kustomize","Helm"],"title":"Kubernetes Multi-environment and Multi-application Orchestration Practice","uri":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/"},{"categories":["Skills"],"content":"By implementing a Kubernetes multi-environment and multi-application orchestration with the same configuration by YAML(Manifests), Kustomize and Helm, we can quickly get started with Kustomize and Helm and understand the differences between them. Tip “Talk is cheap. Show me the code.” - Linus Torvalds Code Sharing: https://github.com/mcsrainbow/k8s-apps ","date":"2024-09-10","objectID":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/:0:0","tags":["Kubernetes","Kustomize","Helm"],"title":"Kubernetes Multi-environment and Multi-application Orchestration Practice","uri":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/"},{"categories":["Skills"],"content":"Manifests Native YAML (Manifests) is the most straightforward and simple method for configuring Kubernetes resources. For multi-environment and multi-application orchestration, the approach is direct: create directories for different environments and separate resources into different YAML files based on common and application-specific configurations. manifests/apps-overlays ├── development │ ├── bu-project-all.yaml │ ├── bu-project-apiproxy.yaml │ └── bu-project-webfront.yaml ├── production │ ├── bu-project-all.yaml │ ├── bu-project-apiproxy.yaml │ └── bu-project-webfront.yaml └── staging ├── bu-project-all.yaml ├── bu-project-apiproxy.yaml └── bu-project-webfront.yaml Pros: Highly intuitive, with no coupling between configurations for different environments and applications, reducing the impact of configuration errors. Cons: Repeated configuration code and difficulty modifying parameters, as changes require text-based search and replace. ","date":"2024-09-10","objectID":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/:1:0","tags":["Kubernetes","Kustomize","Helm"],"title":"Kubernetes Multi-environment and Multi-application Orchestration Practice","uri":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/"},{"categories":["Skills"],"content":"Kustomize Kustomize is a resource configuration method that sits between YAML and Helm, enhancing YAML by introducing built-in plugins for code reuse and parameter modification. Kustomize reads from the kustomization.yaml file by default, and a layered design can be achieved by referencing kustomization.yaml files in different directories. kustomize/apps-overlays ├── base │ ├── apps │ │ ├── apiproxy │ │ │ ├── deployment.yaml │ │ │ ├── ingress.yaml │ │ │ ├── kustomization.yaml │ │ │ └── service.yaml │ │ └── webfront │ │ ├── deployment.yaml │ │ ├── ingress.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ ├── files │ │ └── dockerconfigjson.encrypted │ ├── kustomization.yaml │ └── patches │ └── imagePullSecrets.yaml └── overlays ├── development │ ├── files │ │ └── nginx.conf │ └── kustomization.yaml ├── production │ ├── files │ │ └── nginx.conf │ └── kustomization.yaml └── staging ├── files │ └── nginx.conf └── kustomization.yaml Rendering of native YAML can be previewed using the kustomize build command. cd kustomize/apps-overlays kustomize build overlays/development kustomize build overlays/staging kustomize build overlays/production Pros: Compatible with native YAML, supports reusable configurations, and allows reading files to generate Secret and ConfigMap, as well as modifying parameters (e.g., adding labels, prefixes, adjusting CPU and memory). Cons: Code reuse introduces coupling, increasing the impact of configuration errors. The built-in plugins are limited, which restricts highly flexible reuse and parameter modifications. Kustomize Built-in Plugins: https://kubectl.docs.kubernetes.io/references/kustomize/builtins/ ","date":"2024-09-10","objectID":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/:2:0","tags":["Kubernetes","Kustomize","Helm"],"title":"Kubernetes Multi-environment and Multi-application Orchestration Practice","uri":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/"},{"categories":["Skills"],"content":"Helm Helm is a Kubernetes package manager and deployment tool, similar to Ansible + Yum on Linux. Helm reads variables defined in the values.yaml file along with templates from the templates directory. This allows for highly flexible code reuse, parameter modification, and file generation. Multi-environment and multi-application orchestration can be achieved by placing default and common parameters in the values.yaml file, and supplementing or overriding them through environment-specific values files. helm/apps-overlays ├── Chart.yaml ├── files │ ├── dockerconfigjson.encrypted │ └── nginx.conf ├── templates │ ├── apiproxy │ │ ├── _helpers.tpl │ │ ├── configmap.yaml │ │ ├── deployment.yaml │ │ ├── ingress.yaml │ │ └── service.yaml │ ├── secret.yaml │ └── webfront │ ├── _helpers.tpl │ ├── deployment.yaml │ ├── ingress.yaml │ └── service.yaml ├── values │ ├── development.yaml │ ├── production.yaml │ └── staging.yaml └── values.yaml Rendering of native YAML can be previewed using the helm template command. cd helm/apps-overlays helm template . -f values/development.yaml helm template . -f values/staging.yaml helm template . -f values/production.yaml Pros: Powerful templating, with highly flexible code reuse, parameter modification, and file generation. Configurations can be packaged and published to Helm Repos for package management. Cons: High flexibility leads to high coupling, and the templated code is less intuitive. Configuration errors can have a larger impact and be more difficult to debug. Helm Template Functions and Pipelines: https://helm.sh/docs/chart_template_guide/functions_and_pipelines/ ","date":"2024-09-10","objectID":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/:3:0","tags":["Kubernetes","Kustomize","Helm"],"title":"Kubernetes Multi-environment and Multi-application Orchestration Practice","uri":"/en/2024/09/kubernetes-multi-environment-and-multi-application-orchestration-practice/"},{"categories":["Skills"],"content":"Kubernetes container health checks and graceful termination enhances production stability, reduces deployment incidents and false alarms.","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Kubernetes container health checks and graceful termination enhances production stability, reduces deployment incidents and false alarms. ","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:0:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Parameters terminationGracePeriodSeconds: Global setting for Pod termination grace period, must greater than lifecycle.preStop. If containers aren’t stopped within this period, the Pod will be forcibly terminated. lifecycle.preStop: Hook to execute commands before container stops, delaying termination to release connections for in-flight requests. startupProbe: Checks the container startup status, providing additional preparation time. kubelet kills and restarts the container if the check fails. livenessProbe: Checks if the container is alive. kubelet kills and restarts the container if the check fails. readinessProbe: Checks if the container is ready to accept traffic. kubelet adds the Pod to the Service’s load balancer pool only if this check passes. ","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:1:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Practice Kubernetes deployment configurations with health checks and graceful termination: --- apiVersion: apps/v1 kind: Deployment metadata: namespace: default name: myapp spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # default: 30 terminationGracePeriodSeconds: 120 imagePullSecrets: - name: mysecret containers: - name: myapp image: registry.example.com/myapp:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 resources: requests: cpu: 500m memory: 1Gi limits: cpu: 500m memory: 1Gi env: - name: TZ value: Asia/Shanghai startupProbe: tcpSocket: port: 8080 # default: 0 initialDelaySeconds: 10 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 10 # default: 3 failureThreshold: 30 # default: 1 # cannot be changed, must be 1 by design successThreshold: 1 # default: 1 timeoutSeconds: 2 livenessProbe: tcpSocket: port: 8080 # default: 0 initialDelaySeconds: 10 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 30 # default: 3 failureThreshold: 5 # default: 1 # cannot be changed, must be 1 by design successThreshold: 1 # default: 1 timeoutSeconds: 5 readinessProbe: tcpSocket: port: 8080 # default: 0 initialDelaySeconds: 5 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 20 # default: 3 failureThreshold: 3 # default: 1 # set to 2 to avoid mistaking unstable new containers as ready successThreshold: 2 # default: 1 timeoutSeconds: 2 lifecycle: # delay container stop by 60 seconds preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 60\"] Default Kubernetes configurations: Startup Check: None Liveness: Minimum: 0 seconds Failure determination: 21 seconds ( failureThreshold(3) - 1 ) * periodSeconds(10) + timeoutSeconds(1) Readiness: Minimum: 0 seconds Failure determination: 21 seconds ( failureThreshold(3) - 1 ) * periodSeconds(10) + timeoutSeconds(1) Recovery determination: 10 seconds periodSeconds(10) Termination: Minimum: 0 seconds Maximum: 30 seconds terminationGracePeriodSeconds(30) Practice configurations: Startup: Minimum: 10 seconds initialDelaySeconds(10) Failure determination: 302 seconds initialDelaySeconds(10) + ( failureThreshold(30) - 1 ) * periodSeconds(10) + timeoutSeconds(2) Note: The working principle determines that startupProbe.successThreshold can only be set to 1 Liveness: Minimum: 20 seconds Startup(10) + initialDelaySeconds(10) Failure determination (first): 135 seconds initialDelaySeconds(10) + ( failureThreshold(5) - 1 ) * periodSeconds(30) + timeoutSeconds(5) Failure determination (running): 125 seconds ( failureThreshold(5) - 1 ) * periodSeconds(30) + timeoutSeconds(5) Note: The working principle determines that livenessProbe.successThreshold can only be set to 1 Readiness: Minimum: 35 seconds Startup(10) + initialDelaySeconds(5) + ( readinessProbe.successThreshold(2) - 1 ) * periodSeconds(20) Failure determination (first): 47 seconds initialDelaySeconds(5) + ( failureThreshold(3) - 1 ) * periodSeconds(20) + timeoutSeconds(2) Failure determination (running): 42 seconds ( failureThreshold(3) - 1 ) * periodSeconds(20) + timeoutSeconds(2) Recovery determination: 40 seconds successThreshold(2) * periodSeconds(20) Termination: Minimum 60 seconds sleep 60 Maximum 120 seconds terminationGracePeriodSeconds(120) Optimizations compared to default Kubernetes configurations: Startup: 10 seconds delay, failure threshold 302 seconds, failed checks trigger container restart. Liveness: 20 seconds delay, failure threshold 125 seconds, failed checks trigger container restart. Readiness: 35 seconds delay, 2 times health checks to avoid mistaking unstable new containers as ready, failure threshold 42 seconds blocks inbound traffic, 40 seconds recovery threshold allows inbound traffic again. Termination: Immediately block inbound traffic to old container, allow 60 seconds delay before termination to ensure in-flight user requests complete grac","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:2:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Optimization Solution: Create a /healthz endpoint for accurate health checks. Upgrade from tcpSocket to httpGet health checks for precise assessments. For more detailed health check endpoint /healthz design, please refer to the article: Health Check Endpoint /healthz Design Practice. Kubernetes deployment configurations enables the /healthz endpoint: --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp namespace: default spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # default: 30 terminationGracePeriodSeconds: 120 imagePullSecrets: - name: mysecret containers: - name: myapp image: registry.example.com/myapp:1.0 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 resources: requests: cpu: 500m memory: 1Gi limits: cpu: 500m memory: 1Gi env: - name: TZ value: Asia/Shanghai startupProbe: tcpSocket: port: 8080 # default: 0 initialDelaySeconds: 10 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 10 # default: 3 failureThreshold: 30 # default: 1 # cannot be changed, must be 1 by design successThreshold: 1 # default: 1 timeoutSeconds: 2 livenessProbe: httpGet: path: /healthz port: 8080 # default: 0 initialDelaySeconds: 10 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 30 # default: 3 failureThreshold: 5 # default: 1 # cannot be changed, must be 1 by design successThreshold: 1 # default: 1 timeoutSeconds: 5 readinessProbe: httpGet: path: /healthz port: 8080 # default: 0 initialDelaySeconds: 5 # default: 10 # fixed interval, does not wait for the previous probe to complete periodSeconds: 20 # default: 3 failureThreshold: 3 # default: 1 # set to 2 to avoid mistaking unstable new containers as ready successThreshold: 2 # default: 1 timeoutSeconds: 2 lifecycle: # delay container stop by 60 seconds preStop: exec: command: [\"/bin/sh\", \"-c\", \"sleep 60\"] ","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:3:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Graceful Draining Container health checks ensure that Pods are running properly, while graceful termination allows Pods to delay shutdown. Typically, user requests are routed through an Ingress. However, some Ingress controllers such as Alibaba Cloud Kubernetes may remove Pods from backend pools before the Pod fully Terminated. To avoid interrupting in-flight user requests, the Ingress should be configured with a graceful connection drain timeout that aligns with the container lifecycle.preStop. This ensures the Ingress keeps connections alive until the timeout expires. Alibaba Cloud Kubernetes graceful draining configuration: --- apiVersion: v1 kind: Service metadata: namespace: default name: myapp-svc spec: selector: app: myapp clusterIP: None ports: - port: 8080 targetPort: 8080 protocol: TCP --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: default name: myapp-ingress annotations: # enable graceful draining alb.ingress.kubernetes.io/connection-drain-enabled: \"true\" # match lifecycle.preStop alb.ingress.kubernetes.io/connection-drain-timeout: \"60\" spec: ingressClassName: alb rules: - host: example.com http: paths: - path: /myapp pathType: Prefix backend: service: name: myapp-svc port: number: 8080 ","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:4:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Skills"],"content":"Reference https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ https://help.aliyun.com/zh/ack/serverless-kubernetes/user-guide/advanced-alb-ingress-settings#c5bf22507239t ","date":"2024-07-03","objectID":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/:5:0","tags":["Kubernetes"],"title":"Kubernetes Container Healthcheck and Graceful Termination","uri":"/en/2024/07/kubernetes-container-healthcheck-and-graceful-termination/"},{"categories":["Notes"],"content":"B=MAP, which means 'Behavior = Motivation + Ability + Prompt'. By breaking down a goal into multiple small habits, it makes what seems to be an impossible goal achievable.","date":"2024-03-01","objectID":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/","tags":["Self-management"],"title":"Notes from \"Tiny Habits: The Small Changes That Change Everything\"","uri":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/"},{"categories":["Notes"],"content":"B=MAP, which means “Behavior = Motivation + Ability + Prompt”. By breaking down a goal into multiple small habits, it makes what seems to be an impossible goal achievable. ","date":"2024-03-01","objectID":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/:0:0","tags":["Self-management"],"title":"Notes from \"Tiny Habits: The Small Changes That Change Everything\"","uri":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/"},{"categories":["Notes"],"content":"Preface Dr. B.J. Fogg runs Stanford University’s Behavior Design Lab and created the field of behavior design. He has been studying how people behave for over 20 years. In this book, Dr. Fogg breaks down his behavior model and shows us the three things that make any behavior happen: Behavior = Motivation + Ability + Prompt. He explains how each part works and gives us practical ways to use them to change our habits. ","date":"2024-03-01","objectID":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/:1:0","tags":["Self-management"],"title":"Notes from \"Tiny Habits: The Small Changes That Change Everything\"","uri":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/"},{"categories":["Notes"],"content":"Summary This book offers a systematic approach to habit formation that differs fundamentally from traditional methods. The core principle is the B=MAP formula: behavior occurs when motivation, ability, and prompt converge simultaneously. Here are the key insights of behavior change: Motivation is unreliable: The common approach of relying on willpower and motivation fails because motivation fluctuates constantly. Rather than fighting this natural variation, effective change focuses on helping people do what they already want to do. Start with micro-behaviors: Instead of attempting dramatic lifestyle changes, begin with actions so small they seem almost trivial. For example, if you want to exercise regularly, start by simply putting on workout clothes. When behaviors require minimal effort, resistance naturally decreases. Design effective prompts: Without proper triggers, even highly motivated and capable individuals fail to act. The most reliable prompts are anchored to existing daily routines, such as “after I brush my teeth, I will floss one tooth.” Immediate celebration is crucial: Instant positive reinforcement creates the emotional connection necessary for habit formation. The brain needs to associate the new behavior with positive feelings to encode it as automatic. Small victories create momentum: Success generates more success through increased confidence and motivation. Frequent small wins are more powerful than occasional large achievements in building long-term behavioral patterns. The book includes comprehensive practical resources, including the Behavior Design Toolbox with ready-to-use habit recipes covering various life domains from sleep improvement to relationship building. These templates provide concrete starting points for implementing the methodology. What makes Fogg’s approach particularly compelling is how it challenges the traditional “all or nothing” mindset. Instead of relying on willpower to force change, the method makes new behaviors so accessible and rewarding that they integrate naturally into daily life. This represents more than just habit formation techniques - it’s a fundamental shift in how we approach personal development and behavioral change. ","date":"2024-03-01","objectID":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/:2:0","tags":["Self-management"],"title":"Notes from \"Tiny Habits: The Small Changes That Change Everything\"","uri":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/"},{"categories":["Notes"],"content":"Thinking Reading this book was honestly eye-opening for me. I finally understood why so many of my past attempts at building habits had failed. Looking back, I was always counting on motivation to carry me through, not realizing that the B=MAP formula shows motivation is actually the weakest link in the chain. What really struck me was how this approach works with human nature instead of fighting against it. For years, I’d been my own worst critic, thinking that beating myself up would somehow motivate better behavior. I now realize this was completely backwards. The biggest revelation for me was understanding that all that negative self-talk and “just push through it” mentality was actually working against my efforts. My brain doesn’t respond well to constant criticism - it responds to feeling good about progress, no matter how small. This has completely changed how I think about personal development. Instead of setting these massive, intimidating goals that make me feel overwhelmed before I even start, I’ll break everything down into tiny steps and actually celebrate when I complete them. The shift from “I need more willpower” to “I need better design” feels transformative. It’s not about being stronger or more disciplined - it’s about being smarter about how I set myself up for success. I’m going to start applying this approach to several areas of my life. ","date":"2024-03-01","objectID":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/:3:0","tags":["Self-management"],"title":"Notes from \"Tiny Habits: The Small Changes That Change Everything\"","uri":"/en/2024/03/notes-from-tiny-habits-the-small-changes-that-change-everything/"},{"categories":["Skills"],"content":"I collected a complete set of Alibaba Cloud vector icons and generated the draw.io icon library.","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"I collected a complete set of Alibaba Cloud vector icons and generated the draw.io icon library. ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:0:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Background In many well-known diagramming software (such as draw.io, Lucidchart, Gliffy, ProcessOn, etc.), there is no built-in Alibaba Cloud official vector icons. So I collected a complete set of Alibaba Cloud vector icons and generated the draw.io icon library. ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:1:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Screenshot of draw.io Custom library Icons ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:2:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Download draw.io Custom Library Alibaba Cloud.xml ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:3:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Sources of Alibaba Cloud Vector Icons Links: Alibaba Cloud Design Center | Categories of Alibaba Cloud Products Color Code: #ff6a00 ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:4:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Order of Icons in draw.io . ├── 00 Logo ├── 01 Networking and CDN ├── 02 Computing ├── 03 Container ├── 04 Serverless ├── 05 Database ├── 06 Storage ├── 07 Middleware ├── 08 Migration and Operations Management ├── 09 Developer Tools ├── 10 Enterprise Services and Cloud Communication ├── 11 Security ├── 12 Analytics Computing ├── 13 AI and Machine Learning ├── 14 Internet of Things └── 15 Media Services ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:5:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"Followup I have brought some changes to the fact. We will soon see the built-in Alibaba Cloud official vector icons in draw.io.😄 https://github.com/jgraph/drawio/issues/4086 ","date":"2024-01-04","objectID":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/:6:0","tags":["Alibaba Cloud"],"title":"Alibaba Cloud Official Vector Icons and draw.io Custom Libraries","uri":"/en/2024/01/alibaba-cloud-official-vector-icons-and-drawio-custom-libraries/"},{"categories":["Skills"],"content":"git-crypt is a tool for encrypting and decrypting files that integrates seamlessly with Git repositories, allowing users to securely store and share sensitive data.","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"git-crypt is a tool for encrypting and decrypting files that integrates seamlessly with Git repositories, allowing users to securely store and share sensitive data. ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:0:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Install packages Install git-crypt and gnupg ❯ brew install git-crypt gnupg ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:1:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Create directories Create required directories to simulate multi-user collaboration ❯ mkdir -p /Users/damonguo/Workspace/demo/{repo,gnupg_keys,tmp} ❯ cd /Users/damonguo/Workspace/demo/ ❯ ls -1 gnupg_keys repo tmp ❯ mkdir -p /Users/damonguo/Workspace/demo/gnupg_keys/{alice,bob,jack} ❯ chmod 700 /Users/damonguo/Workspace/demo/gnupg_keys/{alice,bob,jack} ❯ ls -l /Users/damonguo/Workspace/demo/gnupg_keys/ total 0 drwx------@ 2 damonguo staff 64 Jan 28 11:28 alice drwx------@ 2 damonguo staff 64 Jan 28 11:28 bob drwx------@ 2 damonguo staff 64 Jan 28 11:28 jack ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:2:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Clone Git repository Clone the demo repo for user Alice and initialize the git-crypt and secret.txt ❯ cd repo ❯ git clone git@github.com:mcsrainbow/git-crypt-demo.git ❯ mv git-crypt-demo git-crypt-demo_alice ❯ cd git-crypt-demo_alice ❯ git-crypt init Generating key... ❯ echo \"secret.txt filter=git-crypt diff=git-crypt\" \u003e .gitattributes ❯ cat .gitattributes secret.txt filter=git-crypt diff=git-crypt ❯ git add .gitattributes ❯ git commit -m \"chore: configure git-crypt\" [main f50794d] chore: configure git-crypt 1 file changed, 1 insertion(+) create mode 100644 .gitattributes ❯ echo \"TOP SECRET from Alice\" \u003e secret.txt ❯ cat secret.txt TOP SECRET from Alice ❯ git add secret.txt ❯ git commit -m \"feat: add encrypted secret\" [main e7f174b] feat: add encrypted secret 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 secret.txt ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:3:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Generate GPG keys Generate GPG keys for Alice, Bob and Jack ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/alice ❯ gpg --batch --passphrase '' --quick-generate-key \"Alice \u003calice@example.com\u003e\" rsa4096 sign,encrypt 10y gpg: keybox '/Users/damonguo/Workspace/demo/gnupg_keys/alice/pubring.kbx' created gpg: /Users/damonguo/Workspace/demo/gnupg_keys/alice/trustdb.gpg: trustdb created gpg: directory '/Users/damonguo/Workspace/demo/gnupg_keys/alice/openpgp-revocs.d' created gpg: revocation certificate stored as '/Users/damonguo/Workspace/demo/gnupg_keys/alice/openpgp-revocs.d/4EC36059B196849885E438499BAF6C28C32813A4.rev' ❯ ls /Users/damonguo/Workspace/demo/gnupg_keys/alice openpgp-revocs.d pubring.kbx S.gpg-agent S.gpg-agent.extra trustdb.gpg private-keys-v1.d pubring.kbx~ S.gpg-agent.browser S.gpg-agent.ssh ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/bob ❯ gpg --batch --passphrase '' --quick-generate-key \"Bob \u003cbob@example.com\u003e\" rsa4096 sign,encrypt 10y gpg: keybox '/Users/damonguo/Workspace/demo/gnupg_keys/bob/pubring.kbx' created gpg: /Users/damonguo/Workspace/demo/gnupg_keys/bob/trustdb.gpg: trustdb created gpg: directory '/Users/damonguo/Workspace/demo/gnupg_keys/bob/openpgp-revocs.d' created gpg: revocation certificate stored as '/Users/damonguo/Workspace/demo/gnupg_keys/bob/openpgp-revocs.d/B2E7F419BACD36339E1FECE34636D98424360512.rev' ❯ mkdir GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/jack ❯ gpg --batch --passphrase '' --quick-generate-key \"Jack \u003cjack@example.com\u003e\" rsa4096 sign,encrypt 10y gpg: keybox '/Users/damonguo/Workspace/demo/gnupg_keys/jack/pubring.kbx' created gpg: /Users/damonguo/Workspace/demo/gnupg_keys/jack/trustdb.gpg: trustdb created gpg: directory '/Users/damonguo/Workspace/demo/gnupg_keys/jack/openpgp-revocs.d' created gpg: revocation certificate stored as '/Users/damonguo/Workspace/demo/gnupg_keys/jack/openpgp-revocs.d/4D1DED527513BE77A3546955107E34937CCFEA8E.rev' ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:4:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Grant permissions Export the GPG public key of Bob for Alice to import ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/bob ❯ gpg --armor --export bob@example.com \u003e /Users/damonguo/Workspace/demo/tmp/bob.pub ❯ file /Users/damonguo/Workspace/demo/tmp/bob.pub /Users/damonguo/Workspace/demo/tmp/bob.pub: PGP public key block Public-Key Add Alice and Bob as git-crypt collaborators and push to GitHub ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/alice ❯ git-crypt add-gpg-user alice@example.com gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: next trustdb check due at 2036-01-26 [main 46edf42] Add 1 git-crypt collaborator 2 files changed, 4 insertions(+) create mode 100644 .git-crypt/.gitattributes create mode 100644 .git-crypt/keys/default/0/4EC36059B196849885E438499BAF6C28C32813A4.gpg ❯ gpg --import /Users/damonguo/Workspace/demo/tmp/bob.pub gpg: key 4636D98424360512: public key \"Bob \u003cbob@example.com\u003e\" imported gpg: Total number processed: 1 gpg: imported: 1 ❯ gpg --batch --yes --lsign-key bob@example.com pub rsa4096/4636D98424360512 created: 2026-01-28 expires: 2036-01-26 usage: SCE trust: unknown validity: unknown [ unknown] (1). Bob \u003cbob@example.com\u003e pub rsa4096/4636D98424360512 created: 2026-01-28 expires: 2036-01-26 usage: SCE trust: unknown validity: unknown Primary key fingerprint: B2E7 F419 BACD 3633 9E1F ECE3 4636 D984 2436 0512 Bob \u003cbob@example.com\u003e This key is due to expire on 2036-01-26. Are you sure that you want to sign this key with your key \"Alice \u003calice@example.com\u003e\" (9BAF6C28C32813A4) The signature will be marked as non-exportable. ❯ git-crypt add-gpg-user bob@example.com gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 1 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: depth: 1 valid: 1 signed: 0 trust: 1-, 0q, 0n, 0m, 0f, 0u gpg: next trustdb check due at 2036-01-26 [main c09d72d] Add 1 git-crypt collaborator 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 .git-crypt/keys/default/0/B2E7F419BACD36339E1FECE34636D98424360512.gpg ❯ git-crypt status not encrypted: .git-crypt/.gitattributes not encrypted: .git-crypt/keys/default/0/4EC36059B196849885E438499BAF6C28C32813A4.gpg not encrypted: .git-crypt/keys/default/0/B2E7F419BACD36339E1FECE34636D98424360512.gpg not encrypted: .gitattributes not encrypted: README.md encrypted: secret.txt ❯ git push origin main Enumerating objects: 22, done. Counting objects: 100% (22/22), done. Delta compression using up to 10 threads Compressing objects: 100% (15/15), done. Writing objects: 100% (21/21), 3.28 KiB | 1.64 MiB/s, done. Total 21 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0) remote: Resolving deltas: 100% (3/3), done. To github.com:mcsrainbow/git-crypt-demo.git 3beff32..c09d72d main -\u003e main ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:5:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"View encrypted file View encrypted secret.txt as View raw on GitHub ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:6:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Verify permissions Bob cannot view the encrypted secret.txt without unlock ❯ cd .. ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/bob ❯ git clone git@github.com:mcsrainbow/git-crypt-demo.git ❯ mv git-crypt-demo git-crypt-demo_bob ❯ cd git-crypt-demo_bob ❯ file secret.txt secret.txt: data ❯ cat secret.txt GITCRYPT Jack cannot unlock the encrypted secret.txt without being granted ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/jack ❯ git-crypt unlock gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: next trustdb check due at 2036-01-26 Error: no GPG secret key available to unlock this repository. To unlock with a shared symmetric key instead, specify the path to the symmetric key as an argument to 'git-crypt unlock'. Bob can unlock and lock the secret.txt ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/bob ❯ git-crypt unlock gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u gpg: next trustdb check due at 2036-01-26 ❯ cat secret.txt TOP SECRET from Alice ❯ git-crypt lock ❯ cat secret.txt GITCRYPT ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:7:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Remove permissions Bob can remove the Alice GPG key from repo ❯ git log | grep 'Add 1 git-crypt collaborator' -A5 | grep -B1 alice@example.com 4EC36059B196849885E438499BAF6C28C32813A4 Alice \u003calice@example.com\u003e ❯ rm .git-crypt/keys/default/0/4EC36059B196849885E438499BAF6C28C32813A4.gpg ❯ git add -u ❯ git commit -m \"chore: remove git-crypt access for Alice\" [main bf48d82] chore: remove git-crypt access for Alice 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 .git-crypt/keys/default/0/4EC36059B196849885E438499BAF6C28C32813A4.gpg ❯ git push Alice cannot unlock the secret.txt after re-cloning the repository ❯ cd .. ❯ export GNUPGHOME=/Users/damonguo/Workspace/demo/gnupg_keys/alice ❯ git clone git@github.com:mcsrainbow/git-crypt-demo.git ❯ mv git-crypt-demo git-crypt-demo_noalice ❯ cd git-crypt-demo_noalice ❯ git-crypt status not encrypted: .git-crypt/.gitattributes not encrypted: .git-crypt/keys/default/0/B2E7F419BACD36339E1FECE34636D98424360512.gpg not encrypted: .gitattributes not encrypted: README.md encrypted: secret.txt ❯ cat secret.txt GITCRYPT ❯ git-crypt unlock Error: no GPG secret key available to unlock this repository. To unlock with a shared symmetric key instead, specify the path to the symmetric key as an argument to 'git-crypt unlock'. ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:8:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Complete journey View the complete git commits ❯ git log commit bf48d82e3ce40e237465e2af608b0ff990eb6735 (HEAD -\u003e main, origin/main, origin/HEAD) Author: mcsrainbow \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:44:31 2026 +0800 chore: remove git-crypt access for Alice commit c09d72da79784054ebf2b2b8c5d5bb0a878c02c0 Author: mcsrainbow \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:39:44 2026 +0800 Add 1 git-crypt collaborator New collaborators: B2E7F419BACD36339E1FECE34636D98424360512 Bob \u003cbob@example.com\u003e commit 46edf42fc7ca6a658c874ae196ba4f75ac2d2aff Author: mcsrainbow \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:36:25 2026 +0800 Add 1 git-crypt collaborator New collaborators: 4EC36059B196849885E438499BAF6C28C32813A4 Alice \u003calice@example.com\u003e commit e7f174b8cd6b10fc5837b89892481ed08b139398 Author: mcsrainbow \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:31:56 2026 +0800 feat: add encrypted secret commit f50794db3651b72ee56c91bbdb25a021a768c691 Author: mcsrainbow \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:31:40 2026 +0800 chore: configure git-crypt commit 3beff32096f2b50d2f012c6ba5e68cba1a0af5fb Author: Dong Guo / Damon \u003cguosuiyu@gmail.com\u003e Date: Wed Jan 28 11:23:25 2026 +0800 Initial commit ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:9:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"References https://buddy.works/guides/git-crypt ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:10:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Recommended A solution with more convenient private key management, more intuitive operations, and easier integration with CI: Using SOPS + AGE to Encrypt Files ","date":"2023-12-18","objectID":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/:11:0","tags":["Git","Security"],"title":"Using git-crypt to Encrypt Files in Git","uri":"/en/2023/12/using-git-crypt-to-encrypt-files-in-git/"},{"categories":["Skills"],"content":"Deploying and managing native Kubernetes clusters could be challenging, so the community developed lightweight Kubernetes distributions to make operations easier.","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploying and managing native Kubernetes clusters could be challenging, so the community developed lightweight Kubernetes distributions to make operations easier. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:0:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using Minikube with Podman Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Podman is an open-source container runtime tool offering functionality similar to Docker, but operates without a daemon and supports enhanced security features and rootless mode execution. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Driver: Podman CPUs: 2 Memory: 2Gi Disk: 20GiB Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Podman ➜ brew install podman ==\u003e Installing dependencies for podman: capstone, dtc, pcre2, gettext, glib, gmp, libtasn1, nettle, p11-kit, openssl@3, libnghttp2, unbound, gnutls, jpeg-turbo, libpng, libslirp, libssh, libusb, lzo, pixman, snappy, vde and qemu ==\u003e podman ➜ podman machine init --cpus 2 --memory 2048 --disk-size 20 --rootful Downloading VM image: fedora-coreos-39.20231204.2.1-qemu.aarch64.qcow2.xz: done Extracting compressed file: podman-machine-default_fedora-coreos-39.20231204.2.1-qemu.aarch64.qcow2: done Image resized. Machine init complete ➜ podman machine start Starting machine \"podman-machine-default\" Waiting for VM ... Mounting volume... /Users:/Users Mounting volume... /private:/private Mounting volume... /var/folders:/var/folders API forwarding listening on: /Users/damonguo/.local/share/containers/podman/machine/qemu/podman.sock The system helper service is not installed; the default Docker API socket address can't be used by podman. If you would like to install it, run the following commands: sudo /opt/homebrew/Cellar/podman/4.8.1/bin/podman-mac-helper install podman machine stop; podman machine start You can still connect Docker API clients by setting DOCKER_HOST using the following command in your terminal session: export DOCKER_HOST='unix:///Users/damonguo/.local/share/containers/podman/machine/qemu/podman.sock' Machine \"podman-machine-default\" started successfully ➜ podman machine list NAME VM TYPE CREATED LAST UP CPUS MEMORY DISK SIZE podman-machine-default qemu 8 minutes ago Currently running 2 2GiB 20GiB ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Minikube ➜ brew install minikube ==\u003e Installing dependencies for minikube: kubernetes-cli ==\u003e minikube ➜ minikube config set driver podman These changes will take effect upon a minikube delete and then a minikube start ➜ minikube start --driver=podman --kubernetes-version=v1.28.3 minikube v1.32.0 on Darwin 14.1.2 (arm64) Using the podman (experimental) driver based on user configuration Using Podman driver with root privileges Starting control plane node minikube in cluster minikube Pulling base image ... Downloading Kubernetes v1.28.3 preload ... preloaded-images-k8s-v18-v1...: 341.16 MiB / 341.16 MiB 100.00% 13.99 M gcr.io/k8s-minikube/kicbase...: 410.58 MiB / 410.58 MiB 100.00% 13.22 M Creating podman container (CPUs=2, Memory=1887MB) ... Preparing Kubernetes v1.28.3 on Docker 24.0.7 ... Generating certificates and keys ... Booting up control plane ... Configuring RBAC rules ... Configuring bridge CNI (Container Networking Interface) ... Verifying Kubernetes components... Using image gcr.io/k8s-minikube/storage-provisioner:v5 Enabled addons: storage-provisioner, default-storageclass Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default ➜ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured ➜ kubectl get namespaces NAME STATUS AGE default Active 4m40s kube-node-lease Active 4m40s kube-public Active 4m40s kube-system Active 4m40s ➜ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane 4m44s v1.28.3 ➜ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 16m ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ vim nginx-deploy-svc.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 ➜ kubectl apply -f nginx-deploy-svc.yaml deployment.apps/nginx-deploy created service/nginx-svc created ➜ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-7c5ddbdf54-4d8c2 1/1 Running 0 67s pod/nginx-deploy-7c5ddbdf54-cmcg2 1/1 Running 0 67s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 24m service/nginx-svc NodePort 10.103.72.229 \u003cnone\u003e 80:31985/TCP 67s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 67s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-7c5ddbdf54 2 2 2 67s ➜ minikube service nginx-svc --url http://127.0.0.1:51726 Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Access Nginx via Tunnel: http://127.0.0.1:51726 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup Minikube and Podman ➜ minikube stop Stopping node \"minikube\" ... Powering off \"minikube\" via SSH ... 1 node stopped. ➜ minikube status minikube type: Control Plane host: Stopped kubelet: Stopped apiserver: Stopped kubeconfig: Stopped ➜ minikube delete Deleting \"minikube\" in podman ... Deleting container \"minikube\" ... Removing /Users/damonguo/.minikube/machines/minikube ... Removed all traces of the \"minikube\" cluster. ➜ podman machine list NAME VM TYPE CREATED LAST UP CPUS MEMORY DISK SIZE podman-machine-default qemu 54 minutes ago Currently running 2 2GiB 20GiB ➜ podman machine stop podman-machine-default Waiting for VM to exit... Machine \"podman-machine-default\" stopped successfully ➜ podman machine rm podman-machine-default The following files will be deleted: /Users/damonguo/.ssh/podman-machine-default /Users/damonguo/.ssh/podman-machine-default.pub /Users/damonguo/.config/containers/podman/machine/qemu/podman-machine-default.ign /Users/damonguo/.local/share/containers/podman/machine/qemu/podman-machine-default_fedora-coreos-39.20231204.2.1-qemu.aarch64.qcow2 /Users/damonguo/.local/share/containers/podman/machine/qemu/podman.sock /Users/damonguo/.local/share/containers/podman/machine/qemu/podman-machine-default_ovmf_vars.fd /Users/damonguo/.config/containers/podman/machine/qemu/podman-machine-default.json Are you sure you want to continue? [y/N] y ➜ minikube delete --purge --all Successfully deleted all profiles Successfully purged minikube directory located at - [/Users/damonguo/.minikube] ➜ brew uninstall minikube ➜ brew uninstall podman ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:1:5","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using KinD KinD (Kubernetes in Docker) is a tool for running local Kubernetes clusters in Docker. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:2:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Driver: Docker Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:2:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup KinD ➜ brew install kind ==\u003e Installing kind ==\u003e Pouring kind--0.20.0.arm64_sonoma.bottle.tar.gz ==\u003e kind ➜ kind version kind v0.20.0 go1.21.1 darwin/arm64 Create a cluster ‘mycluster’, allow the local host to make requests to the Ingress controller over port 30080. ➜ vim config-with-port-mapping.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30080 hostPort: 30080 ➜ kind create cluster --name mycluster --config=config-with-port-mapping.yaml Creating cluster \"mycluster\" ... ✓ Ensuring node image (kindest/node:v1.27.3) ✓ Preparing nodes ✓ Writing configuration ✓ Starting control-plane ✓ Installing CNI ✓ Installing StorageClass Set kubectl context to \"kind-mycluster\" You can now use your cluster with: kubectl cluster-info --context kind-mycluster Thanks for using kind! ➜ kubectl cluster-info --context kind-mycluster Kubernetes control plane is running at https://127.0.0.1:64070 CoreDNS is running at https://127.0.0.1:64070/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy ➜ kind get clusters mycluster ➜ kubectl get nodes NAME STATUS ROLES AGE VERSION mycluster-control-plane Ready control-plane 85s v1.27.3 ➜ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-5d78c9869d-6zdpz 1/1 Running 0 81s coredns-5d78c9869d-twr96 1/1 Running 0 81s etcd-mycluster-control-plane 1/1 Running 0 94s kindnet-x9zrb 1/1 Running 0 81s kube-apiserver-mycluster-control-plane 1/1 Running 0 96s kube-controller-manager-mycluster-control-plane 1/1 Running 0 94s kube-proxy-5zzch 1/1 Running 0 81s kube-scheduler-mycluster-control-plane 1/1 Running 0 94s ➜ kubectl get namespaces NAME STATUS AGE default Active 108s kube-node-lease Active 108s kube-public Active 108s kube-system Active 108s local-path-storage Active 104s ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:2:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 119s ➜ vim nginx-deploy-svc-portmapping.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 nodePort: 30080 ➜ kubectl apply -f nginx-deploy-svc-portmapping.yaml deployment.apps/nginx-deploy created service/nginx-svc created ➜ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-55f598f8d-f2c2q 1/1 Running 0 35s pod/nginx-deploy-55f598f8d-ljxd8 1/1 Running 0 35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 5m5s service/nginx-svc NodePort 10.96.221.64 \u003cnone\u003e 80:30080/TCP 35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 35s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-55f598f8d 2 2 2 35s Access Nginx via NodePort Local PortMapping: http://localhost:30080 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:2:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup KinD ➜ kind delete cluster --name mycluster Deleting cluster \"mycluster\" ... Deleted nodes: [\"mycluster-control-plane\"] ➜ kind get clusters No kind clusters found. ➜ kubectl config delete-context kind-mycluster deleted context kind-mycluster from /Users/damonguo/.kube/config ➜ brew uninstall kind ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:2:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using K3S with Multipass K3S is a lightweight, easy-to-install distribution of Kubernetes. Multipass is a tool for quickly creating, managing, and operating Ubuntu virtual machines, by Canonical, the company behind Ubuntu. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Virtualization: Multipass CPUs: 1 Memory: 1Gi Disk: 10GiB Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Multipass ➜ brew install --cask multipass ==\u003e Downloading https://github.com/canonical/multipass/releases/download/v1.12.2/multipass-1.12.2+mac-Darwin.pkg ==\u003e Installing Cask multipass installer: Package name is multipass installer: Installing at base path / installer: The install was successful. multipass was successfully installed! ➜ multipass launch --name k3s-server --cpus 1 --memory 1G --disk 10G Launched: k3s-server ➜ multipass info k3s-server Name: k3s-server State: Running IPv4: 192.168.64.2 Release: Ubuntu 22.04.3 LTS Image hash: 9256911742f0 (Ubuntu 22.04 LTS) CPU(s): 1 Load: 0.56 0.15 0.05 Disk usage: 1.4GiB out of 9.6GiB Memory usage: 140.0MiB out of 962.3MiB Mounts: -- ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup K3S ➜ multipass shell k3s-server Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-89-generic aarch64) ubuntu@k3s-server:~$ curl -sfL https://get.k3s.io | sh - [INFO] Finding release for channel stable [INFO] Using v1.27.7+k3s2 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.27.7+k3s2/sha256sum-arm64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.27.7+k3s2/k3s-arm64 [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s.service [INFO] systemd: Enabling k3s unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service. [INFO] systemd: Starting k3s ubuntu@k3s-server:~$ sudo k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-server Ready control-plane,master 19s v1.27.7+k3s2 ubuntu@k3s-server:~$ sudo ss -lntpu | grep k3s-server tcp LISTEN 0 4096 127.0.0.1:10248 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=172)) tcp LISTEN 0 4096 127.0.0.1:10249 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=208)) tcp LISTEN 0 4096 127.0.0.1:6444 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=15)) tcp LISTEN 0 4096 127.0.0.1:10256 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=206)) tcp LISTEN 0 4096 127.0.0.1:10257 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=86)) tcp LISTEN 0 4096 127.0.0.1:10258 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=202)) tcp LISTEN 0 4096 127.0.0.1:10259 0.0.0.0:* users:((\"k3s-server\",pid=2786,fd=209)) tcp LISTEN 0 4096 *:10250 *:* users:((\"k3s-server\",pid=2786,fd=168)) tcp LISTEN 0 4096 *:6443 *:* users:((\"k3s-server\",pid=2786,fd=13)) ubuntu@k3s-server:~$ sudo cat /var/lib/rancher/k3s/server/node-token K10fa8d62310e361852c7607ba12b9667cd05f52122df80ca928448200295bb0969::server:c421b343a4f042a2a3511156664a76b1 ubuntu@k3s-server:~$ exit logout ➜ multipass launch --name k3s-agent --cpus 1 --memory 1G --disk 10G Launched: k3s-agent ➜ multipass list Name State IPv4 Image k3s-server Running 192.168.64.2 Ubuntu 22.04 LTS 10.42.0.0 10.42.0.1 k3s-agent Running 192.168.64.3 Ubuntu 22.04 LTS ➜ multipass shell k3s-agent Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-89-generic aarch64) ubuntu@k3s-agent:~$ curl -sfL https://get.k3s.io | K3S_URL=https://192.168.64.2:6443 K3S_TOKEN=\"K10fa8d62310e361852c7607ba12b9667cd05f52122df80ca928448200295bb0969::server:c421b343a4f042a2a3511156664a76b1\" sh - [INFO] Finding release for channel stable [INFO] Using v1.27.7+k3s2 as release [INFO] Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.27.7+k3s2/sha256sum-arm64.txt [INFO] Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.27.7+k3s2/k3s-arm64 [INFO] Verifying binary download [INFO] Installing k3s to /usr/local/bin/k3s [INFO] Skipping installation of SELinux RPM [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s [INFO] Creating killall script /usr/local/bin/k3s-killall.sh [INFO] Creating uninstall script /usr/local/bin/k3s-agent-uninstall.sh [INFO] env: Creating environment file /etc/systemd/system/k3s-agent.service.env [INFO] systemd: Creating service file /etc/systemd/system/k3s-agent.service [INFO] systemd: Enabling k3s-agent unit Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service. [INFO] systemd: Starting k3s-agent ubuntu@k3s-agent:~$ exit logout ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ multipass shell k3s-server Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-89-generic aarch64) ubuntu@k3s-server:~$ sudo k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-server Ready control-plane,master 9m26s v1.27.7+k3s2 k3s-agent Ready \u003cnone\u003e 71s v1.27.7+k3s2 ubuntu@k3s-server:~$ vim nginx-deploy-svc.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 ubuntu@k3s-server:~$ sudo k3s kubectl apply -f nginx-deploy-svc.yaml deployment.apps/nginx-deploy created service/nginx-svc created ubuntu@k3s-server:~$ sudo k3s kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-55f598f8d-pzr6n 1/1 Running 0 20m pod/nginx-deploy-55f598f8d-z55ng 1/1 Running 0 20m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u003cnone\u003e 443/TCP 68m service/nginx-svc NodePort 10.43.202.7 \u003cnone\u003e 80:32711/TCP 20m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 20m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-55f598f8d 2 2 2 20m ubuntu@k3s-server:~$ exit logout ➜ multipass list Name State IPv4 Image k3s-server Running 192.168.64.2 Ubuntu 22.04 LTS 10.42.0.0 10.42.0.1 k3s-agent Running 192.168.64.3 Ubuntu 22.04 LTS 10.42.1.0 10.42.1.1 Access Nginx via NodePort: http://192.168.64.2:32711 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup Multipass and K3S ➜ multipass delete k3s-server k3s-agent ➜ multipass list Name State IPv4 Image k3s-server Deleted -- Not Available k3s-agent Deleted -- Not Available ➜ multipass purge ➜ multipass list No instances found. ➜ brew uninstall --cask multipass ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:3:5","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using K3D K3D is a lightweight wrapper to run K3S in docker. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:4:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Driver: Docker Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:4:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup K3D ➜ brew install k3d ==\u003e Installing k3d ==\u003e Pouring k3d--5.6.0.arm64_sonoma.bottle.tar.gz ==\u003e k3d ➜ k3d version k3d version v5.6.0 k3s version v1.27.5-k3s1 (default) Create a cluster ‘mycluster’, mapping the ingress port 80 to localhost:8081. ➜ k3d cluster create mycluster -p \"8081:80@loadbalancer\" --agents 1 INFO[0000] portmapping '8081:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] INFO[0000] Prep: Network INFO[0000] Created network 'k3d-mycluster' INFO[0000] Created image volume k3d-mycluster-images INFO[0000] Starting new tools node... INFO[0000] Starting Node 'k3d-mycluster-tools' INFO[0001] Creating node 'k3d-mycluster-server-0' INFO[0001] Creating node 'k3d-mycluster-agent-0' INFO[0001] Creating LoadBalancer 'k3d-mycluster-serverlb' INFO[0001] Using the k3d-tools node to gather environment information INFO[0001] HostIP: using network gateway 192.168.167.1 address INFO[0001] Starting cluster 'mycluster' INFO[0001] Starting servers... INFO[0001] Starting Node 'k3d-mycluster-server-0' INFO[0004] Starting agents... INFO[0004] Starting Node 'k3d-mycluster-agent-0' INFO[0007] Starting helpers... INFO[0007] Starting Node 'k3d-mycluster-serverlb' INFO[0013] Injecting records for hostAliases (incl. host.k3d.internal) and for 3 network members into CoreDNS configmap... INFO[0015] Cluster 'mycluster' created successfully! INFO[0015] You can now use it like this: kubectl cluster-info ➜ kubectl cluster-info Kubernetes control plane is running at https://0.0.0.0:56685 CoreDNS is running at https://0.0.0.0:56685/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://0.0.0.0:56685/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:4:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u003cnone\u003e 443/TCP 48s ➜ vim nginx-deploy-svc-ingress.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ingress annotations: ingress.kubernetes.io/ssl-redirect: \"false\" spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-svc port: number: 80 ➜ kubectl apply -f nginx-deploy-svc-ingress.yaml deployment.apps/nginx-deploy created service/nginx-svc created ingress.networking.k8s.io/nginx-ingress created ➜ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-55f598f8d-z9n5v 1/1 Running 0 2m pod/nginx-deploy-55f598f8d-h5zkb 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.43.0.1 \u003cnone\u003e 443/TCP 3m12s service/nginx-svc NodePort 10.43.58.173 \u003cnone\u003e 80:32459/TCP 2m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 2m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-55f598f8d 2 2 2 2m ➜ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE nginx-ingress \u003cnone\u003e * 192.168.167.2,192.168.167.3 80 2m56s Access Nginx via Ingress: http://localhost:8081 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:4:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup K3D ➜ k3d cluster list NAME SERVERS AGENTS LOADBALANCER mycluster 1/1 1/1 true ➜ k3d cluster delete mycluster INFO[0000] Deleting cluster 'mycluster' INFO[0000] Deleting cluster network 'k3d-mycluster' INFO[0000] Deleting 1 attached volumes... INFO[0000] Removing cluster details from default kubeconfig... INFO[0000] Removing standalone kubeconfig file (if there is one)... INFO[0000] Successfully deleted cluster mycluster! ➜ k3d cluster list NAME SERVERS AGENTS LOADBALANCER ➜ brew uninstall k3d ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:4:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using MicroK8s with Multipass MicroK8s is a low-ops, minimal production Kubernetes by Canonical, the company behind Ubuntu. Multipass is a tool for quickly creating, managing, and operating Ubuntu virtual machines, by Canonical, the company behind Ubuntu. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Virtualization: Multipass Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Multipass ➜ brew install --cask multipass ==\u003e Downloading https://github.com/canonical/multipass/releases/download/v1.12.2/multipass-1.12.2+mac-Darwin.pkg ==\u003e Installing Cask multipass installer: Package name is multipass installer: Installing at base path / installer: The install was successful. multipass was successfully installed! ➜ multipass list No instances found. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup MicroK8s ➜ brew install ubuntu/microk8s/microk8s ==\u003e Tapping ubuntu/microk8s Cloning into '/opt/homebrew/Library/Taps/ubuntu/homebrew-microk8s'... ==\u003e Fetching ubuntu/microk8s/microk8s ==\u003e Installing microk8s from ubuntu/microk8s Run `microk8s install` to start with MicroK8s ➜ microk8s install Launched: microk8s-vm microk8s (1.28/stable) v1.28.15 from Canonical✓ installed microk8s-integrator-macos 0.1 from Canonical✓ installed MicroK8s is up and running. See the available commands with `microk8s --help`. ➜ microk8s status --wait-ready microk8s is running high-availability: no datastore master nodes: 127.0.0.1:19001 datastore standby nodes: none addons: enabled: dns # (core) CoreDNS ha-cluster # (core) Configure high availability on the current node helm # (core) Helm - the package manager for Kubernetes helm3 # (core) Helm 3 - the package manager for Kubernetes disabled: cert-manager # (core) Cloud native certificate management cis-hardening # (core) Apply CIS K8s hardening community # (core) The community addons repository dashboard # (core) The Kubernetes dashboard host-access # (core) Allow Pods connecting to Host services smoothly hostpath-storage # (core) Storage class; allocates storage from host directory ingress # (core) Ingress controller for external access kube-ovn # (core) An advanced network fabric for Kubernetes mayastor # (core) OpenEBS MayaStor metallb # (core) Loadbalancer for your Kubernetes cluster metrics-server # (core) K8s Metrics Server for API access to service metrics minio # (core) MinIO object storage observability # (core) A lightweight observability stack for logs, traces and metrics prometheus # (core) Prometheus operator for monitoring and logging rbac # (core) Role-Based Access Control for authorisation registry # (core) Private image registry exposed on localhost:32000 rook-ceph # (core) Distributed Ceph storage using Rook storage # (core) Alias to hostpath-storage add-on, deprecated ➜ multipass info microk8s-vm Name: microk8s-vm State: Running Snapshots: 0 IPv4: 192.168.64.6 10.1.254.64 Release: Ubuntu 22.04.5 LTS Image hash: 7b86a56f8069 (Ubuntu 22.04 LTS) CPU(s): 2 Load: 0.73 0.40 0.16 Disk usage: 2.9GiB out of 48.4GiB Memory usage: 624.6MiB out of 3.8GiB Mounts: -- Enable addon ingress ➜ microk8s enable ingress Infer repository core for addon ingress Enabling Ingress ingressclass.networking.k8s.io/public created ingressclass.networking.k8s.io/nginx created namespace/ingress created serviceaccount/nginx-ingress-microk8s-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-microk8s-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-microk8s-role created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created rolebinding.rbac.authorization.k8s.io/nginx-ingress-microk8s created configmap/nginx-load-balancer-microk8s-conf created configmap/nginx-ingress-tcp-microk8s-conf created configmap/nginx-ingress-udp-microk8s-conf created daemonset.apps/nginx-ingress-microk8s-controller created Ingress is enabled ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ vim nginx-deploy-svc-ingress.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: nginx-svc port: number: 80 ➜ microk8s kubectl apply -f nginx-deploy-svc-ingress.yaml deployment.apps/nginx-deploy created service/nginx-svc created ingress.networking.k8s.io/nginx-ingress created ➜ microk8s kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-7c5ddbdf54-ld8xc 1/1 Running 0 62s pod/nginx-deploy-7c5ddbdf54-t44vz 1/1 Running 0 62s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.152.183.1 \u003cnone\u003e 443/TCP 22m service/nginx-svc NodePort 10.152.183.115 \u003cnone\u003e 80:30945/TCP 62s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 62s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-7c5ddbdf54 2 2 2 62s ➜ microk8s kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE nginx-ingress public * 127.0.0.1 80 2m17s Access Nginx via Ingress: http://192.168.64.6:80 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup Multipass and MicroK8s ➜ microk8s stop Stopped. ➜ brew uninstall ubuntu/microk8s/microk8s ➜ brew untap ubuntu/microk8s ➜ rm -rf ~/.microk8s ➜ multipass delete microk8s-vm ➜ multipass purge ➜ multipass list No instances found. ➜ brew uninstall --cask multipass ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:5:5","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using K0S with Multipass K0S is a simple, solid \u0026 certified Kubernetes distribution. Multipass is a tool for quickly creating, managing, and operating Ubuntu virtual machines, by Canonical, the company behind Ubuntu. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Virtualization: Multipass CPUs: 1 Memory: 1Gi Disk: 10GiB Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Multipass ➜ brew install --cask multipass ==\u003e Downloading https://github.com/canonical/multipass/releases/download/v1.16.0/multipass-1.16.0+mac-Darwin.pkg ==\u003e Installing Cask multipass installer: Package name is multipass installer: Installing at base path / installer: The install was successful. multipass was successfully installed! ➜ multipass launch --name k0s-controller --cpus 1 --mem 1G --disk 10G Launched: k0s-controller ➜ multipass launch --name k0s-worker --cpus 1 --mem 1G --disk 10G Launched: k0s-worker ➜ multipass info k0s-controller Name: k0s-controller State: Running Snapshots: 0 IPv4: 192.168.64.7 Release: Ubuntu 24.04.2 LTS Image hash: bbecbb88100e (Ubuntu 24.04 LTS) CPU(s): 1 Load: 0.04 0.03 0.01 Disk usage: 2.0GiB out of 9.6GiB Memory usage: 222.6MiB out of 952.9MiB Mounts: -- ➜ multipass info k0s-worker Name: k0s-worker State: Running Snapshots: 0 IPv4: 192.168.64.8 Release: Ubuntu 24.04.2 LTS Image hash: bbecbb88100e (Ubuntu 24.04 LTS) CPU(s): 1 Load: 0.04 0.01 0.00 Disk usage: 2.0GiB out of 9.6GiB Memory usage: 222.4MiB out of 952.9MiB Mounts: -- ➜ multipass exec k0s-controller -- bash -c \"mkdir -p ~/.ssh \u0026\u0026 echo '$(cat ~/.ssh/id_rsa.pub)' \u003e\u003e ~/.ssh/authorized_keys\" ➜ multipass exec k0s-worker -- bash -c \"mkdir -p ~/.ssh \u0026\u0026 echo '$(cat ~/.ssh/id_rsa.pub)' \u003e\u003e ~/.ssh/authorized_keys\" ➜ ssh ubuntu@192.168.64.7 Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ubuntu@k0s-worker:~$ exit logout Connection to 192.168.64.7 closed. ➜ ssh ubuntu@192.168.64.8 Are you sure you want to continue connecting (yes/no/[fingerprint])? yes ubuntu@k0s-worker:~$ exit logout Connection to 192.168.64.8 closed. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup K0S ➜ brew install k0sproject/tap/k0sctl ==\u003e Tapping k0sproject/tap Cloning into '/opt/homebrew/Library/Taps/k0sproject/homebrew-tap'... Tapped 2 formulae (17 files, 98KB). ==\u003e Fetching k0sproject/tap/k0sctl ==\u003e Downloading https://github.com/k0sproject/homebrew-tap/releases/download/k0sctl-0.25.1/k0sctl-0.25.1.arm64_sonoma.bottle.tar.gz ==\u003e Installing k0sctl from k0sproject/tap ==\u003e Pouring k0sctl-0.25.1.arm64_sonoma.bottle.tar.gz /opt/homebrew/Cellar/k0sctl/0.25.1: 9 files, 18.6MB ➜ k0sctl init \u003e k0sctl.yaml ➜ vim k0sctl.yaml apiVersion: k0sctl.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s-cluster user: admin spec: hosts: - ssh: address: 192.168.64.7 user: ubuntu port: 22 keyPath: ~/.ssh/id_rsa role: controller - ssh: address: 192.168.64.8 user: ubuntu port: 22 keyPath: ~/.ssh/id_rsa role: worker options: wait: enabled: true drain: enabled: true gracePeriod: 2m0s timeout: 5m0s force: true ignoreDaemonSets: true deleteEmptyDirData: true podSelector: \"\" skipWaitForDeleteTimeout: 0s concurrency: limit: 30 workerDisruptionPercent: 10 uploads: 5 evictTaint: enabled: false taint: k0sctl.k0sproject.io/evict=true effect: NoExecute controllerWorkers: false ➜ k0sctl apply --config k0sctl.yaml INFO ==\u003e Running phase: Set k0s version INFO Looking up latest stable k0s version INFO Using k0s version v1.33.2+k0s.0 INFO ==\u003e Running phase: Connect to hosts INFO [ssh] 192.168.64.8:22: connected INFO [ssh] 192.168.64.7:22: connected INFO ==\u003e Running phase: Detect host operating systems INFO [ssh] 192.168.64.7:22: is running Ubuntu 24.04.2 LTS INFO [ssh] 192.168.64.8:22: is running Ubuntu 24.04.2 LTS INFO ==\u003e Running phase: Acquire exclusive host lock INFO ==\u003e Running phase: Prepare hosts INFO ==\u003e Running phase: Gather host facts INFO [ssh] 192.168.64.7:22: using k0s-controller as hostname INFO [ssh] 192.168.64.8:22: using k0s-worker as hostname INFO [ssh] 192.168.64.7:22: discovered enp0s1 as private interface INFO [ssh] 192.168.64.8:22: discovered enp0s1 as private interface INFO ==\u003e Running phase: Validate hosts INFO validating clock skew INFO ==\u003e Running phase: Validate facts INFO ==\u003e Running phase: Download k0s on hosts INFO [ssh] 192.168.64.8:22: downloading k0s v1.33.2+k0s.0 INFO [ssh] 192.168.64.7:22: downloading k0s v1.33.2+k0s.0 INFO ==\u003e Running phase: Install k0s binaries on hosts INFO [ssh] 192.168.64.7:22: validating configuration INFO ==\u003e Running phase: Configure k0s INFO [ssh] 192.168.64.7:22: installing new configuration INFO ==\u003e Running phase: Initialize the k0s cluster INFO [ssh] 192.168.64.7:22: installing k0s controller INFO [ssh] 192.168.64.7:22: waiting for the k0s service to start INFO [ssh] 192.168.64.7:22: wait for kubernetes to reach ready state INFO ==\u003e Running phase: Install workers INFO [ssh] 192.168.64.7:22: generating a join token for worker 1 INFO [ssh] 192.168.64.8:22: validating api connection to https://192.168.64.7:6443 using join token INFO [ssh] 192.168.64.8:22: writing join token to /etc/k0s/k0stoken INFO [ssh] 192.168.64.8:22: installing k0s worker INFO [ssh] 192.168.64.8:22: starting service INFO [ssh] 192.168.64.8:22: waiting for node to become ready INFO ==\u003e Running phase: Release exclusive host lock INFO ==\u003e Running phase: Disconnect from hosts INFO ==\u003e Finished in 1m18s INFO k0s cluster version v1.33.2+k0s.0 is now installed INFO Tip: To access the cluster you can now fetch the admin kubeconfig using: INFO k0sctl kubeconfig ➜ k0sctl kubeconfig \u003e k0s.kubeconfig ➜ kubectl --kubeconfig=k0s.kubeconfig get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-5c8cb48c4-rr9k2 1/1 Running 0 8m54s kube-system pod/konnectivity-agent-sndm9 1/1 Running 0 8m50s kube-system pod/kube-proxy-csvw7 1/1 Running 0 8m53s kube-system pod/kube-router-5nn4t 1/1 Running 0 8m53s kube-system pod/metrics-server-7db8586f5-d7p8h 1/1 Running 0 8m50s NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 9m5s kube-system se","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ vim nginx-deploy-svc.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 ➜ kubectl --kubeconfig=k0s.kubeconfig apply -f nginx-deploy-svc.yaml deployment.apps/nginx-deploy created service/nginx-svc created ➜ kubectl --kubeconfig=k0s.kubeconfig get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-86c57bc6b8-pm68d 1/1 Running 0 22s pod/nginx-deploy-86c57bc6b8-tchhj 1/1 Running 0 22s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 33m service/nginx-svc NodePort 10.104.182.55 \u003cnone\u003e 80:32110/TCP 22s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 22s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-86c57bc6b8 2 2 2 22s Access Nginx via NodePort: http://192.168.64.8:32110 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup Multipass and K0S ➜ multipass shell k0s-controller ubuntu@k0s-controller:~$ sudo k0s status Version: v1.33.2+k0s.0 Process ID: 2213 Role: controller Workloads: false SingleNode: false ubuntu@k0s-controller:~$ sudo k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s-worker Ready \u003cnone\u003e 40m v1.33.2+k0s ubuntu@k0s-controller:~$ sudo k0s stop ubuntu@k0s-controller:~$ exit logout ➜ multipass delete k0s-controller k0s-worker ➜ multipass list Name State IPv4 Image k0s-controller Deleted -- Ubuntu 24.04 LTS k0s-worker Deleted -- Ubuntu 24.04 LTS ➜ multipass purge ➜ multipass list No instances found. ➜ brew uninstall --cask multipass ➜ brew uninstall k0sctl ➜ brew untap k0sproject/tap ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:6:5","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Using Talos Talos is a secure, immutable, and minimal container optimized Linux distro designed for Kubernetes. ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:7:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Basic OS: macOS Architecture: ARM64 Driver: Docker Installer: Homebrew ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:7:1","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Setup Talos ➜ brew install siderolabs/tap/talosctl ==\u003e Tapping siderolabs/tap Cloning into '/opt/homebrew/Library/Taps/siderolabs/homebrew-tap'... ==\u003e Fetching siderolabs/tap/talosctl ==\u003e Downloading https://github.com/siderolabs/talos/releases/download/v1.10.5/talosctl-darwin-arm64 ==\u003e Installing talosctl from siderolabs/tap /opt/homebrew/Cellar/talosctl/1.10.5: 7 files, 85.6MB, built in 4 seconds ➜ talosctl cluster create validating CIDR and reserving IPs generating PKI and tokens creating state directory in \"/Users/damonguo/.talos/clusters/talos-default\" downloading ghcr.io/siderolabs/talos:v1.10.5 creating network talos-default creating controlplane nodes creating worker nodes waiting for API bootstrapping cluster waiting for etcd to be healthy: OK waiting for etcd members to be consistent across nodes: OK waiting for etcd members to be control plane nodes: OK waiting for apid to be ready: OK waiting for all nodes memory sizes: OK waiting for all nodes disk sizes: OK waiting for no diagnostics: OK waiting for kubelet to be healthy: OK waiting for all nodes to finish boot sequence: OK waiting for all k8s nodes to report: OK waiting for all control plane static pods to be running: OK waiting for all control plane components to be ready: OK waiting for all k8s nodes to report ready: OK waiting for kube-proxy to report ready: OK waiting for coredns to report ready: OK waiting for all k8s nodes to report schedulable: OK merging kubeconfig into \"/Users/damonguo/.kube/config\" PROVISIONER docker NAME talos-default NETWORK NAME talos-default NETWORK CIDR 10.5.0.0/24 NETWORK GATEWAY 10.5.0.1 NETWORK MTU 1500 KUBERNETES ENDPOINT https://127.0.0.1:60132 NODES: NAME TYPE IP CPU RAM DISK /talos-default-controlplane-1 controlplane 10.5.0.2 2.00 2.1 GB - /talos-default-worker-1 worker 10.5.0.3 2.00 2.1 GB - ➜ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME talos-default-controlplane-1 Ready control-plane 108s v1.33.2 10.5.0.2 \u003cnone\u003e Talos (v1.10.5) 6.14.10-orbstack-00291-g1b252bd3edea containerd://2.0.5 talos-default-worker-1 Ready \u003cnone\u003e 102s v1.33.2 10.5.0.3 \u003cnone\u003e Talos (v1.10.5) 6.14.10-orbstack-00291-g1b252bd3edea containerd://2.0.5 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:7:2","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Deploy Nginx Service ➜ vim nginx-deploy-svc.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: selector: app: nginx type: NodePort ports: - name: http port: 80 ➜ kubectl apply -f nginx-deploy-svc.yaml Warning: would violate PodSecurity \"restricted:latest\": allowPrivilegeEscalation != false (container \"nginx\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"nginx\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"nginx\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"nginx\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\") deployment.apps/nginx-deploy created service/nginx-svc created ➜ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deploy-86c57bc6b8-dk5fh 1/1 Running 0 33s pod/nginx-deploy-86c57bc6b8-hb44f 1/1 Running 0 33s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 8m7s service/nginx-svc NodePort 10.109.129.119 \u003cnone\u003e 80:32659/TCP 33s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deploy 2/2 2 2 33s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deploy-86c57bc6b8 2 2 2 33s ➜ kubectl port-forward svc/nginx-svc 30081:80 Forwarding from 127.0.0.1:30081 -\u003e 80 Forwarding from [::1]:30081 -\u003e 80 Access Nginx via NodePort Local PortMapping: http://127.0.0.1:30081 ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:7:3","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Cleanup Talos ➜ talosctl cluster destroy destroying node talos-default-controlplane-1 destroying node talos-default-worker-1 destroying network talos-default ➜ talosctl cluster show PROVISIONER docker NAME talos-default NETWORK NAME NETWORK CIDR NETWORK GATEWAY NETWORK MTU 0 KUBERNETES ENDPOINT NODES: NAME TYPE IP CPU RAM DISK ➜ kubectl config delete-context admin@talos-default deleted context admin@talos-default from /Users/damonguo/.kube/config ➜ brew uninstall talosctl ➜ brew untap siderolabs/tap ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:7:4","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"Summary Lightweight Kubernetes differ in cluster mode, storage, runtime, requirements, setup, and plugins. Choose based on your use case and technical preferences. Distribution Cluster Mode Storage Runtime Requirements Setup Plugins Use Case Minikube Single-node etcd Podman / Docker / containerd Podman / Docker Easy Good Dev KinD Single-node etcd containerd Docker Easy Poor Dev, CI/CD K3S Multi-node SQLite / etcd / MySQL / PostgreSQL containerd Linux Moderate Fair Edge, Prod K3D Single-node SQLite / etcd / MySQL / PostgreSQL containerd Docker Easy Fair Dev, CI/CD MicroK8s Multi-node etcd / dqlite (distributed SQLite) containerd / Kata Linux Moderate Good Dev, Prod K0S Multi-node etcd / SQLite / MySQL / PostgreSQL containerd Linux Moderate Average Edge, Prod Talos Multi-node etcd containerd Docker Easy Poor Dev, Prod ","date":"2023-12-07","objectID":"/en/2023/12/lightweight-kubernetes-distributions-practices/:8:0","tags":["Kubernetes","Podman"],"title":"Lightweight Kubernetes Distributions Practices","uri":"/en/2023/12/lightweight-kubernetes-distributions-practices/"},{"categories":["Skills"],"content":"In the Internet addressing architecture, the Internet Engineering Task Force (IETF) and the Internet Assigned Numbers Authority (IANA) have reserved various IP addresses for special purposes.","date":"2023-03-29","objectID":"/en/2023/03/what-are-private-ip-addresses/","tags":["Network"],"title":"What are Private IP Addresses","uri":"/en/2023/03/what-are-private-ip-addresses/"},{"categories":["Skills"],"content":"In the Internet addressing architecture, the Internet Engineering Task Force (IETF) and the Internet Assigned Numbers Authority (IANA) have reserved various IP addresses for special purposes. ","date":"2023-03-29","objectID":"/en/2023/03/what-are-private-ip-addresses/:0:0","tags":["Network"],"title":"What are Private IP Addresses","uri":"/en/2023/03/what-are-private-ip-addresses/"},{"categories":["Skills"],"content":"Misunderstandings about Private IP Addresses The concepts of internal and external IP addresses are not fixed, but relative. It’s easier to understand them as private and public IP addresses or local area network and internet IP addresses. Almost all textbooks tell us that there are three types of private IP addresses: Class A: 10.0.0.0 - 10.255.255.255 Class B: 172.16.0.0 - 172.31.255.255 Class C: 192.168.0.0 - 192.168.255.255 But in fact, private IP addresses are more than these. From the Wikipedia page Reserved IP addresses, in the Internet addressing architecture, the Internet Engineering Task Force (IETF) and the Internet Assigned Numbers Authority (IANA) have reserved various IP addresses for special purposes. ","date":"2023-03-29","objectID":"/en/2023/03/what-are-private-ip-addresses/:1:0","tags":["Network"],"title":"What are Private IP Addresses","uri":"/en/2023/03/what-are-private-ip-addresses/"},{"categories":["Skills"],"content":"List of Reserved IPv4 Addresses The IPv4 addresses are shown below: Address block CIDR Address range Number of addresses Scope Description 0.0.0.0/8 0.0.0.0 – 0.255.255.255 16,777,216 Software Current network 10.0.0.0/8 10.0.0.0 – 10.255.255.255 16,777,216 Private network Used for local communications within a private network 100.64.0.0/10 100.64.0.0 – 100.127.255.255 4,194,304 Private network Shared address space for communications between a service provider and its subscribers when using a carrier-grade NAT 127.0.0.0/8 127.0.0.0 – 127.255.255.255 16,777,216 Host Used for loopback addresses to the local host 169.254.0.0/16 169.254.0.0 – 169.254.255.255 65,536 Subnet Used for link-local addresses[5] between two hosts on a single link when no IP address is otherwise specified, such as would have normally been retrieved from a DHCP server 172.16.0.0/12 172.16.0.0 – 172.31.255.255 1,048,576 Private network Used for local communications within a private network 192.0.0.0/24 192.0.0.0 – 192.0.0.255 256 Private network IETF Protocol Assignments 192.0.2.0/24 192.0.2.0 – 192.0.2.255 256 Documentation Assigned as TEST-NET-1, documentation and examples 192.31.196.0/24 192.31.196.0 - 192.31.196.255 256 Private network AS112-v4 192.52.193.0/24 192.52.193.0 - 192.52.193.255 256 Private network AMT 192.88.99.0/24 192.88.99.0 – 192.88.99.255 256 Internet Deprecated (6to4 Relay Anycast) 192.168.0.0/16 192.168.0.0 – 192.168.255.255 65,536 Private network Used for local communications within a private network 192.175.48.0/24 192.175.48.0 - 192.175.48.255 256 Private network Direct Delegation AS112 Service 198.18.0.0/15 198.18.0.0 – 198.19.255.255 131,072 Private network Used for benchmark testing of inter-network communications between two separate subnets 198.51.100.0/24 198.51.100.0 – 198.51.100.255 256 Documentation Assigned as TEST-NET-2, documentation and examples 203.0.113.0/24 203.0.113.0 – 203.0.113.255 256 Documentation Assigned as TEST-NET-3, documentation and examples 240.0.0.0/4 240.0.0.0 – 255.255.255.254 268,435,455 Internet Reserved for future use 255.255.255.255/32 255.255.255.255 1 Subnet Reserved for the “limited broadcast” destination address ","date":"2023-03-29","objectID":"/en/2023/03/what-are-private-ip-addresses/:2:0","tags":["Network"],"title":"What are Private IP Addresses","uri":"/en/2023/03/what-are-private-ip-addresses/"},{"categories":["Skills"],"content":"References https://en.wikipedia.org/wiki/Reserved_IP_addresses https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml ","date":"2023-03-29","objectID":"/en/2023/03/what-are-private-ip-addresses/:3:0","tags":["Network"],"title":"What are Private IP Addresses","uri":"/en/2023/03/what-are-private-ip-addresses/"},{"categories":["Skills"],"content":"After restricting the source of S3 Bucket access requests through Bucket Level Policy, the data in AWS S3 Bucket can still be secured even if AKSK is compromised.","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"After restricting the source of S3 Bucket access requests through Bucket Level Policy, the data in AWS S3 Bucket can still be secured even if AKSK is compromised. ","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/:0:0","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"Background Due to weak security awareness, many people prefer to access resources through AKSK (Access Key and Secret Key). AKSK contains only two strings and the user permissions can be obtained via command line tools or API codes. Take AWS as an example, a simple aws s3 sync command can drag away all the data stored in S3 Bucket. After restricting the source of S3 Bucket access requests through Bucket Level Policy, the data in AWS S3 Bucket can still be secured even if AKSK is compromised. One prerequisite is that the compromised AKSK should not have administrator privileges or full privileges of services such as IAM, EC2, Lambda, etc. Otherwise, it may bypass the Bucket Level Policy restrictions or modify the Bucket Level Policy through those services. For such cases, restrictions could be imposed on various services from the organization account level by SCP. ","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/:1:0","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"Policy Example The following Bucket Level Policy uses the Deny by default, only allows the following types of requests. When requests come directly from AWS services. When requests come from the specified VPC IDs. When requests come from the specified Roles. When requests come from the specified IPs. { \"Version\": \"2012-10-17\", \"Id\": \"RestrictVPCsAndARNsAndSourceIPs\", \"Statement\": [ { \"Sid\": \"VPCsAndARNsAndSourceIPs\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::bucket-name\", \"arn:aws:s3:::bucket-name/*\" ], \"Condition\": { \"Bool\": { \"aws:ViaAWSService\": \"false\" }, \"StringNotEqualsIfExists\": { \"aws:SourceVpc\": [ \"vpc-857abc857abc875aa\", \"vpc-857cba857cba875bb\" ] }, \"ArnNotLikeIfExists\": { \"aws:PrincipalArn\": [ \"arn:aws:iam::857857857857:role/role-name\", \"arn:aws:iam::361361361361:role/role-name\", \"arn:aws:iam::857857857857:role/role*\", \"arn:aws:iam::361361361361:role/role*\" ] }, \"NotIpAddressIfExists\": { \"aws:SourceIp\": [ \"8.5.7.11/32\", \"8.5.7.22/32\" ] } } } ] } ","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/:2:0","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"Better solution Here are some strong protection mechanisms for the data in Amazon S3, including least privilege access, encryption of data at rest, restricted sources, logging, monitoring, and configuration checks. Restrict the sources of requests at the organization level for S3 and other AWS services Use bucket policies to verify all access granted is restricted and specific Enable S3 protection in GuardDuty to detect suspicious activities Use Macie to scan for sensitive data outside of designated areas Use KMS to encrypt data in S3 Protect data in S3 from accidental deletion using S3 Versioning and S3 Object Lock Enable logging for S3 using CloudTrail and S3 server access logging Monitor S3 using Security Hub and CloudWatch Logs Use Cross-region replication to backup data in S3 ","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/:3:0","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"References https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html https://aws.amazon.com/cn/blogs/security/top-10-security-best-practices-for-securing-data-in-amazon-s3/ ","date":"2022-11-20","objectID":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/:4:0","tags":["AWS","Security"],"title":"Enhance AWS S3 Data Security with Bucket Level Policy","uri":"/en/2022/11/enhance-aws-s3-data-security-with-bucket-level-policy/"},{"categories":["Skills"],"content":"Every punctuation mark has a name.","date":"2022-05-31","objectID":"/en/2022/05/punctuation-marks/","tags":["English"],"title":"Punctuation Marks","uri":"/en/2022/05/punctuation-marks/"},{"categories":["Skills"],"content":"Every punctuation mark has a name. Reference: List of typographical symbols and punctuation marks Punctuation Mark Name ~ tilde ` backquote ! exclamation mark @ at # hash , number sign , pound sign $ dollar % percent ^ caret \u0026 and , ampersand * asterisk , star / multiply / pointer ( parenleft , opening parenthesis ) parenright , closing parenthesis ( ) parentheses _ underscore - minus / hyphen + plus = equal { braceleft , opening brace } braceright , closing brace { } braces [ bracketleft , opening bracket ] bracketright , closing bracket [ ] brackets | bar , vertical bar \\ backslash : colon ; semicolon \" double quote \" \" quotation marks ' quote / apostrophe \u003c less-than / opening angle \u003e greater-than / closing angle \u003c \u003e angle brackets , comma . full stop , period / dot ? question mark / slash – dash … dots , ellipsis || parallel → arrow ✓ tick , check x cross ","date":"2022-05-31","objectID":"/en/2022/05/punctuation-marks/:0:0","tags":["English"],"title":"Punctuation Marks","uri":"/en/2022/05/punctuation-marks/"},{"categories":["Notes"],"content":"The rich buy assets. The poor only have expenses. The middle class buy liabilities they think are assets.","date":"2022-05-29","objectID":"/en/2022/05/notes-from-rich-dad-poor-dad/","tags":["Finance"],"title":"Notes from \"Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not\"","uri":"/en/2022/05/notes-from-rich-dad-poor-dad/"},{"categories":["Notes"],"content":"The rich buy assets. The poor only have expenses. The middle class buy liabilities they think are assets. ","date":"2022-05-29","objectID":"/en/2022/05/notes-from-rich-dad-poor-dad/:0:0","tags":["Finance"],"title":"Notes from \"Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not\"","uri":"/en/2022/05/notes-from-rich-dad-poor-dad/"},{"categories":["Notes"],"content":"Preface Robert Kiyosaki had two fathers: a rich one and a poor one. One was highly educated with a Ph.D. and so intelligent he completed his undergraduate degree in only two years. The other father didn’t even finish the eighth grade. While both men worked hard, were successful, and earned a lot of money, there was always one who struggled with money. And the other dad, well, he became one of the richest people in Hawaii. “Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not” is Robert Kiyosaki’s best-selling book about the difference in mindset between the poor, middle class, and rich. ","date":"2022-05-29","objectID":"/en/2022/05/notes-from-rich-dad-poor-dad/:1:0","tags":["Finance"],"title":"Notes from \"Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not\"","uri":"/en/2022/05/notes-from-rich-dad-poor-dad/"},{"categories":["Notes"],"content":"Summary Here is a summary from Tess: Make Money Work for You The middle class and the poor work for money. Instead the rich make the money work for them. Asset = puts money in your pocket (Generate money) Liability = takes money out of your pocket (Cost you money) Pay Yourself First To pay yourself first means allocating your money to the assets column, before paying your monthly expenses and bills. Business vs Job Keep your daytime job but start buying real assets: Businesses that do not require your presence Invest in stocks, bonds, real estate The Most Important Asset Your mind is your greatest asset, so be careful what you put into it. Actively seek out ways to grow your knowledge about money and managing your finances. Increase your financial literacy. As Warren Buffett said, “The most important investment you can make is in yourself.” What is Your Reason? Underlying everything that we do, there is a fundamental need we are trying to fulfill. As with anything in life, you don’t have a strong enough “why” it makes the process difficult when you are faced with setbacks. Your reason or purpose is usually a mix of “wants” and “don’t wants”. “I don’t want to work my entire life and be stuck in the rat race.” “I don’t want to live paycheck to paycheck.” “I want control over my time and my life. I want to make money work for me so I can be free and travel the world with my loved ones.” “I want control over my time and my life.” Dig deeper and ask yourself these questions. “What does being financially free means to me?” “Why is it so important for me to gain financial independence?” Your purpose lies somewhere in these answers. 5 Key Takeaways To recap, here are the five key main points: Make money work for you by Investing Always Pay Yourself First Know the difference between your Business and your Job Your Mind is the Most Important Asset Find your reason for financial independence Among all these 5 takeaways, the last one is the most important. Find your personal reason and understand why it is important for you to “gain financial freedom”. If you gain clarity on your why, you’ll naturally get to the how. Your journey starts with this step. ","date":"2022-05-29","objectID":"/en/2022/05/notes-from-rich-dad-poor-dad/:2:0","tags":["Finance"],"title":"Notes from \"Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not\"","uri":"/en/2022/05/notes-from-rich-dad-poor-dad/"},{"categories":["Notes"],"content":"Thinking “The rich buy assets. The poor only have expenses. The middle class buy liabilities they think are assets.” “The rich don’t work for money. They make money work for them.” These two statements were enlightening to me. And the phrase “Rat Race” made me realize the reality of my current situation. With this book, I have started to learn about finance and try to get out of the “Rat Race” dilemma. ","date":"2022-05-29","objectID":"/en/2022/05/notes-from-rich-dad-poor-dad/:3:0","tags":["Finance"],"title":"Notes from \"Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not\"","uri":"/en/2022/05/notes-from-rich-dad-poor-dad/"},{"categories":["Notes"],"content":"How an Economy Grows and Why it Crashes uses illustration, humor, and accessible storytelling to explain complex topics of economic growth and monetary systems.","date":"2022-04-08","objectID":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/","tags":["Economics"],"title":"Notes from \"How an Economy Grows and Why It Crashes\"","uri":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/"},{"categories":["Notes"],"content":"How an Economy Grows and Why it Crashes uses illustration, humor, and accessible storytelling to explain complex topics of economic growth and monetary systems. ","date":"2022-04-08","objectID":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/:0:0","tags":["Economics"],"title":"Notes from \"How an Economy Grows and Why It Crashes\"","uri":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/"},{"categories":["Notes"],"content":"Preface How an Economy Grows and Why It Crashes incorporates the spirit of the original while tackling the latest economic issues. With wit and humor, the Schiffs explain the roots of economic growth, the uses of capital, the destructive nature of consumer credit, the source of inflation, the importance of trade, savings, and risk, and many other topical principles of economics. ","date":"2022-04-08","objectID":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/:1:0","tags":["Economics"],"title":"Notes from \"How an Economy Grows and Why It Crashes\"","uri":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/"},{"categories":["Notes"],"content":"Summary Here is a summary from Devin Cooper’s customer review: Understanding how the economy crashes from the Austrian point of view can be very complicated. You have to know capital theory, what interest rates are, how resources are coordinated, etc. Many people do not have the time to learn all of this. Peter Schiff’s book tries to take these complicated theories and teaches his readers through a short story about the progression of a small island economy. Peter’s book is an updated version of his father’s book How an Economy Grows and Why It Doesn’t by adding recent events and characters to the story. The book introduces us to a little bit of the history of the science of economics in the last hundred years. Peter talks about the early Austrians and the rise of Keynesianism in the 1930s as a counter to the Great Depression. Keynesianism became the dominant paradigm and has plagued economics and the world since. Once Upon a Time The story starts with three islanders – Able, Baker, and Charlie. They were in dire poverty. The only resource they can gather is fish, and only one per day since they are using their bare hands (just enough to survive in this story). Since they consumed everything they caught, there were no savings in case something bad had happened. But they wanted more for their lives than spending the entire day catching one fish. Able came up with an idea to create a fish catcher, but in order to create this device, he must sacrifice a day and become on the verge of starvation to try and produce this net since there are no savings. When he created this net, his productivity doubled as he is now able to catch two fish per day. Throughout the book, Peter slips in economic ideas by explaining what the islanders are doing and sums up these ideas at the end of each chapter. For example, when Able created the net, he had to sacrifice eating a fish for a day, which meant he under-consumed, in order to create a net, or a capital good. As the story progresses, as there are now savings thanks to Able’s invention of the net, economic expansion accelerates. Able is able to do more than just fish because he now has a net. As they continue to save and create capital goods, their lives become better. The story continues with an explanation of other economic theories, such as the interest rate and what they do, why banks are created, how trade expands, the division of labor, and finally, how governments inflate the currency and drive interest rates lower to create an economic boom with an inevitable crash. Epilogue The story ends and Peter shifts to the modern day, as he explains how the bursting of the dot-com bubble and George Bush and Alan Greenspan’s heavy intervention into the market has fueled the bubble for the housing bubble and created the crash. He continues to describe how the government, instead of learning from the lessons of the past, keeps trying the same thing that caused the last crisis. When Obama came to office, he did the same thing Bush did, which was to stimulate the economy in order to make it look good for re-election. So if you really want to understand the cause of recessions in an easy and fun manner, Peter Schiff’s book will tell you a good story. ","date":"2022-04-08","objectID":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/:2:0","tags":["Economics"],"title":"Notes from \"How an Economy Grows and Why It Crashes\"","uri":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/"},{"categories":["Notes"],"content":"Thinking As a layman in economics, I have gained some rough understanding of the principles of economics by reading this fable-like book, and I will also start to learn about economics. ","date":"2022-04-08","objectID":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/:3:0","tags":["Economics"],"title":"Notes from \"How an Economy Grows and Why It Crashes\"","uri":"/en/2022/04/notes-from-how-an-economy-grows-and-why-it-crashes/"},{"categories":["Notes"],"content":"Four steps of Nonviolent Communication: observe, identify and state feelings, identify and state needs, and make requests.","date":"2022-03-24","objectID":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/","tags":["Psychology"],"title":"Notes from \"Nonviolent Communication: A Language of Life\"","uri":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/"},{"categories":["Notes"],"content":"Four steps of Nonviolent Communication: observe, identify and state feelings, identify and state needs, and make requests. ","date":"2022-03-24","objectID":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/:0:0","tags":["Psychology"],"title":"Notes from \"Nonviolent Communication: A Language of Life\"","uri":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/"},{"categories":["Notes"],"content":"Preface Nonviolent Communication (NVC) has been described as a language of compassion, as a tool for positive social change, and as a spiritual practice. NVC gives us the tools and consciousness to understand what triggers us, to take responsibility for our reactions, and to deepen our connection with ourselves and others, thereby transforming our habitual responses to life. Ultimately, it involves a radical change in how we think about life and meaning. NVC is based on a fundamental principle: Underlying all human actions are needs that people are seeking to meet, and understanding and acknowledging these needs can create a shared basis for connection, cooperation, and more globally – peace. NVC was developed by Dr. Marshall B. Rosenberg, who has introduced it to individuals and organizations world-wide. NVC has been used between warring tribes and in war-torn countries; in schools, prisons, and corporations; in health care, social change, and government institutions; and in intimate personal relationships. Currently, over 200 hundred certified trainers and many more non-certified trainers around the world are sharing NVC in their communities. ","date":"2022-03-24","objectID":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/:1:0","tags":["Psychology"],"title":"Notes from \"Nonviolent Communication: A Language of Life\"","uri":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/"},{"categories":["Notes"],"content":"Summary The language of NVC includes two parts: honestly expressing ourselves to others, and empathically hearing others. Both are expressed through four components – observations, feelings, needs, and requests. Four steps of Nonviolent Communication: observe, identify and state feelings, identify and state needs, and make requests. Observations: Observe what’s happening, what’s really going on? What is happening or being said that you either like or dislike? Feelings: Identify your feelings about it, anger, joy, hopeful, inspired, lonely? Needs: Figure out what need you have that is driving that feeling. Requests: Ask for what you need (explicitly). When you feel an emotional response to a situation, it’s always based on some unmet need. So figure out what that need is and then request (don’t demand) for the other person to fulfill it. Use phrases like: “Would you be willing to set the table?” rather than “Set the table.” Expressing our own observations, feelings, needs and requests to others is one part of NVC. The second part is empathy: the process of connecting with another by guessing their feelings and needs. When we use NVC to connect empathically, we use the same four components in the form of a question, since we can never know what is going on inside the other. The other person will always be the ultimate authority on what is going on for them. Our empathy may meet other people’s needs for understanding, or it may spark their own self-discovery. We may ask something like: When you [see, hear, etc…] …. Are you feeling ….. Because you need ….. And would you like …..? ","date":"2022-03-24","objectID":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/:2:0","tags":["Psychology"],"title":"Notes from \"Nonviolent Communication: A Language of Life\"","uri":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/"},{"categories":["Notes"],"content":"Thinking Nonviolence means integrating love into our life. Leading life with respect, understanding, appreciation, gratitude, compassion and friendship, rather than selfishness, greed, hatred, prejudice, suspicion and hostility. Nonviolent communication reminds us that listening to the different voices within, and the needs they reflect, can promote self-understanding and inner harmony. By developing good communication through a pattern of observations, feelings, needs, and requests, makes life more harmonious and beautiful. I will practice the Nonviolent Communication to make life full of love and happiness. ","date":"2022-03-24","objectID":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/:3:0","tags":["Psychology"],"title":"Notes from \"Nonviolent Communication: A Language of Life\"","uri":"/en/2022/03/notes-from-nonviolent-communication-a-language-of-life/"},{"categories":["Notes"],"content":"The 6 principles of persuasion are Reciprocation, Commitment and Consistency, Social Proof, Liking, Authority, Scarcity.","date":"2022-03-06","objectID":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/","tags":["Psychology"],"title":"Notes from \"Influence: The Psychology of Persuasion\"","uri":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/"},{"categories":["Notes"],"content":"The 6 principles of persuasion are Reciprocation, Commitment and Consistency, Social Proof, Liking, Authority, Scarcity. ","date":"2022-03-06","objectID":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/:0:0","tags":["Psychology"],"title":"Notes from \"Influence: The Psychology of Persuasion\"","uri":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/"},{"categories":["Notes"],"content":"Preface These 6 ways to influence people were first put forward by Robert B. Cialdini in his 1984 book called “Influence: The Psychology of Persuasion.” ","date":"2022-03-06","objectID":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/:1:0","tags":["Psychology"],"title":"Notes from \"Influence: The Psychology of Persuasion\"","uri":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/"},{"categories":["Notes"],"content":"Summary Here is a summary of 6 principles of persuasion, which from James Beswick’s customer review: Reciprocation I do you a favor, and you are predisposed to do the same thing for me. Those free food samples at Costco aren’t so free after all. Contrast: Extreme positions make less extreme positions more palatable - You are more likely to spend $100 on a shirt if you already spent on a suit, and the good cop is more likely to win your confession after the bad cop has done his work. Commitment and Consistency Studies show that gamblers are more convinced of a horse’s capability to win after placing the bet versus before the bet. Placing a commitment causes more loyalty to an idea. People will defend a position once they take it - Competitions that ask you to describe the virtues of a product in 15 words or less essentially convert you to promote positive messages of that product to other people, since you will aim to be consistent with yourself. Social proof People will look to others to aid their decision-making process. If “people like you” claim to like an idea, chances are that you will too (hence canned laughter in TV shows). Liking You are more likely to accept an idea from a person you like. Studies show that attractive people have twice the likelihood of con vicing others than average-looking individuals, and salespeople are frequently trained to feign interest in your hobbies and mirror your body language for the same reason. Authority If those we respect take a position, we’re more convinced by its validity. From the sales frenzy over Sarah Palin’s glasses to the use of celebrities to endorse products, authority is a trump card in persuasion. Scarcity We are driven to ideas that seem scarce - Hence the use of the “Buy now, offer won’t last!” approach to sales, or the use of high prices to create exclusivity. ","date":"2022-03-06","objectID":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/:2:0","tags":["Psychology"],"title":"Notes from \"Influence: The Psychology of Persuasion\"","uri":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/"},{"categories":["Notes"],"content":"Thinking After reading the book “Influence”, I have gained a psychological level of understanding of some business activities and social phenomena. In the future, when facing similar psychological manipulation, I should be able to keep a clearer head and think independently to avoid being easily misled. ","date":"2022-03-06","objectID":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/:3:0","tags":["Psychology"],"title":"Notes from \"Influence: The Psychology of Persuasion\"","uri":"/en/2022/03/notes-from-influence-the-psychology-of-persuasion/"},{"categories":["Thinking"],"content":"The rest of life is precious and unpredictable, don't let the regrets be too long.","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Thinking"],"content":"The rest of life is precious and unpredictable, don’t let the regrets be too long. ","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/:0:0","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Thinking"],"content":"Be Open-minded Be not afraid. Do it best. Keep doing and stop worrying about results. When you feel lost, revisit your list of unaccomplished goals. Regular review enables us to preserve existing knowledge and obtain new insights. Be honest with myself and not struggle. Keep a real record of my life to use real data for analysis, decision making and continuous improvement. ","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/:1:0","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Thinking"],"content":"Be Positive Get out and get some exercise! You’ll be back in full spirit! Focus now! You’ll experience the happiness of flow! Keep it tidy and clean! You’ll be easeful and clear! ","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/:2:0","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Thinking"],"content":"Love and Happiness When parents are alive, we know where we came from. When parents are gone, the only way we have left in our life towards the death. Love my parents, love my wife’s parents. Love my wife, love our children. Love myself, love our relatives and friends. ","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/:3:0","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Thinking"],"content":"Choices Reasoning isn’t the ultimate solution to problems, showing love is. When falling into bad emotions, the wisest and most valuable choice is to observe inner emotional changes, do more behaviors that are conducive to good emotions, and reduce unnecessary emotional consumption to maintain a positive mental state. ","date":"2022-01-20","objectID":"/en/2022/01/life-is-short-and-love-is-long/:4:0","tags":["Love","Happiness","Family"],"title":"Life is Short and Love is Long","uri":"/en/2022/01/life-is-short-and-love-is-long/"},{"categories":["Notes"],"content":"Flee as a bird to your mountain.","date":"2022-01-16","objectID":"/en/2022/01/notes-from-educated-a-memoir/","tags":["Biography"],"title":"Notes from \"Educated: A Memoir\"","uri":"/en/2022/01/notes-from-educated-a-memoir/"},{"categories":["Notes"],"content":"Flee as a bird to your mountain. ","date":"2022-01-16","objectID":"/en/2022/01/notes-from-educated-a-memoir/:0:0","tags":["Biography"],"title":"Notes from \"Educated: A Memoir\"","uri":"/en/2022/01/notes-from-educated-a-memoir/"},{"categories":["Notes"],"content":"Preface This book is a true story of how a young woman grew up living off-the-grid, with no birth certificate or education, in a violent, mentally unstable, survivalist family, and found her own way, step by jagged step, to a Ph.D. at Cambridge University. ","date":"2022-01-16","objectID":"/en/2022/01/notes-from-educated-a-memoir/:1:0","tags":["Biography"],"title":"Notes from \"Educated: A Memoir\"","uri":"/en/2022/01/notes-from-educated-a-memoir/"},{"categories":["Notes"],"content":"Summary The special childhood experience made Tara different from everyone else in Cambridge. Part of her will always belong to the mountain back home, and another part of her is destined to fly like a bird to her own mountain. Education gives Tara a new perspective on how her family sees the world, a new view of her father’s gaslighting, her mother’s cowardice, and her brother Shawn’s violence. She courageously resisted her family’s opposition and obstacles to her education, grew in her education and freed herself from gaslighting. As Tara said: “You could call this selfhood many things. Transformation. Metamorphosis. Falsity. Betrayal. I call it an education.” ","date":"2022-01-16","objectID":"/en/2022/01/notes-from-educated-a-memoir/:2:0","tags":["Biography"],"title":"Notes from \"Educated: A Memoir\"","uri":"/en/2022/01/notes-from-educated-a-memoir/"},{"categories":["Notes"],"content":"Thinking Tara’s experience was full of trauma and struggle. Putting those feelings aside, I prefer to get some energy from a positive perspective as the following. The power of role models Tara’s older brother, Tyler, was the first child who left home, went to university and got his Ph.D. He proved himself, enlightened and encouraged Tara to enter the university. He is a role model for Tara. Tara’s other brother, Richard, also stepped into university after her and got his Ph.D. In a family with seven children, three have PhDs, while the other four didn’t even go to high school. It is the power of role models. The wealth of suffering Tara’s hardships were instead transformed into an asset, valuable material for her dissertation research, which won her professors’ favor and led to her subsequent legendary experiences at Cambridge and Harvard. The warmth of human kindness Her father’s gaslighting, her mother’s cowardice, and her brother Shawn’s violence were not the whole story of Tara’s life. The love of her grandmother, the concern of her mother, and the encouragement of her brother Tyler brought warmth and hope to her life. The friendliness of her classmates, the concern of her friends, the financial support from her church, and the attentive care of her professors were also all blessings for Tara to be thankful for. I will keep such a positive perspective, gain strength from role models, turn hardships into wealth, feel and be grateful for the warmth of human kindness, keep learning to be educated, and fly as a bird to my mountain. ","date":"2022-01-16","objectID":"/en/2022/01/notes-from-educated-a-memoir/:3:0","tags":["Biography"],"title":"Notes from \"Educated: A Memoir\"","uri":"/en/2022/01/notes-from-educated-a-memoir/"},{"categories":["Notes"],"content":"The six ingredients of happiness are togetherness, money, health, freedom, trust and kindness.","date":"2022-01-09","objectID":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/","tags":["Happiness"],"title":"Notes from \"The Little Book of Lykke: Secrets of the World's Happiest People\"","uri":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/"},{"categories":["Notes"],"content":"The six ingredients of happiness are togetherness, money, health, freedom, trust and kindness. ","date":"2022-01-09","objectID":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/:0:0","tags":["Happiness"],"title":"Notes from \"The Little Book of Lykke: Secrets of the World's Happiest People\"","uri":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/"},{"categories":["Notes"],"content":"Preface In The Little Book of Lykke, Meik identifies the six ingredients that explain the majority of differences in happiness across the world: togetherness, money, health, freedom, trust and kindness, and explores what actions we can take to become happier. ","date":"2022-01-09","objectID":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/:1:0","tags":["Happiness"],"title":"Notes from \"The Little Book of Lykke: Secrets of the World's Happiest People\"","uri":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/"},{"categories":["Notes"],"content":"Summary Here are the six ingredients as outlined in the book. Togetherness Sharing food nurtures more than our physical body. It feeds our friendships, bolsters our bonds, and nourishes our sense of community. Creating common areas for the community encourages social interactions and impromptu conversations between residents. If you think back to a time you felt happy - a time you felt good, laughed, or smiled, chances are you were together with other people. The happiness tip is to make the effort to speak to your neighbours, meet them for coffee, help them in the shared garden, or just stop to chat next time you see them. Money Lack of money is a cause of unhappiness. Take time to enjoy the journey towards your goal, but also understand that achieving your goal will not fulfill you completely. We continuously raise the bar for what we want or what we need to feel happy. There is no single thing that will quench our ambitions forever. Focus on the journey, not the destination. Health Health is wealth. Good health allows us to play, enjoy life, and seek adventures. Freedom Life is defined by time. Time is a finite, non-replenishable, and perishable resource. It is also the only resource that is granted to all humans evenly. Three key areas have an impact on how you spend your time: work, family/relationships, and commute. Trust People who trust other people are happier. When there is trust, you don’t need to write up a contract for every simple transaction, a deal is a deal, and your word is your word. If you tell the truth, you don’t have to remember everything. Kindness Find ways you can volunteer to help others. Improve your community and develop your sense of purpose. Smile and chat with strangers. Hand out smiles and friendly remarks. They are free. Learn the name of someone you see every day. Greet that person by name. Talk to the shy person who’s by themselves at a party or the office. Give someone a genuine compliment. ","date":"2022-01-09","objectID":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/:2:0","tags":["Happiness"],"title":"Notes from \"The Little Book of Lykke: Secrets of the World's Happiest People\"","uri":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/"},{"categories":["Notes"],"content":"Thinking Our real life is always not so good, but we can try some of the practices in the book to make us happier. I’m looking forward to traveling to northern Europe with my family to experience the kind of happy life in the book. ","date":"2022-01-09","objectID":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/:3:0","tags":["Happiness"],"title":"Notes from \"The Little Book of Lykke: Secrets of the World's Happiest People\"","uri":"/en/2022/01/notes-from-secrets-of-the-worlds-happiest-people/"},{"categories":["Thinking"],"content":"The most important is that I have unlocked my thirst of knowledge and could not stop learning.","date":"2022-01-06","objectID":"/en/2022/01/my-2021-year-in-review/","tags":["Growth","Year in Review"],"title":"My 2021 Year in Review","uri":"/en/2022/01/my-2021-year-in-review/"},{"categories":["Thinking"],"content":"The most important is that I have unlocked my thirst of knowledge and could not stop learning. ","date":"2022-01-06","objectID":"/en/2022/01/my-2021-year-in-review/:0:0","tags":["Growth","Year in Review"],"title":"My 2021 Year in Review","uri":"/en/2022/01/my-2021-year-in-review/"},{"categories":["Thinking"],"content":"Achievements Here are my achievements in the 2021: 🎓 Completed all postgraduate courses with credits 👶 Had a baby son, cute and smart 🏠 Bought a bigger house 📚 Memorized 12k English words 📑 Wrote this new blog with all posts in both Chinese and English 🏆 Got 3 certifications: AWS-SAP, ITIL, and CKA 💸 Awarded the MSTAR and promoted as AVP(Assistant Vice President) in the company I have also started to lose weight 🏃 and learn Cybersecurity for the CISSP. And the most important is that I have unlocked my thirst of knowledge and could not stop learning ☀️. ","date":"2022-01-06","objectID":"/en/2022/01/my-2021-year-in-review/:1:0","tags":["Growth","Year in Review"],"title":"My 2021 Year in Review","uri":"/en/2022/01/my-2021-year-in-review/"},{"categories":["Notes"],"content":"The Pomodoro Technique is designed to maintain focus and prevent burnout.","date":"2022-01-01","objectID":"/en/2022/01/notes-from-pomodoro-technique-illustrated/","tags":["Pomodoro Technique","Time Management"],"title":"Notes from \"Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time\"","uri":"/en/2022/01/notes-from-pomodoro-technique-illustrated/"},{"categories":["Notes"],"content":"The Pomodoro Technique is designed to maintain focus and prevent burnout. ","date":"2022-01-01","objectID":"/en/2022/01/notes-from-pomodoro-technique-illustrated/:0:0","tags":["Pomodoro Technique","Time Management"],"title":"Notes from \"Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time\"","uri":"/en/2022/01/notes-from-pomodoro-technique-illustrated/"},{"categories":["Notes"],"content":"Preface The name Pomodoro comes from a tomato-shaped kitchen timer that was used to manage bursts of work. The book “Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time” shows the Pomodoro Technique as the one action-planning technique that fits exactly as conceived into Agile approaches to projects. It’s a good book to learn the technique and become excellent at it. ","date":"2022-01-01","objectID":"/en/2022/01/notes-from-pomodoro-technique-illustrated/:1:0","tags":["Pomodoro Technique","Time Management"],"title":"Notes from \"Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time\"","uri":"/en/2022/01/notes-from-pomodoro-technique-illustrated/"},{"categories":["Notes"],"content":"Summary The basic unit of work in the Pomodoro Technique can be split into five simple steps: Choose a task to be accomplished Set a timer to 25 minutes Work on the task until the Pomodoro rings, then put a check on your sheet of paper Take a 5-minute break (marks the completion of one Pomodoro) Every 4 Pomodoros take a 15-30 minute longer break PDCA is an iterative problem-solving process that represents Plan-Do-Check-Act. Plan: Define the goals and processes needed to deliver the expected results Do: Implement the process Check: Measure the process and compare the results to the expected outcome in order to find discrepancies Act: Analyze the discrepancies and try to understand the root cause of them This particular type of planning, monitoring, measuring, and improving is also the core of the Pomodoro Technique. The Pomodoro Technique gets us to focus on execution. It could mean that we ignore Systems Thinking and holism. That’s why it’s often beneficial to let processes like Scrum, XP, or GTD enclose the Pomodoro Technique. ","date":"2022-01-01","objectID":"/en/2022/01/notes-from-pomodoro-technique-illustrated/:2:0","tags":["Pomodoro Technique","Time Management"],"title":"Notes from \"Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time\"","uri":"/en/2022/01/notes-from-pomodoro-technique-illustrated/"},{"categories":["Notes"],"content":"Thinking From this book, I’ve learned about the 25/5/15 time management method, the scientific theory of the daily Pomodoro Technique PDCA Cycle, how to handle interruptions and customize my own process. I plan every morning and review in the evening, concentrating on a single activity in a Pomodoro. No longer feel anxious, because all tasks just become the activities made up of Pomodoros. ","date":"2022-01-01","objectID":"/en/2022/01/notes-from-pomodoro-technique-illustrated/:3:0","tags":["Pomodoro Technique","Time Management"],"title":"Notes from \"Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time\"","uri":"/en/2022/01/notes-from-pomodoro-technique-illustrated/"},{"categories":["Skills"],"content":"kubeadm performs the actions necessary to get a minimum viable cluster up and running.","date":"2021-12-21","objectID":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/","tags":["Kubernetes"],"title":"Setup Kubernetes Cluster Using kubeadm","uri":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/"},{"categories":["Skills"],"content":"kubeadm performs the actions necessary to get a minimum viable cluster up and running. ","date":"2021-12-21","objectID":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/:0:0","tags":["Kubernetes"],"title":"Setup Kubernetes Cluster Using kubeadm","uri":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/"},{"categories":["Skills"],"content":"Prepare the Nodes Note: root user by default on all Nodes Prepare the EC2 instances(replace the IP addresses as required) OS: CentOS 7.9 x86_64 CPU: 2 vCores Memory: 4 Gi kubeadm01: 172.31.8.8 kubeadm02: 172.31.5.5 kubeadm03: 172.31.7.7 Upgrade kernel and packages yum install -y http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm yum install --enablerepo=elrepo-kernel -y kernel-lt kernel_str=$(grep -E 'menuentry.*elrepo' /boot/grub2/grub.cfg | cut -d\\' -f2) grub2-set-default \"${kernel_str}\" grub2-editenv list yum -y update Configure /etc/hostname [root@kubeadm01 ~]# cat \u003e /etc/hostname \u003c\u003cEOF kubeadm01 EOF [root@kubeadm01 ~]# hostname kubeadm01 [root@kubeadm02 ~]# cat \u003e /etc/hostname \u003c\u003cEOF kubeadm02 EOF [root@kubeadm02 ~]# hostname kubeadm02 [root@kubeadm01 ~]# cat \u003e /etc/hostname \u003c\u003cEOF kubeadm03 EOF [root@kubeadm03 ~]# hostname kubeadm03 Configure /etc/hosts(replace the IP addresses as required) cat \u003e\u003e /etc/hosts \u003c\u003cEOF 172.31.8.8 kubeadm01 172.31.5.5 kubeadm02 172.31.7.7 kubeadm03 EO Configure /etc/sysctl.conf cat \u003e /etc/sysctl.d/sysctl.conf \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 vm.swappiness=0 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF sysctl -p /etc/sysctl.d/sysctl.conf Install required packages yum install -y epel-release yum install -y chrony conntrack ipvsadm ipset jq iptables curl sysstat libseccomp wget socat git vim lrzsz wget man tree rsync gcc gcc-c++ cmake telnet Enable Chrony service systemctl start chronyd systemctl enable chronyd Disable Firewalld, Swap and SELinux systemctl stop firewalld systemctl disable firewalld swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config Configure /etc/modules-load.d/kubernetes.conf and reboot cat \u003e /etc/modules-load.d/kubernetes.conf \u003c\u003cEOF ip_vs_dh ip_vs_ftp ip_vs ip_vs_lblc ip_vs_lblcr ip_vs_lc ip_vs_nq ip_vs_pe_sip ip_vs_rr ip_vs_sed ip_vs_sh ip_vs_wlc ip_vs_wrr nf_conntrack_ipv4 overlay br_netfilter EOF systemctl enable systemd-modules-load.service sync reboot Verify the kernel modules after reboot lsmod | grep -e ip_vs -e nf_conntrack_ipv4 Install Docker yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce-20.10.8 cat \u003e /etc/docker/daemon.json \u003c\u003cEOF {\"exec-opts\": [\"native.cgroupdriver=systemd\"]} EOF usermod -G centos,adm,wheel,systemd-journal,docker,root centos systemctl start docker systemctl enable docker Disable postfix and create required directories systemctl stop postfix systemctl disable postfix mkdir -p /opt/k8s/{bin,work} /etc/{kubernetes,etcd}/cert ","date":"2021-12-21","objectID":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/:1:0","tags":["Kubernetes"],"title":"Setup Kubernetes Cluster Using kubeadm","uri":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/"},{"categories":["Skills"],"content":"Setup the Cluster Note: Replace the version 1.22.1 as required Install kubeadm, kubelet and kubectl on all Nodes cat \u003e /etc/yum.repos.d/kubernetes.repo \u003c\u003cEOF [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 EOF yum install -y kubelet-1.22.1 kubeadm-1.22.1 kubectl-1.22.1 systemctl enable kubelet systemctl start kubelet Pull required Docker images on all Nodes kubeadm config images pull --kubernetes-version v1.22.1 Run init on kubeadm01(replace the CIDR of Node, Pod and Service networks as required) # Networks: # Node: 172.31.0.0/16 # Pod: 10.192.0.0/16 # Service: 10.254.0.0/16 [root@kubeadm01 ~]# kubeadm init --apiserver-advertise-address=0.0.0.0 \\ --apiserver-bind-port=6443 \\ --kubernetes-version=v1.22.1 \\ --pod-network-cidr=10.192.0.0/16 \\ --service-cidr=10.254.0.0/16 \\ --image-repository=k8s.gcr.io \\ --ignore-preflight-errors=swap \\ --token-ttl=0 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.31.8.8:6443 --token 2333y7.y7xev857t8n4w5em \\ --discovery-token-ca-cert-hash sha256:df7857bdae645dad4072db71ae9e92efd248ead2d8fb184edd1720a4cddc5049 Create .kube/config on kubeadm01 [root@kubeadm01 ~]# mkdir -p $HOME/.kube [root@kubeadm01 ~]# cp /etc/kubernetes/admin.conf $HOME/.kube/config [centos@kubeadm01 ~]$ mkdir -p $HOME/.kube [centos@kubeadm01 ~]$ sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config [centos@kubeadm01 ~]$ sudo chown $(id -u):$(id -g) $HOME/.kube/config Join the cluster via command on kubeadm02 and kubeadm03(repace the values as required) [root@kubeadm02 ~]# kubeadm join 172.31.8.8:6443 --ignore-preflight-errors=swap \\ --token 2333y7.y7xev857t8n4w5em \\ --discovery-token-ca-cert-hash sha256:df7857bdae645dad4072db71ae9e92efd248ead2d8fb184edd1720a4cddc5049 [root@kubeadm03 ~]# kubeadm join 172.31.8.8:6443 --ignore-preflight-errors=swap \\ --token 2333y7.y7xev857t8n4w5em \\ --discovery-token-ca-cert-hash sha256:df7857bdae645dad4072db71ae9e92efd248ead2d8fb184edd1720a4cddc5049 This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. Check all Nodes status on kubeadm01 [centos@kubeadm01 ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION kubeadm01 Ready control-plane,master 5m v1.22.1 kubeadm02 Ready \u003cnone\u003e 7m v1.22.1 kubeadm03 Ready \u003cnone\u003e 9m v1.22.1 Install Flannel on kubeadm01(replace the CIDR of Pod network as required) [centos@kubeadm01 ~]$ mkdir flannel [centos@kubeadm01 ~]$ cd flannel [centos@kubeadm01 flannel]$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml [centos@kubeadm01 flannel]$ vim kube-flannel.yml net-conf.json: | { \"Network\": \"10.192.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } [centos@kubeadm01 flannel]$ kubectl create -f kube-flannel.yml Fix scheduler and controller-manager on kubeadm01 [centos@kubeadm01 ~]$ sudo cp -rpa /etc/kubernetes/manifests/etc/kubernetes/manifests.default [centos@kubeadm01 ~]$ sudo sed -i '/port=0/d' /etc/kubernetes/manifests/kube-scheduler.yaml [centos@kubeadm01 ~]$ sudo sed -i '/port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml [centos@kubeadm01 ~]$ sudo systemctl restart kubelet Check clusterservice status on kubeadm01 [centos@kube","date":"2021-12-21","objectID":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/:2:0","tags":["Kubernetes"],"title":"Setup Kubernetes Cluster Using kubeadm","uri":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/"},{"categories":["Skills"],"content":"Optimize the Cluster Change the kube-proxy mode to ipvs, by default it’s iptables kubectl edit configmap kube-proxy -n kube-system mode: \"ipvs\" kubectl get pods -n kube-system | grep kube-proxy | awk '{print $2}' | xargs kubectl -n kube-system delete pods","date":"2021-12-21","objectID":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/:3:0","tags":["Kubernetes"],"title":"Setup Kubernetes Cluster Using kubeadm","uri":"/en/2021/12/setup-kubernetes-cluster-using-kubeadm/"},{"categories":["Skills"],"content":"Here are some of my personal bookmarks about technology solutions and insights.","date":"2021-12-19","objectID":"/en/2021/12/solutions-and-insights/","tags":["Solutions","Insights"],"title":"Solutions and Insights","uri":"/en/2021/12/solutions-and-insights/"},{"categories":["Skills"],"content":"Here are some of my personal bookmarks about technology solutions and insights. Gartner Insights https://www.gartner.com/en/insights Gartner Special Reports https://www.gartner.com/en/products/special-reports Enterprise Software and Service Reviews https://www.gartner.com/reviews/home ThoughtWorks Insights https://www.thoughtworks.com/insights ThoughtWorks Radar https://www.thoughtworks.com/radar IBM Architecture Center https://www.ibm.com/cloud/architecture DevOps architecture https://www.ibm.com/cloud/architecture/architectures/devOpsArchitecture Data and AI architecture https://www.ibm.com/cloud/architecture/architectures/dataAIArchitecture Cloud-native architecture https://www.ibm.com/cloud/architecture/architectures/cloud-native Cloud data lake https://www.ibm.com/cloud/architecture/architectures/cloud-data-lake AOE Technology Radar https://www.aoe.com/techradar/ AWS Solutions Library https://aws.amazon.com/solutions/ CI/CD Pipeline https://aws.amazon.com/getting-started/hands-on/set-up-ci-cd-pipeline/ Data Lake House https://aws.amazon.com/big-data/datalakes-and-analytics/data-lake-house/ CNCF End User Technology Radar https://radar.cncf.io Developer Roadmaps https://roadmap.sh DevOps Roadmap https://roadmap.sh/devops Databricks Solutions https://databricks.com/solutions Migrate to Lakehouse Platform https://databricks.com/solutions/migration Matt Turck https://mattturck.com Machine Learning, AI and Data (MAD) Landscape https://mattturck.com/category/big-data/ G2 Business Software and Services Reviews https://www.g2.com Accenture Insights Blog https://www.accenture.com/nl-en/blogs/insights Artificial Intelligence https://www.accenture.com/us-en/insights/artificial-intelligence-summary-index Forrester Predictions https://www.forrester.com/predictions ","date":"2021-12-19","objectID":"/en/2021/12/solutions-and-insights/:0:0","tags":["Solutions","Insights"],"title":"Solutions and Insights","uri":"/en/2021/12/solutions-and-insights/"},{"categories":["Notes"],"content":"The core elements of deliberate practice include focus, feedback and fix, stepping out of the comfort zone, extensive training, and mentorship.","date":"2021-12-11","objectID":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/","tags":["Growth","Self-management"],"title":"Notes from \"Peak: Secrets from the New Science of Expertise\"","uri":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/"},{"categories":["Notes"],"content":"The core elements of deliberate practice include focus, feedback and fix, stepping out of the comfort zone, extensive training, and mentorship. ","date":"2021-12-11","objectID":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/:0:0","tags":["Growth","Self-management"],"title":"Notes from \"Peak: Secrets from the New Science of Expertise\"","uri":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/"},{"categories":["Notes"],"content":"Preface Anders Ericsson became famous for his work on what he called “deliberate practice”, a set of recipes that could help someone gain expertise in an area. In this readable and well-researched book, he expands upon this concept and brings several time-tested and scientifically reviewed ideas to bear on the search for perfection in our lives. ","date":"2021-12-11","objectID":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/:1:0","tags":["Growth","Self-management"],"title":"Notes from \"Peak: Secrets from the New Science of Expertise\"","uri":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/"},{"categories":["Notes"],"content":"Summary The core elements of deliberate practice include focus, feedback and fix, stepping out of the comfort zone, extensive training, and mentorship. Here is a summary of some key points from the book: Anyone can improve through focus and effective practice. If you are not improving, that’s because you are not practicing in the correct way. 3F rules: Focus. Feedback. Fix it: i.e. You need to identify what’s the exact nature of the skill you want to learn, who’s mastering that, and what practice made it happen. Then break down that skill into small pieces, and design the ways suitable for your situation to enhance that. While continuing training, you need to monitor your performance(or have an experienced teacher do that for you), get feedback about your weakness, and then revise training to specifically address them. Building mental representations are all about observing experts’ performance, spotting the difference between yours and theirs, and finding ways to minimize that difference. That’s not about what you know, but about how you are trying to do and modifying things. At the beginning, IQ and talents can get you start quickly. However, in the long run, it’s the ones who practice more who prevails, not the ones who had some advantage in intelligence or some other talents. In order to develop skill, you need to put in many hours of practice, typically hundreds or thousands of hours of practice over the course of many years. There are no shortcuts. To be effective, the practice needs to be “deliberate”, which means working with a skilled coach, focusing on the development of fundamental skills in the early stages of learning, giving full attention and concentration during the practice, continually stretching beyond comfort zones (which means the practice isn’t fun, so a key challenge is maintaining motivation), setting short-term goals, evolving the practice based on monitoring of performance, and continually refining mental models to make them more sophisticated and extensive. ","date":"2021-12-11","objectID":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/:2:0","tags":["Growth","Self-management"],"title":"Notes from \"Peak: Secrets from the New Science of Expertise\"","uri":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/"},{"categories":["Notes"],"content":"Thinking After reading this book, I have a new understanding of the “10,000 hour rule” and the idea of “natural talent”, I understand that an efficient way of practicing can greatly reduce the training time in certain areas, and all geniuses have also put in a lot of practices. I also realized that people’s skills deteriorate as they get older because they have reduced or stopped practicing. I will keep learning as a lifelong learner and keep practicing deliberately to make life full of possibilities😁. ","date":"2021-12-11","objectID":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/:3:0","tags":["Growth","Self-management"],"title":"Notes from \"Peak: Secrets from the New Science of Expertise\"","uri":"/en/2021/12/notes-from-peak-secrets-from-the-new-science-of-expertise/"},{"categories":["Skills"],"content":"The recognition principles and processes I have learned from the CAPTCHA recognition are really valuable.","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"The recognition principles and processes I have learned from the CAPTCHA recognition are really valuable. ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:0:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"Background My current company has a good learning and sharing atmosphere，sometimes holds various geek competitions, this one is about CAPTCHA recognition. ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:1:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"Processes At first, I was thinking of finding some existing codes from GitHub and just modifying some parameters. Then I tried some programs that seemed to be able to train and recognize without any feature engineering, but the actual results were terrible. During the competition, colleagues kept showing their stage results in the group, and they all did feature engineering. So I also searched the articles in this area and finally completed it successfully with a recognition rate of over 99%. The main processes are as follows. 1.Collect samples Only 100 sample CAPTCHAs were provided to all participants in this CAPTCHA recognition contest. 2.Feature engineering Using the OpenCV library to analyze the CAPTCHA samples, remove the noise such as various random lines, and highlight the characters in the CAPTCHA such as 5UFQ. 3.Manual labeling Manually recognize and rename each original CAPTCHA as its characters such as 5UFQ.jpg, then cut the CAPTCHAs into individual character images after feature engineering and saved as: 5UFQ_5.jpg, 5UFQ_U.jpg, 5UFQ_F.jpg, 5UFQ_Q.jpg. 4.Train the model Use the above images with single characters as training set, and the characters of each image as labels, build a mapping table of images to labels. Perform the OpenCV’s built-in KNN similarity model for Machine learning, to train a model that can recognize similar CAPTCHAs. 5.Recognition Use some original CAPTCHAs samples as the test dataset, remove the noise by feature engineering, then use the trained model to recognize them and save the results in a CSV file. ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:2:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"Details ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"1. Collect samples To prevent people from using the existing codes on the Internet and counting the results out by violence, only 100 image samples were provided to all participants. In practice, a lot more CAPTCHAs samples should be collected. ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:1","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"2. Feature engineering Use the CAPTCHA with characters of 5UFQ as an example. Import the libraries. import os import numpy as np from matplotlib import pyplot as plt import cv2 Read and display the original sample. filepath='imgs/train/5UFQ.jpg' im=cv2.imread(filepath) plt.imshow(im[:,:,[2,1,0]]) plt.show() Convert the image from RGB to grayscale, to remove the color information. # convert the image from BGR into gray im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) plt.imshow(im_gray,cmap=\"gray\") plt.show() Binarize the image to give a distinct black-and-white effect. # image binarization, the default threshold is 127 ret, im_inv = cv2.threshold(im_gray,127,255,cv2.THRESH_BINARY_INV) plt.imshow(im_inv,cmap=\"gray\") plt.show() Use the Gaussian Blur to reduce noise and details, with the visual effect of looking at the image through a translucent frosted screen. # image denoising using the gaussian blur kernel = 1/16*np.array([[1,2,1], [2,4,2], [1,2,1]]) im_blur = cv2.filter2D(im_inv,-1,kernel) plt.imshow(im_blur,cmap=\"gray\") plt.show() Then Binarize again to eliminate the noise such as streaks # image binarization, after debugging, found that 185 is a better value for the blurred CAPTCHAs ret, im_res = cv2.threshold(im_blur,185,255,cv2.THRESH_BINARY) plt.imshow(im_res,cmap=\"gray\") plt.show() ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:2","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"3. Manual labeling Manually identify each original sample and rename it as the characters. By looking at all the sample CAPTCHA images, it is easy to see that each character is in a very consistent area in the image, which greatly reduces the difficulty of determining the character regions and cutting the images. # after debugging, found that all CAPTCHAs characters are same size and positions are quite fit # so just find out the position values of each CAPTCHA character and get the images data roi_dict[0] = im_res[4:25, 8:28] roi_dict[1] = im_res[4:25, 38:58] roi_dict[2] = im_res[4:25, 68:88] roi_dict[3] = im_res[4:25, 98:118] # return all CAPTCHAs characters as a dictionary return roi_dict Cut images that have been feature engineered into individual character images. def cut_img(train_dir,cut_dir,suffix): # walk through the directory for root,dirs,files in os.walk(train_dir): for f in files: # get the file path filepath = os.path.join(root,f) # check the file suffix filesuffix = os.path.splitext(filepath)[1][1:] if filesuffix in suffix: # get the images data of each CAPTCHA character roi_dict = fix_img(filepath) # cut each CAPTCHA character with the filename incluing the label for i in sorted(roi_dict.keys()): cv2.imwrite(\"{0}/{1}_{2}.jpg\".format(cut_dir,f.split('.')[0],f[i]),roi_dict[i]) # close cv2 write operation cv2.waitKey(0) return True ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:3","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"4. Train the model Perform the OpenCV’s built-in KNN similarity model to train a model that can recognize similar CAPTCHAs. def train_model(cut_dir,suffix): # create an empty dataset to store the information of CAPTCHAs characters samples = np.empty((0, 420)) # create an empty lables list labels = [] # walk through the directory for root,dirs,files in os.walk(cut_dir): for f in files: filepath = os.path.join(root,f) filesuffix = os.path.splitext(filepath)[1][1:] if filesuffix in suffix: filepath = os.path.join(root,f) # read the label of each CAPTCHA character label = f.split(\".\")[0].split(\"_\")[-1] labels.append(label) # store the CAPTCHA character data into samples dataset im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE) sample = im.reshape((1, 420)).astype(np.float32) samples = np.append(samples, sample, 0) samples = samples.astype(np.float32) # labels-label_id mapping unique_labels = list(set(labels)) unique_ids = list(range(len(unique_labels))) label_id_map = dict(zip(unique_labels, unique_ids)) id_label_map = dict(zip(unique_ids, unique_labels)) label_ids = list(map(lambda x: label_id_map[x], labels)) label_ids = np.array(label_ids).reshape((-1, 1)).astype(np.float32) # train the model with KNN model = cv2.ml.KNearest_create() model.train(samples, cv2.ml.ROW_SAMPLE, label_ids) # return the model and labels-label_id mapping return {'model':model,'id_label_map':id_label_map} ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:4","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"5. Recognition Load test dataset, remove the noise by feature engineering, then use the trained model to recognize CAPTCHAs and save the results in a CSV file. def rek_img(model_dict,rek_dir,suffix,results_csv): # get the model and labels-label_id mapping model = model_dict['model'] id_label_map = model_dict['id_label_map'] label_dict = {} # walk through the directory for root,dirs,files in os.walk(rek_dir): for f in files: filepath = os.path.join(root,f) filesuffix = os.path.splitext(filepath)[1][1:] if filesuffix in suffix: # get the images data of each CAPTCHA character roi_dict = fix_img(filepath) # get the value of each CAPTCHA character from the model for i in sorted(roi_dict.keys()): sample = roi_dict[i].reshape((1, 420)).astype(np.float32) ret, results, neighbours, distances = model.findNearest(sample, k = 3) label_id = int(results[0,0]) label = id_label_map[label_id] label_dict[i] = label # convert all CAPTCHA characters values into a string result_str = ''.join(str(v) for k,v in sorted(label_dict.items())) # append the result into a csv with open(results_csv, \"a\") as myfile: myfile.write(\"{0},{1}\\n\".format(f,result_str)) myfile.close() return True ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:3:5","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"Summary The CAPTCHA images in this competition were more distinctive and easier to feature engineer, so in the end, although 10k CAPTCHA images were used as the test set, many colleagues still achieved recognition accuracy of over 90%, and the winner’s accuracy even reached an incredible 100%. For me, the recognition principles and processes I have learned from the CAPTCHA recognition are really valuable. ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:4:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"References Source code: captchas-opencv-rek Inspired by: https://www.cnblogs.com/lizm166/p/9969647.html ","date":"2021-11-24","objectID":"/en/2021/11/python-opencv-captcha-recognition-in-action/:5:0","tags":["OpenCV","Machine Learning"],"title":"Python OpenCV CAPTCHA Recognition in Action","uri":"/en/2021/11/python-opencv-captcha-recognition-in-action/"},{"categories":["Skills"],"content":"There are differences in the most appropriate CI/CD pipelines in different environments, especially the toolchains, but the general process and thinking should be similar.","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"There are differences in the most appropriate CI/CD pipelines in different environments, especially the toolchains, but the general process and thinking should be similar. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:0:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"Background At the beginning of the project, the architect asked us to design a DevOps CI/CD (Continuous Integration, Continuous Deployment/Delivery) solution to improve productivity. By taking some best practices into account and combining them with our own situation, we have designed a DevOps CI/CD pipeline which is more general and can balance security, audit, and the ability to take over existing on-premises infrastructure and services. This solution has been successfully implemented and rolled out to other projects within the company. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:1:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"What is DevOps ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:2:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"1. The car industry Everyone knows that most factories already have the automation assembly lines for their products. It is a well-known fact that the car industry went through an evolution and perfected a highly efficient approach to manufacturing cars. It went from highly custom cars built manually, to automated parts production with manual assembly, then to fully automated assembly lines. The car industry perfected a very lean manufacturing process. Compare with the car industry, our traditional software development models have already fallen behind a lot. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:2:1","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"2. DevOps is a culture What is DevOps? Some people may think that DevOps is a set of software and tools which can make development and deployment better. DevOps is more than that, DevOps is a culture, it’s more about the Team and Organization, DevOps is a collaboration of Dev and Ops. The team should work together to design automation processes about the development, testing and deployment, and fix issues. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:2:2","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"3. DevOps toolchains The DevOps lifecycle starts with plan, build, continuous integration, and deploy, operate, then feedback and plan again. Around every part of this lifecycle, there are lots of applications and services. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:2:3","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"Disadvantages of Traditional Development ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:3:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"1. Seven types of waste for manufacturing For manufacturing, it‘s very easy to waste time and resources on over production, transport, movement, over processing, waiting, inventory and products defects. Just because most of them still require manual works and no continuous deployment processes. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:3:1","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"2. Disadvantages of traditional software development models For the traditional software development models, there are also some disadvantages. Manual Intervention Such interventions often lead to non-repeatable procedures and introduce human errors. Manual intervention is also a hindrance to agility especially when it comes to testing and deployment. Inconsistent Environment Teams often waste days and weeks fixing bugs that are caused by inconsistent environment, wasting resources and time. Restricted Monitoring They are solely dependent on manual checks performed by developers. This process introduces errors and delays the delivery or rollout time of products. Lack of Shared Ownership Traditional software development models lack the concept of shared ownership, which creates communication problems in organizations. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:3:2","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"DevOps CI/CD Best Practices ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:4:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"1. Lightweight and easy This is a lightweight and easy CI/CD pipeline solution by GitLab + Docker + Ansible from a fast-growing startup. GitLab is a source code management and also a continuous integration tool, Docker for containers and Ansible for configuration as code. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:4:1","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"2. All on Kubernetes In this solution, it suggests building the CI/CD pipeline based on cloud services and Kubernetes. This is a popular solution named “All on Kubernetes” and many internet companies use this solution. But “All on Kubernetes” requires all applications to be containerized, it is not friendly for complex environments and the traditional software. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:4:2","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"3. IBM DevOps architecture The DevOps solution of IBM suggests using DevOps and Cloud platform to help organizations to accelerate the applications development and delivery lifecycle. It is a very good architecture to tell all the necessary steps and parts in DevOps. More details on https://www.ibm.com/cloud/architecture/architectures/devOpsArchitecture ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:4:3","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"4. End-to-end CI/CD pipeline This end-to-end delivery pipeline of continuous integration and deployment across leading cloud platforms. In each step, it suggests the popular services and tools, and includes most steps in the IBM DevOps architecture. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:4:4","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"Better DevOps CI/CD for Our Own ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:5:0","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"1. DevSecOps What we most concern is security, so we need the DevSecOps, other than DevOps. The DevSecOps does not have to sacrifice automation processes, it only needs to audit them before changes are implemented. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:5:1","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"2. Deliver to Test environment To deliver objects through a secure path to Test environment, this Test environment needs four networks: A, B, C and W. A can connect to Data Center and shared services like JIRA, GitLab, and Nexus. B can create servers and deploy services. C can connect to some trusted official repositories, such as Maven, and the Nexus is integrated with security scan tools such as Symantec and Nessus. W can develop software under the security scan and audit. In this way, we could ensure that all the packages in the test environment are scanned and secure. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:5:2","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Skills"],"content":"3. Our DevSecOps CI/CD pipeline This is the End-to-End DevSecOps CI/CD pipeline for our own. It includes all necessary steps, across three different environments, with the most popular automation tools and Cloud services, including the infrastructure as code and configurations as code, integrates with security tools, with the manually approve function based on the tests and security reports. It includes the following items: Project Management Think about a story, document it on Confluence and create tasks on JIRA, integrate JIRA with GitLab. Code Management Do the Code Review, push the codes to GitLab, and GitLab will automatically trigger the related Jenkins jobs to do the Code Analysis, Build and Unit Test, then deploy the packages to Nexus repository. Artifact Management Automatically scan the packages in Nexus with Nessus. Infrastructure as Code and Configuration as Code Check whether the required infrastructure is ready or not, if not ready, create the infrastructure as code automatically, then deploy the packages and configurations to the servers or Kubernetes clusters. Integration Tests and Security Tests Scan the applications with security tools and test the availability. Audit and Approval Audit the whole pipeline and wait for the manual review, when the release manager approved, automatically continue to run the deployment and integration tests in staging environment, then wait for another review and approval for the production release. This DevSecOps CI/CD pipeline is more general and can balance security, audit, and the ability to take over existing on-premises infrastructure and services. For sure, there are differences in the most appropriate CI/CD pipelines in different environments, especially the toolchains, but the general process and thinking should be similar. ","date":"2021-11-21","objectID":"/en/2021/11/devops-cicd-pipeline-in-action/:5:3","tags":["DevOps","CICD"],"title":"DevOps CI/CD Pipeline in Action","uri":"/en/2021/11/devops-cicd-pipeline-in-action/"},{"categories":["Thinking"],"content":"I will keep this state, no more slacking off, and be positive for every beautiful day ahead.","date":"2021-11-09","objectID":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/","tags":["Growth","Family"],"title":"Cherish the Time to Meet A Bright Future","uri":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/"},{"categories":["Thinking"],"content":"I will keep this state, no more slacking off, and be positive for every beautiful day ahead. ","date":"2021-11-09","objectID":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/:0:0","tags":["Growth","Family"],"title":"Cherish the Time to Meet A Bright Future","uri":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/"},{"categories":["Thinking"],"content":"Time A few days ago, I helped my cousin download some geospatial programming materials, then we chatted for a while. I shared with her how my life has been going lately, which is a regular daily routine of rest, exercise, reading and studying, which made her very envious. She said: “I really envy you for having so much time to study, you should cherish it.” My cousin worked for the Chinese Academy of Sciences for a long time after she graduated with her PhD. In the last few years, she has changed her job to be a teacher at a university. She is very capable and wants to do many things, but she does not have enough time. She said: “I’m too busy, I have a lot of ideas but no enough time and energy to work on them, there are a bunch of things I need to deal with, and I have to take care of my kid.” Her words reminded me of a quote from my college dean in a class last year: “Now my children are living abroad and my husband is very busy. I really feel so happy! Because I can finally have a lot of time to devote to the topics I want to research and keep delving into them, which also makes it easier for me to produce good results.” After I forwarded it to my cousin, she said, “Haha, many people would find such days lonely, but of course I would feel very happy, I enjoy the state of being able to continue to work all day.” ","date":"2021-11-09","objectID":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/:1:0","tags":["Growth","Family"],"title":"Cherish the Time to Meet A Bright Future","uri":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/"},{"categories":["Thinking"],"content":"Reflection Back to myself, it has been more than two years since I left Chengdu and came to Shenzhen. I have grown a lot in the past two years, but have not taken care of my family. I didn’t take the responsibility for the roles of a husband, a father and a son. Although I kept saying it’s all about being able to give my family a better life, and I felt that way inside. But I still wasted too much time and neglected my health in the past two years. I should really wake up and realize how rare it is to have a lot of free time and energy in this middle age! I should spend as much of this precious time as possible on studying and exercising, to gain knowledge faster and have a healthy body, so that I can really live up to the time, and create a better life for my family! ","date":"2021-11-09","objectID":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/:2:0","tags":["Growth","Family"],"title":"Cherish the Time to Meet A Bright Future","uri":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/"},{"categories":["Thinking"],"content":"Future This morning, I shared a message to my friends: “Go to bed early, wake up early, read a few pages, memorize dozens of English words, go out leisurely, breathe fresh air, feel the warm sun shining on my body, look at the blue sky and white clouds, listen to the Geek Time App, sometimes walk, sometimes jog, feel the greenery along the way, go through the sports park with a good atmosphere for exercise, enter the company in good spirits and start a brand new day!” This is a true record of how I felt every morning in the last two weeks, I will keep this state, no more slacking off, and be positive for every beautiful day ahead. ","date":"2021-11-09","objectID":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/:3:0","tags":["Growth","Family"],"title":"Cherish the Time to Meet A Bright Future","uri":"/en/2021/11/cherish-the-time-to-meet-a-bright-future/"},{"categories":["Skills"],"content":"It is also important to deploy HTTPS services for internal private domains.","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"It is also important to deploy HTTPS services for internal private domains. ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:0:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"Background Today, almost all Internet sites and applications are already running on HTTPS site-wide, but many companies still have internal applications running on insecure HTTP. In my current job, internal security requirements are very strict and all applications must support HTTPS. So I’m sharing my notes on how to generate self-signed SSL certificates for your reference. The abbreviations of professional terminology involved: HTTPS - HyperText Transfer Protocol Secure SSL - Secure Sockets Layer TLS - Transport Layer Security SSH - Secure Shell FTP - File Transfer Protocol SHA - Secure Hash Algorithm RSA - Rivest, Shamir, Adleman (Asymmetric encryption algorithm) CSR - Certificate Signing Request CA - Certificate Authority X.509 - A standard defining the format of public-key certificates DER - Distinguished Encoding Rules CER/CRT - Certificate PKCS - Public Key Cryptography Standards P7B - PKCS#7 Binary PFX - Personal Information Exchange JKS - Java KeyStore PEM - Privacy-Enhanced Mail ELB - Elastic Load Balancing ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:1:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"Generate SSL Certificates ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:2:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"1. Generate root certificate and import to OS First, generate the rootCA.heylinux.com.pem, set the encryption length to 4096 to meet security level requirements. openssl genrsa -out rootCA.heylinux.com.key 4096 Generate rootCA.heylinux.com.pem, set encryption sha256, valid days 3650, organization /C=CN/ST=Sichuan/L=Chengdu/O=HEYLINUX/OU=IT/CN=SRE. The rootCA.heylinux.com.pem will be used to generate the server key and the trusted root certification authorities of browsers such as Chrome. The -subj parameter is more straightforward than the interactive steps. openssl req -x509 -new -nodes -key rootCA.heylinux.com.key -sha256 -days 3650 -out rootCA.heylinux.com.pem -subj \"/C=CN/ST=Sichuan/L=Chengdu/O=HEYLINUX/OU=IT/CN=SRE\" Import rootCA.heylinux.com.pem to OS via Chrome, so the Chrome could trust the server certificate generated based on the root certificate. ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:2:1","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"2. Generate server certificate Create ssl.conf, enable serverAuth and clientAuth, wildcard DNS names as *.heylinux.com and *.cloud.heylinux.com. vim ssl.conf subjectAltName = @alt_names authorityKeyIdentifier = keyid,issuer keyUsage = digitalSignature,keyEncipherment,nonRepudiation,dataEncipherment extendedKeyUsage = serverAuth,clientAuth basicConstraints = CA:FALSE subjectKeyIdentifier = hash [alt_names] DNS.1 = *.heylinux.com DNS.2 = *.cloud.heylinux.com Generate server certificate Key star.heylinux.com.key, organization /C=CN/ST=Sichuan/L=Chengdu/O=HEYLINUX/OU=IT/CN=*.heylinux.com. openssl req -new -nodes -out star.heylinux.com.csr -newkey rsa:4096 -keyout star.heylinux.com.key -subj \"/C=CN/ST=Sichuan/L=Chengdu/O=HEYLINUX/OU=IT/CN=*.heylinux.com\" Generate star.heylinux.com.crt, set encryption sha256, valid days 3650. openssl x509 -req -in star.heylinux.com.csr -CA rootCA.heylinux.com.pem -CAkey rootCA.heylinux.com.key -CAcreateserial -out star.heylinux.com.crt -days 3650 -sha256 -extfile ssl.conf View server certificate information openssl x509 -text -noout -in star.heylinux.com.crt Certificate: Data: Version: 3 (0x2) Serial Number: 10:f8:04:91:06:d8:4a:eb:46:ee:90:80:b4:9c:94:4e:ac:60:7b:3e Signature Algorithm: sha256WithRSAEncryption Issuer: C = CN, ST = Sichuan, L = Chengdu, O = HEYLINUX, OU = IT, CN = SRE Validity Not Before: Nov 7 04:38:21 2021 GMT Not After : Nov 5 04:38:21 2031 GMT Subject: C = CN, ST = Sichuan, L = Chengdu, O = HEYLINUX, OU = IT, CN = *.heylinux.com Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (4096 bit) ... Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Alternative Name: DNS:*.heylinux.com, DNS:*.cloud.heylinux.com X509v3 Authority Key Identifier: keyid:36:20:25:BA:1A:D9:36:A3:5C:E7:94:30:7F:76:D5:DF:5D:03:99:C9 X509v3 Key Usage: Digital Signature, Non Repudiation, Key Encipherment, Data Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: CA:FALSE X509v3 Subject Key Identifier: DB:0D:E8:00:A4:3E:95:71:26:1A:0B:57:4C:3C:3B:33:D3:67:F0:16 ... ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:2:2","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"3. Generate certificate bundles in different formats Certificate bundle usually contains multiple server certificates. Generate star.heylinux.com.p12 in PKCS12, set passphrase P_Ss0rdT, alias heylinux_com. It could be used for servers such as Tomcat. openssl pkcs12 -export -in star.heylinux.com.crt -inkey star.heylinux.com.key -password pass:P_Ss0rdT -name heylinux_com -out star.heylinux.com.p12 Convert star.heylinux.com.p12 to star.heylinux.com.jks in JKS, set passphrase P_Ss0rdT, alias heylinux_com. It could be used for servers such as Tomcat. keytool -importkeystore -deststorepass P_Ss0rdT -destkeystore star.heylinux.com.jks -srcstorepass P_Ss0rdT -srckeystore star.heylinux.com.p12 -srcstoretype PKCS12 Convert star.heylinux.com.p12 to star.heylinux.com.pem in PEM, alias heylinux_com,no passphrase. It could be used for servers such as Apache、Nginx、HAProxy and AWS ELB. openssl pkcs12 -password pass:P_Ss0rdT -in star.heylinux.com.p12 -out star.heylinux.com.pem -nodes ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:2:3","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"4. Convert root certificate to JKS format Convert rootCA.heylinux.com.pem to rootCA.heylinux.com.jks, alias heylinux_com, for Java applications to verify server certificate. The principle is similar to the above “import the root certificate to OS via Chrome, so the Chrome could trust the server certificate generated based on the root certificate”. # Convert rootCA.heylinux.com.pem to rootCA.heylinux.com.der openssl x509 -in rootCA.heylinux.com.pem -out rootCA.heylinux.com.der -outform der # View rootCA.heylinux.com.der keytool -v -printcert -file rootCA.heylinux.com.der # Convert rootCA.heylinux.com.der to rootCA.heylinux.com.jks, alias heylinux_com keytool -importcert -alias heylinux_com -keystore rootCA.heylinux.com.jks -storepass P_Ss0rdT -file rootCA.heylinux.com.der # View rootCA.heylinux.com.jks keytool -keystore rootCA.heylinux.com.jks -storepass P_Ss0rdT -list ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:2:4","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"Convert and View SSL Certificates ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"1. From P7B to PEM Convert SubCA_2.p7b to rootCA.heylinux.com.pem. openssl pkcs7 -inform DER -print_certs -in SubCA_2.p7b -out rootCA.heylinux.com.pem ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:1","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"2. From PEM to JKS Convert rootCA.heylinux.com.pem to rootCA.heylinux.com.jks. # Convert rootCA.heylinux.com.pem to rootCA.heylinux.com.der openssl x509 -in rootCA.heylinux.com.pem -out rootCA.heylinux.com.der -outform der # Convert rootCA.heylinux.com.der to rootCA.heylinux.com.jks, alias heylinux_com keytool -importcert -alias heylinux_com -keystore rootCA.heylinux.com.jks -storepass P_Ss0rdT -file rootCA.heylinux.com.der ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:2","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"3. From PFX(P12) to PEM Convert star.heylinux.com.pfx to star.heylinux.com.pem, which including the items of star.heylinux.com.key and star.heylinux.com.crt. openssl pkcs12 -password pass:P_Ss0rdT -nodes -in star.heylinux.com.pfx -out star.heylinux.com.pem ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:3","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"4. From P12 to JKS Convert star.heylinux.com.p12 to star.heylinux.com.jks, alias heylinux_com. keytool -importkeystore -deststorepass P_Ss0rdT -destkeystore star.heylinux.com.jks -srcstorepass P_Ss0rdT -srckeystore star.heylinux.com.p12 -srcstoretype PKCS12 -alias heylinux_com ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:4","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"5. View certificates in different formats # View rootCA.heylinux.com.pem and star.heylinux.com.pem openssl x509 -noout -text -in rootCA.heylinux.com.pem openssl x509 -noout -text -in star.heylinux.com.pem # View star.heylinux.com.p12 keytool -list -v -keystore star.heylinux.com.p12 -storepass P_Ss0rdT -storetype PKCS12 # View star.heylinux.com.jks keytool -list -v -keystore star.heylinux.com.jks -storepass P_Ss0rdT ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:3:5","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"SSL Certificates without Root Certificate For applications such as NiFi, vsFTPd, the SSL certificates are mainly used for client and server verification, similar to the authentication between private and public keys in SSH, this kind of certificates could be used without the root certificate. Create tls.conf, set IPs as 10.8.5.7 and 10.2.3.4, wildcard DNS names as *.heylinux.com and *.cloud.heylinux.com, organization /C=CN/ST=Sichuan/L=Chengdu/O=HEYLINUX/OU=IT/CN=SRE. vim tls.conf [req] prompt = no req_extensions = req_ext distinguished_name = dn [dn] C = CN ST = Sichuan L = Chengdu O = HEYLINUX OU = IT CN = SRE [req_ext] subjectAltName = @alt_names [alt_names] IP.1 = 10.8.5.7 IP.2 = 10.2.3.4 DNS.1 = *.heylinux.com DNS.2 = *.cloud.heylinux.com Generate server certificates heylinux-ssl-keypair.key and heylinux-ssl-keypair.crt, set encryption sha256, valid days 3650, passphrase P_Ss0rdT. # Generate heylinux-ssl-keypair.key and heylinux-ssl-keypair.crt openssl req -x509 -newkey rsa:4096 -keyout heylinux-ssl-keypair.key -out heylinux-ssl-keypair.crt -days 3650 -sha256 -extensions req_ext -config tls.conf # Input password Generating a RSA private key ....................................+++++ writing new private key to 'heylinux-ssl-keypair.key' Enter PEM pass phrase: P_Ss0rdT Verifying - Enter PEM pass phrase: P_Ss0rdT # Convert certificates to certificate bundle heylinux-ssl-keypair.p12 openssl pkcs12 -export -in heylinux-ssl-keypair.crt -inkey heylinux-ssl-keypair.key -password pass:P_Ss0rdT -name heylinux_ssl_keypair -out heylinux-ssl-keypair.p12 # Input password Enter pass phrase for heylinux-ssl-keypair.key: P_Ss0rdT # View certificate bundle heylinux-ssl-keypair.p12 keytool -list -v -keystore heylinux-ssl-keypair.p12 -storepass P_Ss0rdT -storetype PKCS12 ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:4:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"Generate and View SSL Certificates by cfssl tools ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:5:0","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"1. Install cfssl tools Download cfssl, cfssljson, cfssl-certinfo and grant execute permission. sudo wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl sudo wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson sudo wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfo sudo chmod +x /usr/local/bin/cfssl* ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:5:1","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"2. Generate and view root certificate Create rootCA.json, generate the similar rootCA.heylinux.com.key(encryption length is only 2048 due to cfssl cannot set the key with different encryption length) and rootCA.heylinux.com.pem as above. { \"CA\": { \"expiry\": \"87600h\", \"pathlen\": 0 }, \"CN\": \"SRE\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"Sichuan\", \"L\": \"Chengdu\", \"O\": \"HEYLINUX\", \"OU\": \"IT\" } ] } cfssl gencert -initca rootCA.json | cfssljson -bare rootCA.heylinux.com 2021/11/13 05:11:48 [INFO] generating a new CA key and certificate from CSR 2021/11/13 05:11:48 [INFO] generate received request 2021/11/13 05:11:48 [INFO] received CSR 2021/11/13 05:11:48 [INFO] generating key: rsa-2048 2021/11/13 05:11:48 [INFO] encoded CSR 2021/11/13 05:11:48 [INFO] signed certificate with serial number 644632230923530854661361284854682897812867573233 ls -1 rootCA.heylinux.com.csr rootCA.heylinux.com-key.pem rootCA.heylinux.com.pem rootCA.json mv rootCA.heylinux.com-key.pem rootCA.heylinux.com.key View rootCA.heylinux.com.pem. cfssl-certinfo -cert rootCA.heylinux.com.pem { \"subject\": { \"common_name\": \"SRE\", \"country\": \"CN\", \"organization\": \"HEYLINUX\", \"organizational_unit\": \"IT\", \"locality\": \"Chengdu\", \"province\": \"Sichuan\", \"names\": [ \"CN\", \"Sichuan\", \"Chengdu\", \"HEYLINUX\", \"IT\", \"SRE\" ] }, \"issuer\": { \"common_name\": \"SRE\", \"country\": \"CN\", \"organization\": \"HEYLINUX\", \"organizational_unit\": \"IT\", \"locality\": \"Chengdu\", \"province\": \"Sichuan\", \"names\": [ \"CN\", \"Sichuan\", \"Chengdu\", \"HEYLINUX\", \"IT\", \"SRE\" ] }, \"serial_number\": \"644632230923530854661361284854682897812867573233\", \"not_before\": \"2021-11-12T20:53:00Z\", \"not_after\": \"2031-11-10T20:53:00Z\", \"sigalg\": \"SHA256WithRSA\", \"authority_key_id\": \"36:D1:86:6B:27:AE:24:EF:C7:B3:2B:25:E7:92:DE:F1:0:34:2B:E5\", \"subject_key_id\": \"36:D1:86:6B:27:AE:24:EF:C7:B3:2B:25:E7:92:DE:F1:0:34:2B:E5\", \"pem\": \"-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\\n\" } ","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:5:2","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Skills"],"content":"3. Generate and view server certificate Create ssl-config.json and ssl.json, generate the similar star.heylinux.com.key(encryption length is only 2048 due to cfssl cannot set the key with a different encryption length) and star.heylinux.com.crt as above. vim ssl-config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"server\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } vim ssl.json { \"CN\": \"*.heylinux.com\", \"hosts\": [ \"*.heylinux.com\", \"*.cloud.heylinux.com\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [{ \"C\": \"CN\", \"ST\": \"Sichuan\", \"L\": \"Chengdu\", \"O\": \"HEYLINUX\", \"OU\": \"IT\" }] } cfssl gencert -ca=rootCA.heylinux.com.pem -ca-key=rootCA.heylinux.com.key -config=ssl-config.json -profile=server ssl.json | cfssljson -bare star.heylinux.com mv star.heylinux.com.pem star.heylinux.com.crt mv star.heylinux.com-key.pem star.heylinux.com.key ls -1 rootCA.heylinux.com.csr rootCA.heylinux.com.key rootCA.heylinux.com.pem rootCA.json ssl-config.json ssl.json star.heylinux.com.crt star.heylinux.com.csr star.heylinux.com.key View star.heylinux.com.crt. cfssl-certinfo -cert star.heylinux.com.crt { \"subject\": { \"common_name\": \"*.heylinux.com\", \"country\": \"CN\", \"organization\": \"HEYLINUX\", \"organizational_unit\": \"IT\", \"locality\": \"Chengdu\", \"province\": \"Sichuan\", \"names\": [ \"CN\", \"Sichuan\", \"Chengdu\", \"HEYLINUX\", \"IT\", \"*.heylinux.com\" ] }, \"issuer\": { \"common_name\": \"SRE\", \"country\": \"CN\", \"organization\": \"HEYLINUX\", \"organizational_unit\": \"IT\", \"locality\": \"Chengdu\", \"province\": \"Sichuan\", \"names\": [ \"CN\", \"Sichuan\", \"Chengdu\", \"HEYLINUX\", \"IT\", \"SRE\" ] }, \"serial_number\": \"608638693485247133136510097809090433439285866629\", \"sans\": [ \"*.heylinux.com\", \"*.cloud.heylinux.com\" ], \"not_before\": \"2021-11-12T21:20:00Z\", \"not_after\": \"2031-11-10T21:20:00Z\", \"sigalg\": \"SHA256WithRSA\", \"authority_key_id\": \"2B:D4:44:57:4D:9:D8:9A:0:63:4C:5B:B8:78:F4:8F:45:9C:3C:F5\", \"subject_key_id\": \"4E:15:55:4A:34:EA:BA:69:3E:A5:F:40:74:16:52:F0:88:C3:7D:6F\", \"pem\": \"-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----\\n\" }","date":"2021-11-07","objectID":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/:5:3","tags":["Security","SSL"],"title":"How to Generate Self-signed SSL Certificates","uri":"/en/2021/11/how-to-generate-self-signed-ssl-certificates/"},{"categories":["Notes"],"content":"The lack of self-control is not only a psychological problem, but also influenced by physiology, psychological traps and various social factors.","date":"2021-11-06","objectID":"/en/2021/11/notes-from-the-willpower-instinct/","tags":["Psychology","Self-management"],"title":"Notes from \"The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\"","uri":"/en/2021/11/notes-from-the-willpower-instinct/"},{"categories":["Notes"],"content":"The lack of self-control is not only a psychological problem, but also influenced by physiology, psychological traps and various social factors. ","date":"2021-11-06","objectID":"/en/2021/11/notes-from-the-willpower-instinct/:0:0","tags":["Psychology","Self-management"],"title":"Notes from \"The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\"","uri":"/en/2021/11/notes-from-the-willpower-instinct/"},{"categories":["Notes"],"content":"Preface Dr. McGonigal of Stanford University offers a course called “The Science of Willpower” that has been described as “life-changing” by those who have participated in it. This course is the basis for the book “The Willpower Instinct”. Because I can’t stand my own procrastination and laziness, I have tried various methods to change, including making lists, writing plans, self-reflection, painful resolutions, and reading chicken soup, but I still repeatedly fall into degradation, leading to inner torment. I used to think that lack of self-control was just a psychological problem and a personality flaw. After reading the book “The Willpower Instinct”, I realized that the lack of self-control is not only a psychological problem, but also influenced by physiology, psychological traps and various social factors. As I read, I followed the examples, questions and action suggestions in the book to dissect my past habits and failures. I also got to know myself from a more scientific perspective and reconciled with myself, also learned some scientific and effective ways to improve my self-control. ","date":"2021-11-06","objectID":"/en/2021/11/notes-from-the-willpower-instinct/:1:0","tags":["Psychology","Self-management"],"title":"Notes from \"The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\"","uri":"/en/2021/11/notes-from-the-willpower-instinct/"},{"categories":["Notes"],"content":"Summary This book is more like a diagnostic book prescribed by a doctor to get to the root cause of our illness. We used to look for ways to correct and improve our self-control, but we neglected why we can’t control ourselves. This is a course that deals with psychology, physiology and even economics, and it gives a lot of good explanations for the problem of willpower. The book mentions many ways to enhance willpower, there is a good summary from a user on Douban, as follows. 1. Usual practice Focused breathing is a simple and effective meditation technique. Exercise is the way to restore physical fitness and willpower. Make sure your body has an adequate supply of food, which will give your brain more sustained energy. Consistent self-control in the little things will improve overall willpower. 2. Timely adjustment of physical and mental state A quick way to improve willpower: slow down your breathing and take 4-6 deep breaths per minute. A 5-minute “green workout” can reduce stress, get out of the office, find the nearest piece of green space, breathe fresh air and do some simple stretching activities. Effective ways to relieve stress. Exercise or participate in sports, pray or attend religious services, read, listen to music, spend time with friends, get a massage, go for a walk, meditate or yoga, and develop other creative hobbies. 3. Advance preparation after setting a goal 6-7 hours of sleep per day. Do the hardest things when your willpower is strongest, usually in the morning. Predict how you will be tempted and break promises in the process of achieving your goals, and imagine what you should do then to not give up. Put pressure on your future self by making medical appointments, buying an annual gym pass, going out shopping with only cash, putting your alarm clock at the other end of the room, etc. Imagine a better picture of the future. Write a letter to your future self. Describe to your future self what you will do now. Imagine that your future self will feel grateful for what your present self has done. Imagine the self you wish to be and the self you fear to be in a specific scenario, and then compare and contrast. To avoid repeating the mistakes of others who have lost their self-control, take a moment at the beginning of each day to think about your goals. Find a “group” to join that shares your goals. Being in such a group will make you feel that your goals are the social norm. Find someone close to you to be a role model for your willpower. 4. Facing temptation The “willpower muscle” can last longer with the right motivation. Tell yourself, “A lot of things that are hard now will become easy.” There is no difference between tomorrow and today. Tell yourself, “Do you really believe that you can eat this piece of fried chicken today and stick to your veggies all day tomorrow? Do you really believe you won’t study today but you’ll make up for it tomorrow? Don’t lie to yourself!” When expecting to get what tempts you, tell yourself the reality: “When you really go to ……, you’ll find it’s not as happy as you thought!” When you find yourself using the effort you’ve made as an excuse to indulge, tell yourself, “Because I ate vegetarian all day yesterday, so I can reward myself with a McDonald’s today. This is not the right logic. It should be: I ate vegetarian yesterday for the goal of losing weight, and I should do the same today.” Schedule a 10-minute waiting period in front of all temptations. If after 10 minutes you still want it, you can have it. But within the 10 minutes, you must always resist the temptation by thinking about the real goal. If possible, create some physical distance away from the temptation. When you are tempted to decide to do something that is contrary to your long-term interests, imagine that you have already achieved your ultimate goal. Then seriously ask yourself, “Have you really decided to give up your goal of losing weight for the meat in front of you?” Think what you think and fol","date":"2021-11-06","objectID":"/en/2021/11/notes-from-the-willpower-instinct/:2:0","tags":["Psychology","Self-management"],"title":"Notes from \"The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\"","uri":"/en/2021/11/notes-from-the-willpower-instinct/"},{"categories":["Notes"],"content":"Thinking While reading this book, I was surprised many times by the various conclusions analyzed from a physiological point of view, and gained many scientific and practical ways to enhance my willpower. I also gained a new understanding of self-control, stopped doubting myself, learned to reconcile with myself, accept my flaws, act more positively in a good direction, and have a clear vision of a better future😁. ","date":"2021-11-06","objectID":"/en/2021/11/notes-from-the-willpower-instinct/:3:0","tags":["Psychology","Self-management"],"title":"Notes from \"The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It\"","uri":"/en/2021/11/notes-from-the-willpower-instinct/"},{"categories":null,"content":"Author Dong GuoDōng Guō based on Mandarin Pinyin Damon/ˈdeɪmən/ inspired by the Linux daemon/ˈdiːmən/ process and a favored Hollywood actor Matt Damon Bio: A Hands-On Solutions Architect, a DevOps Manager, and a Lifelong Learner Specialties: Hybrid Cloud DevOps Big Data Cybersecurity ","date":"2021-11-02","objectID":"/en/about/:0:1","tags":null,"title":"About","uri":"/en/about/"},{"categories":null,"content":"TO DO LISTSince Feburary 2021 Credentials AWS ⋄ AWS Certified Solutions Architect - Professional ITIL ⋄ ITIL 4 Foundation Certificate in IT Service Management CKA ⋄ Certified Kubernetes Administrator CET6 ⋄ National College English Test Band 6 Degree ⋄ Master of Engineering Management GitLab ⋄ JiHu GitLab DevOps Expert ACP ⋄ Alibaba Cloud Certified Professional - Cloud Computing CISSP ⋄ Certified Information Systems Security Professional PMP ⋄ Project Management Professional CDMP ⋄ Certified Data Management Professional IELTS ⋄ International English Language Testing System - Overall Band Score: 7 Booklist Live for Death: My Experience on Dying The Willpower Instinct: How Self-Control Works, Why It Matters, and What You Can Do to Get More of It Peak: Secrets from the New Science of Expertise Pomodoro Technique Illustrated: The Easy Way to Do More in Less Time The Little Book of Lykke: Secrets of the World’s Happiest People Influence: The Psychology of Persuasion Nonviolent Communication: A Language of Life How an Economy Grows and Why It Crashes Rich Dad Poor Dad: What the Rich Teach Their Kids About Money That the Poor and Middle Class Do Not Tiny Habits: The Small Changes That Change Everything The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses How to Read a Book: The Classic Guide to Intelligent Reading The Crowd: A Study of the Popular Mind ","date":"2021-11-02","objectID":"/en/about/:0:2","tags":null,"title":"About","uri":"/en/about/"},{"categories":["Skills"],"content":"A multi-cloud strategy allows companies to select different cloud services from different providers.","date":"2021-11-01","objectID":"/en/2021/11/why-use-a-multi-cloud-strategy/","tags":["Cloud","Architect"],"title":"Why Use a Multi-Cloud Strategy","uri":"/en/2021/11/why-use-a-multi-cloud-strategy/"},{"categories":["Skills"],"content":"A multi-cloud strategy allows companies to select different cloud services from different providers. ","date":"2021-11-01","objectID":"/en/2021/11/why-use-a-multi-cloud-strategy/:0:0","tags":["Cloud","Architect"],"title":"Why Use a Multi-Cloud Strategy","uri":"/en/2021/11/why-use-a-multi-cloud-strategy/"},{"categories":["Skills"],"content":"Reasons A multi-cloud strategy allows companies to select different cloud services from different providers, implement a multi-cloud environment for the following reasons: Choice Multi-cloud infrastructure can combine the best services that each platform offers, thereby optimizing returns on cloud investments, gives flexibility and the ability to avoid vendor lock-in. Disaster Avoidance As a proverb says don’t put all of your eggs in the same basket. If one vendor happens to have infrastructure meltdown, attack or geopolitical risks, can quickly switch to another cloud service provider. Compliance Multi-cloud environments can help enterprises achieve the goals for governance, risk management and compliance regulations. ","date":"2021-11-01","objectID":"/en/2021/11/why-use-a-multi-cloud-strategy/:1:0","tags":["Cloud","Architect"],"title":"Why Use a Multi-Cloud Strategy","uri":"/en/2021/11/why-use-a-multi-cloud-strategy/"},{"categories":["Skills"],"content":"Architecture After investigating many multi-cloud platforms from different companies, then I found the following diagram of Fully Functional Multi-Cloud Management Platform Architecture from OpsAnyMake Ops Perfect. It shows the various service modules which need to be considered in a multi-cloud architecture, and it is easy to see that there are many difficulties and challenges in building a multi-cloud management platform. ","date":"2021-11-01","objectID":"/en/2021/11/why-use-a-multi-cloud-strategy/:2:0","tags":["Cloud","Architect"],"title":"Why Use a Multi-Cloud Strategy","uri":"/en/2021/11/why-use-a-multi-cloud-strategy/"},{"categories":["Notes"],"content":"Walked through the valley of the shadow of death and tasted the nectar of life.","date":"2021-10-31","objectID":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/","tags":["Biography"],"title":"Notes from \"Live for Death: My Experience on Dying\"","uri":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/"},{"categories":["Notes"],"content":"Walked through the valley of the shadow of death and tasted the nectar of life. ","date":"2021-10-31","objectID":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/:0:0","tags":["Biography"],"title":"Notes from \"Live for Death: My Experience on Dying\"","uri":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/"},{"categories":["Notes"],"content":"Preface Having served as a senior executive at top technology companies like Apple, Microsoft and Google, Kai-Fu Lee is known as a life winner and has a significant influence on the post-80s generation in China. After being diagnosed with lymphoma, he gained a new understanding and appreciation of life. The book “Live for Death: My Experience on Dying” is a true reproduction of what Kai-Fu Lee thought during his illness, and it shows his personality and values after his rebirth. ","date":"2021-10-31","objectID":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/:1:0","tags":["Biography"],"title":"Notes from \"Live for Death: My Experience on Dying\"","uri":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/"},{"categories":["Notes"],"content":"Summary At the launch of the book, Kai-Fu Lee summarized his insights into the seven death credits for remediation. Health is priceless. I loved food in my normal life, did not like to sleep, and rushed to answer emails every night after I got home, after I got really sick, then I realized that once people lose their health, they have nothing. Young people should work hard only after ensuring that sleep, stress, exercise and diet meet the basic requirements. Everything is done for a reason. Think of it as a learning opportunity, just as you would think of a disaster as a cause rather than an effect. Cherish the destiny, learn to be grateful and love. Learn how to live and live in the moment. Avoid some temptations. Although I thought I was on the right path, but the excessive pursuit of fame made me stray from my original intention. In the first tweet I posted after I got cancer, I said that everyone is equal in front of cancer. However, after I slowly awakened, I realized that everyone is equal in everything. Who are we to say that someone is an ordinary person, someone is not good, or a business will not succeed? What is life all about? If this life is used to experience, to learn, to improve, I believe it will also make the world a better place. Then the whole world and the whole human community will become more positive. ","date":"2021-10-31","objectID":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/:2:0","tags":["Biography"],"title":"Notes from \"Live for Death: My Experience on Dying\"","uri":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/"},{"categories":["Notes"],"content":"Thinking By following his personal experience, I also walked through the valley of the shadow of death, tasted the nectar of life, and woke myself up, immediately took action, began to focus on health and family, and benefited greatly. ","date":"2021-10-31","objectID":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/:3:0","tags":["Biography"],"title":"Notes from \"Live for Death: My Experience on Dying\"","uri":"/en/2021/10/notes-from-live-for-death-my-experience-on-dying/"},{"categories":["Thinking"],"content":"I have renewed my confidence and enthusiasm for learning, the courage to explore the unknown, and an ever-increasing belief in a better future.","date":"2021-10-30","objectID":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/","tags":["Beginning"],"title":"If You Are in Full Bloom, Butterflies Will Come Naturally","uri":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/"},{"categories":["Thinking"],"content":"I have renewed my confidence and enthusiasm for learning, the courage to explore the unknown, and an ever-increasing belief in a better future. ","date":"2021-10-30","objectID":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/:0:0","tags":["Beginning"],"title":"If You Are in Full Bloom, Butterflies Will Come Naturally","uri":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/"},{"categories":["Thinking"],"content":"Why start over It was not on a whim or impulse that I made up my mind to close my old blog and start writing a new one. Time flies, and without notice I have been working for fifteen years since graduated. The cloud computing has become very popular now, so most of the posts in my old blog are outdated, and it has been a long time since I updated it. Two years ago, to break the bottleneck of my development and take it to the next level in my future career, I went back to university for a master’s degree, then I have gained a new life in preparing for the exams and pursuing my studies😄, and also renewed the confidence and enthusiasm for learning, the courage to explore the unknown, and an ever-increasing belief in a better future. Therefore, I decided to start over and write a new blog to document the growth of this new phase of my life. ","date":"2021-10-30","objectID":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/:1:0","tags":["Beginning"],"title":"If You Are in Full Bloom, Butterflies Will Come Naturally","uri":"/en/2021/10/if-you-are-in-full-bloom-butterflies-will-come-naturally/"}]